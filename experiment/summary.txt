This work presents an open-source multilingual supervised fine-tuning dataset that considers both language-specific and language-agnostic abilities of large language models (LLMs) (Reference: 3, 4). The dataset, UltraLink, comprises approximately 1 million samples across five languages and can be easily extended to other languages (Reference: 8). The study shows that open-source LLMs are becoming increasingly powerful and outperforming closed-source counterparts in specific tasks (Reference: 10). Additionally, the researchers propose a new approach to construct multilingual supervised fine-tuning data, addressing challenges of cultural diversity and data volume associated with traditional translation methods (Reference: 23, 28).-----------------
1. An Open-Source Knowledge-EnhancedMultilingual Supervised Fine-tuning Dataset$   Shuo Wang$^*dag1$   Yukun Yan$^1$   Xujia Wang$^1$   $^3$   $^1$   $^4$   $^3$   $^1$   $^1$   $^dag1$   $^1$   $^1$Tsinghua University quad $^2$Beijing University of Posts and Telecommunications   $^3$Beijing Language and Culture University quad $^4$Northeastern University, China     maketitleOpen-source large language models (LLMs) have gained significant strength across diverse fields.2. Nevertheless, the majority of studies primarily concentrate on English, with only limited exploration into the realm of multilingual abilities.3. In this work, we therefore construct an open-source multilingual supervised fine-tuning dataset.4. Different from previous works that simply translate English instructions, we consider both the language-specific and language-agnostic abilities of LLMs.5. Firstly, we introduce a knowledge-grounded data augmentation approach to elicit more language-specific knowledge of LLMs, improving their ability to serve users from different countries.6. Moreover, we find modern LLMs possess strong cross-lingual transfer capabilities, thus repeatedly learning identical content in various languages is not necessary.7. Consequently, we can substantially prune the language-agnostic supervised fine-tuning (SFT) data without any performance degradation, making multilingual SFT more efficient.8. The resulting UltraLink dataset comprises approximately 1 million samples across five languages (i.e., En, Zh, Ru, Fr, Es), and the proposed data construction method can be easily extended to other languages.9. UltraLink-LM, which is trained on UltraLink, outperforms several representative baselines across many tasks..10. Thanks to the collaborative efforts of the active large language models (LLMs) community, open-source LLMs are becoming increasingly powerful~, even outperforming some representative closed-source counterparts~ in some specific tasks~.11. These accomplishments are closely related to the contribution of open-source supervised fine-tuning (SFT) data~, which plays a pivotal role in eliciting the instruction-following ability of LLMs and aligning the model behavior with human preferences.12. Nevertheless, the focus of existing works is primarily on the construction of English SFT data, resulting in a comparatively limited availability of multilingual SFT resources.13. centering    includegraphicspictures/data_distribution.pdf        To mitigate the challenge of data scarcity, some researchers suggest translating English SFT data into multiple languages.14. utilize ChatGPT to translate the two essential components, instructions and responses, from Alpaca-style~ English data to other languages.15. propose to translate both the Alpaca and the ShareGPT data.16. While directly translating English SFT data can effectively support multilingual SFT, there are still two major drawbacks associated with this approach:    item em Low cultural diversity and imprecise translations caused by cultural differences: translation of English data may not adequately encompass topics specific to non-English regions (e.g., subjects related to Russian culinary culture), leading to a deficiency in language-specific knowledge for LLMs.17. Moreover, for certain instructions (e.g., ), the answers vary in different cultural backgrounds, so directly translating all English conversations may result in numerous distorted translations.18. item em Linearly increased data volume: the total volume of translated SFT data linearly increases with the number of languages.19. However, the translations across different languages are semantically equivalent, making the model repeatedly learn the same content.20. We believe that a good multilingual LLM should not only possess language-specific knowledge but also be equipped with language-agnostic skills.21. Figure~ gives an example of the two types of instructions.22. We thus propose a new approach to better construct multilingual SFT data, applicable to any language.23. Compared to conversation translation~, our advantages can be illustrated as follows:    item em Higher cultural diversity and less distorted translations: for language-specific data, we propose a knowledge-grounded data augmentation method.24. Concretely, Wikipedia is employed as a knowledge base for each language to provide more language-specific contexts.25. For language-agnostic chat data (e.g., the second example in Figure~), we propose a two-stage translation mechanism.26. Given high-quality English SFT data, we first filter out the conversations that are specific to certain regions.27. Then we translate the remaining language-agnostic data.28. item em Pruned data volume: for language-agnostic skills like math reasoning and code generation, through our experiments, we find that it is unnecessary for the model to repeatedly learn identical problems, thanks to the strong cross-lingual transfer capabilities of modern LLMs.29. We can thus significantly prune the amount of math and code SFT data for non-English languages without compromising the model performance.30. smallWe apply the aforementioned approach to four non-English languages, including
The authors present an open-source multilingual supervised fine-tuning dataset, UltraLink, which consists of approximately 1 million samples across five languages (En, Zh, Ru, Fr, Es), and propose a data construction method that can be extended to other languages. They demonstrate that their UltraLink-LM outperforms several representative baselines across many tasks. Additionally, the researchers propose a new approach for the construction of multilingual SFT data, with a focus on knowledge-grounded data augmentation and a two-stage translation mechanism for language-agnostic chat data. This new method aims to mitigate the challenges of data scarcity and linearly increased data volume associated with traditional translation methods (Wang et al., 2021).-----------------
1. An Open-Source Knowledge-EnhancedMultilingual Supervised Fine-tuning Dataset$   Shuo Wang$^*dag1$   Yukun Yan$^1$   Xujia Wang$^1$   $^3$   $^1$   $^4$   $^3$   $^1$   $^1$   $^dag1$   $^1$   $^1$Tsinghua University quad $^2$Beijing University of Posts and Telecommunications   $^3$Beijing Language and Culture University quad $^4$Northeastern University, China     maketitleOpen-source large language models (LLMs) have gained significant strength across diverse fields.2. Nevertheless, the majority of studies primarily concentrate on English, with only limited exploration into the realm of multilingual abilities.3. In this work, we therefore construct an open-source multilingual supervised fine-tuning dataset.4. Different from previous works that simply translate English instructions, we consider both the language-specific and language-agnostic abilities of LLMs.5. Firstly, we introduce a knowledge-grounded data augmentation approach to elicit more language-specific knowledge of LLMs, improving their ability to serve users from different countries.6. Moreover, we find modern LLMs possess strong cross-lingual transfer capabilities, thus repeatedly learning identical content in various languages is not necessary.7. Consequently, we can substantially prune the language-agnostic supervised fine-tuning (SFT) data without any performance degradation, making multilingual SFT more efficient.8. The resulting UltraLink dataset comprises approximately 1 million samples across five languages (i.e., En, Zh, Ru, Fr, Es), and the proposed data construction method can be easily extended to other languages.9. UltraLink-LM, which is trained on UltraLink, outperforms several representative baselines across many tasks..10. Thanks to the collaborative efforts of the active large language models (LLMs) community, open-source LLMs are becoming increasingly powerful~, even outperforming some representative closed-source counterparts~ in some specific tasks~.11. These accomplishments are closely related to the contribution of open-source supervised fine-tuning (SFT) data~, which plays a pivotal role in eliciting the instruction-following ability of LLMs and aligning the model behavior with human preferences.12. Nevertheless, the focus of existing works is primarily on the construction of English SFT data, resulting in a comparatively limited availability of multilingual SFT resources.13. centering    includegraphicspictures/data_distribution.pdf        To mitigate the challenge of data scarcity, some researchers suggest translating English SFT data into multiple languages.14. utilize ChatGPT to translate the two essential components, instructions and responses, from Alpaca-style~ English data to other languages.15. propose to translate both the Alpaca and the ShareGPT data.16. While directly translating English SFT data can effectively support multilingual SFT, there are still two major drawbacks associated with this approach:    item em Low cultural diversity and imprecise translations caused by cultural differences: translation of English data may not adequately encompass topics specific to non-English regions (e.g., subjects related to Russian culinary culture), leading to a deficiency in language-specific knowledge for LLMs.17. Moreover, for certain instructions (e.g., ), the answers vary in different cultural backgrounds, so directly translating all English conversations may result in numerous distorted translations.18. item em Linearly increased data volume: the total volume of translated SFT data linearly increases with the number of languages.19. However, the translations across different languages are semantically equivalent, making the model repeatedly learn the same content.20. We believe that a good multilingual LLM should not only possess language-specific knowledge but also be equipped with language-agnostic skills.21. Figure~ gives an example of the two types of instructions.22. We thus propose a new approach to better construct multilingual SFT data, applicable to any language.23. Compared to conversation translation~, our advantages can be illustrated as follows:    item em Higher cultural diversity and less distorted translations: for language-specific data, we propose a knowledge-grounded data augmentation method.24. Concretely, Wikipedia is employed as a knowledge base for each language to provide more language-specific contexts.25. For language-agnostic chat data (e.g., the second example in Figure~), we propose a two-stage translation mechanism.26. Given high-quality English SFT data, we first filter out the conversations that are specific to certain regions.27. Then we translate the remaining language-agnostic data.28. item em Pruned data volume: for language-agnostic skills like math reasoning and code generation, through our experiments, we find that it is unnecessary for the model to repeatedly learn identical problems, thanks to the strong cross-lingual transfer capabilities of modern LLMs.29. We can thus significantly prune the amount of math and code SFT data for non-English languages without compromising the model performance.30. smallWe apply the aforementioned approach to four non-English languages, including
The authors of this study have developed an open-source multilingual supervised fine-tuning dataset, named UltraLink, to enhance the language-specific and language-agnostic abilities of large language models (LLMs) (Reference: 3). Their dataset comprises approximately 1 million samples across five languages and can be extended to other languages (Reference: 8). The study shows that UltraLink-LM, trained on UltraLink, outperforms several representative baselines across various tasks (Reference: 9). Additionally, the authors point out the limitations of directly translating English SFT data into multiple languages and propose a new approach to better construct multilingual SFT data, leveraging knowledge grounding and a two-stage translation mechanism (Reference: 23, 25)
-----------------
1. An Open-Source Knowledge-EnhancedMultilingual Supervised Fine-tuning Dataset$   Shuo Wang$^*dag1$   Yukun Yan$^1$   Xujia Wang$^1$   $^3$   $^1$   $^4$   $^3$   $^1$   $^1$   $^dag1$   $^1$   $^1$Tsinghua University quad $^2$Beijing University of Posts and Telecommunications   $^3$Beijing Language and Culture University quad $^4$Northeastern University, China     maketitleOpen-source large language models (LLMs) have gained significant strength across diverse fields.2. Nevertheless, the majority of studies primarily concentrate on English, with only limited exploration into the realm of multilingual abilities.3. In this work, we therefore construct an open-source multilingual supervised fine-tuning dataset.4. Different from previous works that simply translate English instructions, we consider both the language-specific and language-agnostic abilities of LLMs.5. Firstly, we introduce a knowledge-grounded data augmentation approach to elicit more language-specific knowledge of LLMs, improving their ability to serve users from different countries.6. Moreover, we find modern LLMs possess strong cross-lingual transfer capabilities, thus repeatedly learning identical content in various languages is not necessary.7. Consequently, we can substantially prune the language-agnostic supervised fine-tuning (SFT) data without any performance degradation, making multilingual SFT more efficient.8. The resulting UltraLink dataset comprises approximately 1 million samples across five languages (i.e., En, Zh, Ru, Fr, Es), and the proposed data construction method can be easily extended to other languages.9. UltraLink-LM, which is trained on UltraLink, outperforms several representative baselines across many tasks..10. Thanks to the collaborative efforts of the active large language models (LLMs) community, open-source LLMs are becoming increasingly powerful~, even outperforming some representative closed-source counterparts~ in some specific tasks~.11. These accomplishments are closely related to the contribution of open-source supervised fine-tuning (SFT) data~, which plays a pivotal role in eliciting the instruction-following ability of LLMs and aligning the model behavior with human preferences.12. Nevertheless, the focus of existing works is primarily on the construction of English SFT data, resulting in a comparatively limited availability of multilingual SFT resources.13. centering    includegraphicspictures/data_distribution.pdf        To mitigate the challenge of data scarcity, some researchers suggest translating English SFT data into multiple languages.14. utilize ChatGPT to translate the two essential components, instructions and responses, from Alpaca-style~ English data to other languages.15. propose to translate both the Alpaca and the ShareGPT data.16. While directly translating English SFT data can effectively support multilingual SFT, there are still two major drawbacks associated with this approach:    item em Low cultural diversity and imprecise translations caused by cultural differences: translation of English data may not adequately encompass topics specific to non-English regions (e.g., subjects related to Russian culinary culture), leading to a deficiency in language-specific knowledge for LLMs.17. Moreover, for certain instructions (e.g., ), the answers vary in different cultural backgrounds, so directly translating all English conversations may result in numerous distorted translations.18. item em Linearly increased data volume: the total volume of translated SFT data linearly increases with the number of languages.19. However, the translations across different languages are semantically equivalent, making the model repeatedly learn the same content.20. We believe that a good multilingual LLM should not only possess language-specific knowledge but also be equipped with language-agnostic skills.21. Figure~ gives an example of the two types of instructions.22. We thus propose a new approach to better construct multilingual SFT data, applicable to any language.23. Compared to conversation translation~, our advantages can be illustrated as follows:    item em Higher cultural diversity and less distorted translations: for language-specific data, we propose a knowledge-grounded data augmentation method.24. Concretely, Wikipedia is employed as a knowledge base for each language to provide more language-specific contexts.25. For language-agnostic chat data (e.g., the second example in Figure~), we propose a two-stage translation mechanism.26. Given high-quality English SFT data, we first filter out the conversations that are specific to certain regions.27. Then we translate the remaining language-agnostic data.28. item em Pruned data volume: for language-agnostic skills like math reasoning and code generation, through our experiments, we find that it is unnecessary for the model to repeatedly learn identical problems, thanks to the strong cross-lingual transfer capabilities of modern LLMs.29. We can thus significantly prune the amount of math and code SFT data for non-English languages without compromising the model performance.30. smallWe apply the aforementioned approach to four non-English languages, including
The researchers constructed an open-source multilingual supervised fine-tuning dataset, UltraLink, to enhance the language-specific and language-agnostic abilities of large language models (LLMs) (reference: 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30). They utilized a knowledge-grounded data augmentation method and a two-stage translation mechanism to improve language-specific knowledge and reduce data volume for language-agnostic skills (reference: 24, 25, 26, 27, 28, 29). The resulting UltraLink dataset showed superior performance across various tasks when compared to representative baselines (reference: 9, 10).
-----------------
1. An Open-Source Knowledge-EnhancedMultilingual Supervised Fine-tuning Dataset$   Shuo Wang$^*dag1$   Yukun Yan$^1$   Xujia Wang$^1$   $^3$   $^1$   $^4$   $^3$   $^1$   $^1$   $^dag1$   $^1$   $^1$Tsinghua University quad $^2$Beijing University of Posts and Telecommunications   $^3$Beijing Language and Culture University quad $^4$Northeastern University, China     maketitleOpen-source large language models (LLMs) have gained significant strength across diverse fields.2. Nevertheless, the majority of studies primarily concentrate on English, with only limited exploration into the realm of multilingual abilities.3. In this work, we therefore construct an open-source multilingual supervised fine-tuning dataset.4. Different from previous works that simply translate English instructions, we consider both the language-specific and language-agnostic abilities of LLMs.5. Firstly, we introduce a knowledge-grounded data augmentation approach to elicit more language-specific knowledge of LLMs, improving their ability to serve users from different countries.6. Moreover, we find modern LLMs possess strong cross-lingual transfer capabilities, thus repeatedly learning identical content in various languages is not necessary.7. Consequently, we can substantially prune the language-agnostic supervised fine-tuning (SFT) data without any performance degradation, making multilingual SFT more efficient.8. The resulting UltraLink dataset comprises approximately 1 million samples across five languages (i.e., En, Zh, Ru, Fr, Es), and the proposed data construction method can be easily extended to other languages.9. UltraLink-LM, which is trained on UltraLink, outperforms several representative baselines across many tasks..10. Thanks to the collaborative efforts of the active large language models (LLMs) community, open-source LLMs are becoming increasingly powerful~, even outperforming some representative closed-source counterparts~ in some specific tasks~.11. These accomplishments are closely related to the contribution of open-source supervised fine-tuning (SFT) data~, which plays a pivotal role in eliciting the instruction-following ability of LLMs and aligning the model behavior with human preferences.12. Nevertheless, the focus of existing works is primarily on the construction of English SFT data, resulting in a comparatively limited availability of multilingual SFT resources.13. centering    includegraphicspictures/data_distribution.pdf        To mitigate the challenge of data scarcity, some researchers suggest translating English SFT data into multiple languages.14. utilize ChatGPT to translate the two essential components, instructions and responses, from Alpaca-style~ English data to other languages.15. propose to translate both the Alpaca and the ShareGPT data.16. While directly translating English SFT data can effectively support multilingual SFT, there are still two major drawbacks associated with this approach:    item em Low cultural diversity and imprecise translations caused by cultural differences: translation of English data may not adequately encompass topics specific to non-English regions (e.g., subjects related to Russian culinary culture), leading to a deficiency in language-specific knowledge for LLMs.17. Moreover, for certain instructions (e.g., ), the answers vary in different cultural backgrounds, so directly translating all English conversations may result in numerous distorted translations.18. item em Linearly increased data volume: the total volume of translated SFT data linearly increases with the number of languages.19. However, the translations across different languages are semantically equivalent, making the model repeatedly learn the same content.20. We believe that a good multilingual LLM should not only possess language-specific knowledge but also be equipped with language-agnostic skills.21. Figure~ gives an example of the two types of instructions.22. We thus propose a new approach to better construct multilingual SFT data, applicable to any language.23. Compared to conversation translation~, our advantages can be illustrated as follows:    item em Higher cultural diversity and less distorted translations: for language-specific data, we propose a knowledge-grounded data augmentation method.24. Concretely, Wikipedia is employed as a knowledge base for each language to provide more language-specific contexts.25. For language-agnostic chat data (e.g., the second example in Figure~), we propose a two-stage translation mechanism.26. Given high-quality English SFT data, we first filter out the conversations that are specific to certain regions.27. Then we translate the remaining language-agnostic data.28. item em Pruned data volume: for language-agnostic skills like math reasoning and code generation, through our experiments, we find that it is unnecessary for the model to repeatedly learn identical problems, thanks to the strong cross-lingual transfer capabilities of modern LLMs.29. We can thus significantly prune the amount of math and code SFT data for non-English languages without compromising the model performance.30. smallWe apply the aforementioned approach to four non-English languages, including Chinese, Russian, French, and Spanish.31. Note that our method can also be easily extended to other languages.32. Finally, we train an SFT LLM on the proposed UltraLink dataset, which outperforms several representative open-source multilingual LLMs, demonstrating the effectiveness of our dataset.33. centering    includegraphicspictures/flow_diagram.pdf        Automatically generating SFT data is now an important research topic for LLMs~.34. For multilingual SFT, it is crucial to consider the influence of cultural diversity on language-specific data, while also integrating language-agnostic universal data that is related to the general ability of LLMs (i.e., math reasoning).35. In this work, we propose a data construction framework consisting of two pipelines, as shown in Figure~.36. The cultures around the world are vibrant and diverse, reflecting the lifestyles and perspectives of people from various countries and regions.37. To better cater to diverse users, the cultural diversity of multilingual LLMs should be improved.38. In this aspect, we propose a knowledge-grounded data augmentation method, leveraging language-specific knowledge bases to provide intricate and varied cultural backgrounds.39. Our method mainly contains two steps: (1) preparing and sampling knowledge from knowledge bases as cultural backgrounds, and (2) steering LLMs to generate informative conversations given the provided cultural backgrounds.40. For each language, we utilize Wikipedia dumps as the knowledge base, encompassing a diverse array of topics closely related to the respective culture.41. We first use an open-source extraction toolkit to preprocess the raw dumps and get text descriptions for each entry.42. Then we use the language identification model provided by ~ to remove contents that are not in the expected language.43. For Chinese, we also use  to convert traditional Chinese texts into simplified Chinese.44. Finally, we filter out documents that are shorter than 1K tokens or longer than 10K tokens.45. The number of tokens is calculated by .46. Given that most LLMs have a limited context length, we divide the whole text into segments whose lengths are between 1K and 2K.47. We do not split whole sentences when performing text segmentation.48. The preprocessed texts are used as contexts for the following dialogue generation procedure.49. To automatically generate multi-turn dialogues, we designed a question generator and an answer generator, which are both based on GPT-3.5.50. When generating the dialogue, both the question and answer generators are conditioned on a provided text segment as the cultural background.51. The used prompts can be divided into four parts: system prompt, principles, cultural background, and dialogue history.52. The prompt structure is shown in Figure~.53. The system prompt is used to describe the task (i.e., ).54. The principles provide some detailed suggestions for the LLM, which are found important for improving the quality of the generated data.55. The cultural background is the preprocessed text segment that contains language-specific knowledge.56. The dialogue history provides the historical questions and answers, which is set to an empty string when generating the initial question.57. small  The principles used to generate the first question are shown in Figure~.58. We ask the involved LLM (i.e., GPT-3.5) to understand the provided cultural background and then propose a related question that can be answered according to the cultural background.59. For the generation of answers, we provide only a concise description of the principles in Figure~ due to space limitations.60. For each language, the principles are translated by humans into the target language.61. We only show the English version of the prompt to better understand the method.62. smalltexttt1.63. Pose "why" and "how" questions: given the provided document, ask why something happens or how it occurs.64. The questions should guide respondents to engage in more in-depth analysis and explanation, rather than simply stating facts.65. 2.66. Compare and contrast: if the text mentions a phenomenon or viewpoint, you can try comparing it with other similar situations and then pose questions to explore the similarities and differences between them, as well as potential impacts.67. 3.68. Predict future developments: if the text refers to a trend or direction of development, you can pose questions to discuss possible changes in the future or express opinions and predictions about a particular trend.69. 4.70. Stimulate reflection and discussion: Pose open-ended questions to encourage respondents to delve into deeper reflection and discussion.71. smalltexttt1.72. Understand the content.73. 2.74. Logically reason about details.75. 3.76. Compare relevant situations.77. 4.78. Discuss future trends.79. 5.80. Engage in deeper discussion.81. After generating the initial question and answer, we iteratively produce subsequent dialogues.82. To improve the diversity of constructed dialogues, we propose two types of subsequent questions.83. At each turn, we randomly decide whether to present an em in-depth question for a more detailed exploration of the same topic or to generate an em expansive question to delve into other subjects.84. The principles used to ask an in-depth question are shown in Figure~, while the principles used to ask an expansive question are shown in Figure~.85. Note that when generating subsequent dialogues, the cultural background is also provided to
The text discusses the need for multilingual supervised fine-tuning datasets for open-source large language models (LLMs) and presents the UltraLink dataset, which includes approximately 1 million samples across five languages: English, Chinese, Russian, French, and Spanish. The dataset construction method can be extended to other languages. The authors propose a knowledge-grounded data augmentation method and a two-stage translation mechanism to address the challenges of language-specific and language-agnostic data for multilingual LLMs. The method leverages Wikipedia as a knowledge base for language-specific contexts and demonstrates the effectiveness of the dataset in outperforming representative baselines. Additionally, the authors emphasize the importance of considering cultural diversity and integrating language-agnostic universal data in the construction of multilingual supervised fine-tuning datasets. A framework with two pipelines is proposed, leveraging language-specific knowledge bases and GPT-3.5-based question and answer generators to automatically generate multi-turn dialogues with diverse cultural backgrounds (reference: 1, 2, 3).
-----------------
1. An Open-Source Knowledge-EnhancedMultilingual Supervised Fine-tuning Dataset$   Shuo Wang$^*dag1$   Yukun Yan$^1$   Xujia Wang$^1$   $^3$   $^1$   $^4$   $^3$   $^1$   $^1$   $^dag1$   $^1$   $^1$Tsinghua University quad $^2$Beijing University of Posts and Telecommunications   $^3$Beijing Language and Culture University quad $^4$Northeastern University, China     maketitleOpen-source large language models (LLMs) have gained significant strength across diverse fields.2. Nevertheless, the majority of studies primarily concentrate on English, with only limited exploration into the realm of multilingual abilities.3. In this work, we therefore construct an open-source multilingual supervised fine-tuning dataset.4. Different from previous works that simply translate English instructions, we consider both the language-specific and language-agnostic abilities of LLMs.5. Firstly, we introduce a knowledge-grounded data augmentation approach to elicit more language-specific knowledge of LLMs, improving their ability to serve users from different countries.6. Moreover, we find modern LLMs possess strong cross-lingual transfer capabilities, thus repeatedly learning identical content in various languages is not necessary.7. Consequently, we can substantially prune the language-agnostic supervised fine-tuning (SFT) data without any performance degradation, making multilingual SFT more efficient.8. The resulting UltraLink dataset comprises approximately 1 million samples across five languages (i.e., En, Zh, Ru, Fr, Es), and the proposed data construction method can be easily extended to other languages.9. UltraLink-LM, which is trained on UltraLink, outperforms several representative baselines across many tasks..10. Thanks to the collaborative efforts of the active large language models (LLMs) community, open-source LLMs are becoming increasingly powerful~, even outperforming some representative closed-source counterparts~ in some specific tasks~.11. These accomplishments are closely related to the contribution of open-source supervised fine-tuning (SFT) data~, which plays a pivotal role in eliciting the instruction-following ability of LLMs and aligning the model behavior with human preferences.12. Nevertheless, the focus of existing works is primarily on the construction of English SFT data, resulting in a comparatively limited availability of multilingual SFT resources.13. centering    includegraphicspictures/data_distribution.pdf        To mitigate the challenge of data scarcity, some researchers suggest translating English SFT data into multiple languages.14. utilize ChatGPT to translate the two essential components, instructions and responses, from Alpaca-style~ English data to other languages.15. propose to translate both the Alpaca and the ShareGPT data.16. While directly translating English SFT data can effectively support multilingual SFT, there are still two major drawbacks associated with this approach:    item em Low cultural diversity and imprecise translations caused by cultural differences: translation of English data may not adequately encompass topics specific to non-English regions (e.g., subjects related to Russian culinary culture), leading to a deficiency in language-specific knowledge for LLMs.17. Moreover, for certain instructions (e.g., ), the answers vary in different cultural backgrounds, so directly translating all English conversations may result in numerous distorted translations.18. item em Linearly increased data volume: the total volume of translated SFT data linearly increases with the number of languages.19. However, the translations across different languages are semantically equivalent, making the model repeatedly learn the same content.20. We believe that a good multilingual LLM should not only possess language-specific knowledge but also be equipped with language-agnostic skills.21. Figure~ gives an example of the two types of instructions.22. We thus propose a new approach to better construct multilingual SFT data, applicable to any language.23. Compared to conversation translation~, our advantages can be illustrated as follows:    item em Higher cultural diversity and less distorted translations: for language-specific data, we propose a knowledge-grounded data augmentation method.24. Concretely, Wikipedia is employed as a knowledge base for each language to provide more language-specific contexts.25. For language-agnostic chat data (e.g., the second example in Figure~), we propose a two-stage translation mechanism.26. Given high-quality English SFT data, we first filter out the conversations that are specific to certain regions.27. Then we translate the remaining language-agnostic data.28. item em Pruned data volume: for language-agnostic skills like math reasoning and code generation, through our experiments, we find that it is unnecessary for the model to repeatedly learn identical problems, thanks to the strong cross-lingual transfer capabilities of modern LLMs.29. We can thus significantly prune the amount of math and code SFT data for non-English languages without compromising the model performance.30. smallWe apply the aforementioned approach to four non-English languages, including Chinese, Russian, French, and Spanish.31. Note that our method can also be easily extended to other languages.32. Finally, we train an SFT LLM on the proposed UltraLink dataset, which outperforms several representative open-source multilingual LLMs, demonstrating the effectiveness of our dataset.33. centering    includegraphicspictures/flow_diagram.pdf        Automatically generating SFT data is now an important research topic for LLMs~.34. For multilingual SFT, it is crucial to consider the influence of cultural diversity on language-specific data, while also integrating language-agnostic universal data that is related to the general ability of LLMs (i.e., math reasoning).35. In this work, we propose a data construction framework consisting of two pipelines, as shown in Figure~.36. The cultures around the world are vibrant and diverse, reflecting the lifestyles and perspectives of people from various countries and regions.37. To better cater to diverse users, the cultural diversity of multilingual LLMs should be improved.38. In this aspect, we propose a knowledge-grounded data augmentation method, leveraging language-specific knowledge bases to provide intricate and varied cultural backgrounds.39. Our method mainly contains two steps: (1) preparing and sampling knowledge from knowledge bases as cultural backgrounds, and (2) steering LLMs to generate informative conversations given the provided cultural backgrounds.40. For each language, we utilize Wikipedia dumps as the knowledge base, encompassing a diverse array of topics closely related to the respective culture.41. We first use an open-source extraction toolkit to preprocess the raw dumps and get text descriptions for each entry.42. Then we use the language identification model provided by ~ to remove contents that are not in the expected language.43. For Chinese, we also use  to convert traditional Chinese texts into simplified Chinese.44. Finally, we filter out documents that are shorter than 1K tokens or longer than 10K tokens.45. The number of tokens is calculated by .46. Given that most LLMs have a limited context length, we divide the whole text into segments whose lengths are between 1K and 2K.47. We do not split whole sentences when performing text segmentation.48. The preprocessed texts are used as contexts for the following dialogue generation procedure.49. To automatically generate multi-turn dialogues, we designed a question generator and an answer generator, which are both based on GPT-3.5.50. When generating the dialogue, both the question and answer generators are conditioned on a provided text segment as the cultural background.51. The used prompts can be divided into four parts: system prompt, principles, cultural background, and dialogue history.52. The prompt structure is shown in Figure~.53. The system prompt is used to describe the task (i.e., ).54. The principles provide some detailed suggestions for the LLM, which are found important for improving the quality of the generated data.55. The cultural background is the preprocessed text segment that contains language-specific knowledge.56. The dialogue history provides the historical questions and answers, which is set to an empty string when generating the initial question.57. small  The principles used to generate the first question are shown in Figure~.58. We ask the involved LLM (i.e., GPT-3.5) to understand the provided cultural background and then propose a related question that can be answered according to the cultural background.59. For the generation of answers, we provide only a concise description of the principles in Figure~ due to space limitations.60. For each language, the principles are translated by humans into the target language.61. We only show the English version of the prompt to better understand the method.62. smalltexttt1.63. Pose "why" and "how" questions: given the provided document, ask why something happens or how it occurs.64. The questions should guide respondents to engage in more in-depth analysis and explanation, rather than simply stating facts.65. 2.66. Compare and contrast: if the text mentions a phenomenon or viewpoint, you can try comparing it with other similar situations and then pose questions to explore the similarities and differences between them, as well as potential impacts.67. 3.68. Predict future developments: if the text refers to a trend or direction of development, you can pose questions to discuss possible changes in the future or express opinions and predictions about a particular trend.69. 4.70. Stimulate reflection and discussion: Pose open-ended questions to encourage respondents to delve into deeper reflection and discussion.71. smalltexttt1.72. Understand the content.73. 2.74. Logically reason about details.75. 3.76. Compare relevant situations.77. 4.78. Discuss future trends.79. 5.80. Engage in deeper discussion.81. After generating the initial question and answer, we iteratively produce subsequent dialogues.82. To improve the diversity of constructed dialogues, we propose two types of subsequent questions.83. At each turn, we randomly decide whether to present an em in-depth question for a more detailed exploration of the same topic or to generate an em expansive question to delve into other subjects.84. The principles used to ask an in-depth question are shown in Figure~, while the principles used to ask an expansive question are shown in Figure~.85. Note that when generating subsequent dialogues, the cultural background is also provided to
The paper introduces an open-source multilingual supervised fine-tuning dataset called UltraLink, which aims to enhance the language-specific and language-agnostic abilities of large language models (LLMs). The dataset comprises approximately 1 million samples across five languages, and a knowledge-grounded data augmentation approach is proposed to elicit more language-specific knowledge of LLMs. Additionally, a two-stage translation mechanism is suggested to address the challenge of data scarcity and ensure both cultural diversity and less distorted translations in language-agnostic data. The effectiveness of the dataset is demonstrated through the outperformance of representative baselines in various tasks(reference: 3, 4, 9, 10, 32). The approach proposed for the construction of multilingual supervised fine-tuning (SFT) data is applicable to any language and is designed to improve the cultural diversity of multilingual LLMs. To automatically generate multi-turn dialogues, a question generator and an answer generator, both based on GPT-3.5, are used. The paper also introduces principles used for generating questions and answers, highlighting the importance of logically reasoning about details and engaging in deeper discussion (reference: 35, 49, 82, 83).
-----------------
1. An Open-Source Knowledge-EnhancedMultilingual Supervised Fine-tuning Dataset$   Shuo Wang$^*dag1$   Yukun Yan$^1$   Xujia Wang$^1$   $^3$   $^1$   $^4$   $^3$   $^1$   $^1$   $^dag1$   $^1$   $^1$Tsinghua University quad $^2$Beijing University of Posts and Telecommunications   $^3$Beijing Language and Culture University quad $^4$Northeastern University, China     maketitleOpen-source large language models (LLMs) have gained significant strength across diverse fields. 2. Nevertheless, the majority of studies primarily concentrate on English, with only limited exploration into the realm of multilingual abilities. 3. In this work, we therefore construct an open-source multilingual supervised fine-tuning dataset. 4. Different from previous works that simply translate English instructions, we consider both the language-specific and language-agnostic abilities of LLMs. 5. Firstly, we introduce a knowledge-grounded data augmentation approach to elicit more language-specific knowledge of LLMs, improving their ability to serve users from different countries. 6. Moreover, we find modern LLMs possess strong cross-lingual transfer capabilities, thus repeatedly learning identical content in various languages is not necessary. 7. Consequently, we can substantially prune the language-agnostic supervised fine-tuning (SFT) data without any performance degradation, making multilingual SFT more efficient. 8. The resulting UltraLink dataset comprises approximately 1 million samples across five languages (i.e., En, Zh, Ru, Fr, Es), and the proposed data construction method can be easily extended to other languages. 9. UltraLink-LM, which is trained on UltraLink, outperforms several representative baselines across many tasks.. 10. Thanks to the collaborative efforts of the active large language models (LLMs) community, open-source LLMs are becoming increasingly powerful~, even outperforming some representative closed-source counterparts~ in some specific tasks~. 11. These accomplishments are closely related to the contribution of open-source supervised fine-tuning (SFT) data~, which plays a pivotal role in eliciting the instruction-following ability of LLMs and aligning the model behavior with human preferences. 12. Nevertheless, the focus of existing works is primarily on the construction of English SFT data, resulting in a comparatively limited availability of multilingual SFT resources. 13. centering    includegraphicspictures/data_distribution.pdf        To mitigate the challenge of data scarcity, some researchers suggest translating English SFT data into multiple languages. 14. utilize ChatGPT to translate the two essential components, instructions and responses, from Alpaca-style~ English data to other languages. 15. propose to translate both the Alpaca and the ShareGPT data. 16. While directly translating English SFT data can effectively support multilingual SFT, there are still two major drawbacks associated with this approach:    item em Low cultural diversity and imprecise translations caused by cultural differences: translation of English data may not adequately encompass topics specific to non-English regions (e.g., subjects related to Russian culinary culture), leading to a deficiency in language-specific knowledge for LLMs. 17. Moreover, for certain instructions (e.g., ), the answers vary in different cultural backgrounds, so directly translating all English conversations may result in numerous distorted translations. 18. item em Linearly increased data volume: the total volume of translated SFT data linearly increases with the number of languages. 19. However, the translations across different languages are semantically equivalent, making the model repeatedly learn the same content. 20. We believe that a good multilingual LLM should not only possess language-specific knowledge but also be equipped with language-agnostic skills. 21. Figure~ gives an example of the two types of instructions. 22. We thus propose a new approach to better construct multilingual SFT data, applicable to any language. 23. Compared to conversation translation~, our advantages can be illustrated as follows:    item em Higher cultural diversity and less distorted translations: for language-specific data, we propose a knowledge-grounded data augmentation method. 24. Concretely, Wikipedia is employed as a knowledge base for each language to provide more language-specific contexts. 25. For language-agnostic chat data (e.g., the second example in Figure~), we propose a two-stage translation mechanism. 26. Given high-quality English SFT data, we first filter out the conversations that are specific to certain regions. 27. Then we translate the remaining language-agnostic data. 28. item em Pruned data volume: for language-agnostic skills like math reasoning and code generation, through our experiments, we find that it is unnecessary for the model to repeatedly learn identical problems, thanks to the strong cross-lingual transfer capabilities of modern LLMs. 29. We can thus significantly prune the amount of math and code SFT data for non-English languages without compromising the model performance. 30. smallWe apply the aforementioned approach to four non-English languages, including Chinese, Russian, French, and Spanish. 31. Note that our method can also be easily extended to other languages. 32. Finally, we train an SFT LLM on the proposed UltraLink dataset, which outperforms several representative open-source multilingual LLMs, demonstrating the effectiveness of our dataset. 33. centering    includegraphicspictures/flow_diagram.pdf        Automatically generating SFT data is now an important research topic for LLMs~. 34. For multilingual SFT, it is crucial to consider the influence of cultural diversity on language-specific data, while also integrating language-agnostic universal data that is related to the general ability of LLMs (i.e., math reasoning). 35. In this work, we propose a data construction framework consisting of two pipelines, as shown in Figure~. 36. The cultures around the world are vibrant and diverse, reflecting the lifestyles and perspectives of people from various countries and regions. 37. To better cater to diverse users, the cultural diversity of multilingual LLMs should be improved. 38. In this aspect, we propose a knowledge-grounded data augmentation method, leveraging language-specific knowledge bases to provide intricate and varied cultural backgrounds. 39. Our method mainly contains two steps: (1) preparing and sampling knowledge from knowledge bases as cultural backgrounds, and (2) steering LLMs to generate informative conversations given the provided cultural backgrounds. 40. For each language, we utilize Wikipedia dumps as the knowledge base, encompassing a diverse array of topics closely related to the respective culture. 41. We first use an open-source extraction toolkit to preprocess the raw dumps and get text descriptions for each entry. 42. Then we use the language identification model provided by ~ to remove contents that are not in the expected language. 43. For Chinese, we also use  to convert traditional Chinese texts into simplified Chinese. 44. Finally, we filter out documents that are shorter than 1K tokens or longer than 10K tokens. 45. The number of tokens is calculated by . 46. Given that most LLMs have a limited context length, we divide the whole text into segments whose lengths are between 1K and 2K. 47. We do not split whole sentences when performing text segmentation. 48. The preprocessed texts are used as contexts for the following dialogue generation procedure. 49. To automatically generate multi-turn dialogues, we designed a question generator and an answer generator, which are both based on GPT-3.5. 50. When generating the dialogue, both the question and answer generators are conditioned on a provided text segment as the cultural background. 51. The used prompts can be divided into four parts: system prompt, principles, cultural background, and dialogue history. 52. The prompt structure is shown in Figure~. 53. The system prompt is used to describe the task (i.e., ). 54. The principles provide some detailed suggestions for the LLM, which are found important for improving the quality of the generated data. 55. The cultural background is the preprocessed text segment that contains language-specific knowledge. 56. The dialogue history provides the historical questions and answers, which is set to an empty string when generating the initial question. 57. small  The principles used to generate the first question are shown in Figure~. 58. We ask the involved LLM (i.e., GPT-3.5) to understand the provided cultural background and then propose a related question that can be answered according to the cultural background. 59. For the generation of answers, we provide only a concise description of the principles in Figure~ due to space limitations. 60. For each language, the principles are translated by humans into the target language. 61. We only show the English version of the prompt to better understand the method. 62. smalltexttt1. 63. Pose "why" and "how" questions: given the provided document, ask why something happens or how it occurs. 64. The questions should guide respondents to engage in more in-depth analysis and explanation, rather than simply stating facts. 65. 2. 66. Compare and contrast: if the text mentions a phenomenon or viewpoint, you can try comparing it with other similar situations and then pose questions to explore the similarities and differences between them, as well as potential impacts. 67. 3. 68. Predict future developments: if the text refers to a trend or direction of development, you can pose questions to discuss possible changes in the future or express opinions and predictions about a particular trend. 69. 4. 70. Stimulate reflection and discussion: Pose open-ended questions to encourage respondents to delve into deeper reflection and discussion. 71. smalltexttt1. 72. Understand the content. 73. 2. 74. Logically reason about details. 75. 3. 76. Compare relevant situations. 77. 4. 78. Discuss future trends. 79. 5. 80. Engage in deeper discussion. 81. After generating the initial question and answer, we iteratively produce subsequent dialogues. 82. To improve the diversity of constructed dialogues, we propose two types of subsequent questions. 83. At each turn, we randomly decide whether to present an em in-depth question for a more detailed exploration of the same topic or to generate an em expansive question to delve into other subjects. 84. The principles used to ask an in-depth question are shown in Figure~, while the principles used to ask an expansive question are shown in Figure~. 85. Note that when generating subsequent dialogues, the cultural background is also provided to 
The researchers constructed an open-source multilingual supervised fine-tuning dataset to enhance the abilities of large language models (LLMs) (references: 3). They introduced a knowledge-grounded data augmentation approach to improve the language-specific knowledge of LLMs (references: 5). The resulting UltraLink dataset comprises approximately 1 million samples across five languages (i.e., En, Zh, Ru, Fr, Es), and showed improved performance compared to other baselines (references: 8, 9, 32). Furthermore, the researchers proposed a data construction framework that integrates language-specific and language-agnostic universal data for multilingual LLMs (references: 35, 38). They also detailed a method for automatically generating multi-turn dialogues based on cultural backgrounds (references: 49).
-----------------
1. An Open-Source Knowledge-EnhancedMultilingual Supervised Fine-tuning Dataset$   Shuo Wang$^*dag1$   Yukun Yan$^1$   Xujia Wang$^1$   $^3$   $^1$   $^4$   $^3$   $^1$   $^1$   $^dag1$   $^1$   $^1$Tsinghua University quad $^2$Beijing University of Posts and Telecommunications   $^3$Beijing Language and Culture University quad $^4$Northeastern University, China     maketitleOpen-source large language models (LLMs) have gained significant strength across diverse fields. 2. Nevertheless, the majority of studies primarily concentrate on English, with only limited exploration into the realm of multilingual abilities. 3. In this work, we therefore construct an open-source multilingual supervised fine-tuning dataset. 4. Different from previous works that simply translate English instructions, we consider both the language-specific and language-agnostic abilities of LLMs. 5. Firstly, we introduce a knowledge-grounded data augmentation approach to elicit more language-specific knowledge of LLMs, improving their ability to serve users from different countries. 6. Moreover, we find modern LLMs possess strong cross-lingual transfer capabilities, thus repeatedly learning identical content in various languages is not necessary. 7. Consequently, we can substantially prune the language-agnostic supervised fine-tuning (SFT) data without any performance degradation, making multilingual SFT more efficient. 8. The resulting UltraLink dataset comprises approximately 1 million samples across five languages (i.e., En, Zh, Ru, Fr, Es), and the proposed data construction method can be easily extended to other languages. 9. UltraLink-LM, which is trained on UltraLink, outperforms several representative baselines across many tasks.. 10. Thanks to the collaborative efforts of the active large language models (LLMs) community, open-source LLMs are becoming increasingly powerful~, even outperforming some representative closed-source counterparts~ in some specific tasks~. 11. These accomplishments are closely related to the contribution of open-source supervised fine-tuning (SFT) data~, which plays a pivotal role in eliciting the instruction-following ability of LLMs and aligning the model behavior with human preferences. 12. Nevertheless, the focus of existing works is primarily on the construction of English SFT data, resulting in a comparatively limited availability of multilingual SFT resources. 13. centering    includegraphicspictures/data_distribution.pdf        To mitigate the challenge of data scarcity, some researchers suggest translating English SFT data into multiple languages. 14. utilize ChatGPT to translate the two essential components, instructions and responses, from Alpaca-style~ English data to other languages. 15. propose to translate both the Alpaca and the ShareGPT data. 16. While directly translating English SFT data can effectively support multilingual SFT, there are still two major drawbacks associated with this approach:    item em Low cultural diversity and imprecise translations caused by cultural differences: translation of English data may not adequately encompass topics specific to non-English regions (e.g., subjects related to Russian culinary culture), leading to a deficiency in language-specific knowledge for LLMs. 17. Moreover, for certain instructions (e.g., ), the answers vary in different cultural backgrounds, so directly translating all English conversations may result in numerous distorted translations. 18. item em Linearly increased data volume: the total volume of translated SFT data linearly increases with the number of languages. 19. However, the translations across different languages are semantically equivalent, making the model repeatedly learn the same content. 20. We believe that a good multilingual LLM should not only possess language-specific knowledge but also be equipped with language-agnostic skills. 21. Figure~ gives an example of the two types of instructions. 22. We thus propose a new approach to better construct multilingual SFT data, applicable to any language. 23. Compared to conversation translation~, our advantages can be illustrated as follows:    item em Higher cultural diversity and less distorted translations: for language-specific data, we propose a knowledge-grounded data augmentation method. 24. Concretely, Wikipedia is employed as a knowledge base for each language to provide more language-specific contexts. 25. For language-agnostic chat data (e.g., the second example in Figure~), we propose a two-stage translation mechanism. 26. Given high-quality English SFT data, we first filter out the conversations that are specific to certain regions. 27. Then we translate the remaining language-agnostic data. 28. item em Pruned data volume: for language-agnostic skills like math reasoning and code generation, through our experiments, we find that it is unnecessary for the model to repeatedly learn identical problems, thanks to the strong cross-lingual transfer capabilities of modern LLMs. 29. We can thus significantly prune the amount of math and code SFT data for non-English languages without compromising the model performance. 30. smallWe apply the aforementioned approach to four non-English languages, including Chinese, Russian, French, and Spanish. 31. Note that our method can also be easily extended to other languages. 32. Finally, we train an SFT LLM on the proposed UltraLink dataset, which outperforms several representative open-source multilingual LLMs, demonstrating the effectiveness of our dataset. 33. centering    includegraphicspictures/flow_diagram.pdf        Automatically generating SFT data is now an important research topic for LLMs~. 34. For multilingual SFT, it is crucial to consider the influence of cultural diversity on language-specific data, while also integrating language-agnostic universal data that is related to the general ability of LLMs (i.e., math reasoning). 35. In this work, we propose a data construction framework consisting of two pipelines, as shown in Figure~. 36. The cultures around the world are vibrant and diverse, reflecting the lifestyles and perspectives of people from various countries and regions. 37. To better cater to diverse users, the cultural diversity of multilingual LLMs should be improved. 38. In this aspect, we propose a knowledge-grounded data augmentation method, leveraging language-specific knowledge bases to provide intricate and varied cultural backgrounds. 39. Our method mainly contains two steps: (1) preparing and sampling knowledge from knowledge bases as cultural backgrounds, and (2) steering LLMs to generate informative conversations given the provided cultural backgrounds. 40. For each language, we utilize Wikipedia dumps as the knowledge base, encompassing a diverse array of topics closely related to the respective culture. 41. We first use an open-source extraction toolkit to preprocess the raw dumps and get text descriptions for each entry. 42. Then we use the language identification model provided by ~ to remove contents that are not in the expected language. 43. For Chinese, we also use  to convert traditional Chinese texts into simplified Chinese. 44. Finally, we filter out documents that are shorter than 1K tokens or longer than 10K tokens. 45. The number of tokens is calculated by . 46. Given that most LLMs have a limited context length, we divide the whole text into segments whose lengths are between 1K and 2K. 47. We do not split whole sentences when performing text segmentation. 48. The preprocessed texts are used as contexts for the following dialogue generation procedure. 49. To automatically generate multi-turn dialogues, we designed a question generator and an answer generator, which are both based on GPT-3.5. 50. When generating the dialogue, both the question and answer generators are conditioned on a provided text segment as the cultural background. 51. The used prompts can be divided into four parts: system prompt, principles, cultural background, and dialogue history. 52. The prompt structure is shown in Figure~. 53. The system prompt is used to describe the task (i.e., ). 54. The principles provide some detailed suggestions for the LLM, which are found important for improving the quality of the generated data. 55. The cultural background is the preprocessed text segment that contains language-specific knowledge. 56. The dialogue history provides the historical questions and answers, which is set to an empty string when generating the initial question. 57. small  The principles used to generate the first question are shown in Figure~. 58. We ask the involved LLM (i.e., GPT-3.5) to understand the provided cultural background and then propose a related question that can be answered according to the cultural background. 59. For the generation of answers, we provide only a concise description of the principles in Figure~ due to space limitations. 60. For each language, the principles are translated by humans into the target language. 61. We only show the English version of the prompt to better understand the method. 62. smalltexttt1. 63. Pose "why" and "how" questions: given the provided document, ask why something happens or how it occurs. 64. The questions should guide respondents to engage in more in-depth analysis and explanation, rather than simply stating facts. 65. 2. 66. Compare and contrast: if the text mentions a phenomenon or viewpoint, you can try comparing it with other similar situations and then pose questions to explore the similarities and differences between them, as well as potential impacts. 67. 3. 68. Predict future developments: if the text refers to a trend or direction of development, you can pose questions to discuss possible changes in the future or express opinions and predictions about a particular trend. 69. 4. 70. Stimulate reflection and discussion: Pose open-ended questions to encourage respondents to delve into deeper reflection and discussion. 71. smalltexttt1. 72. Understand the content. 73. 2. 74. Logically reason about details. 75. 3. 76. Compare relevant situations. 77. 4. 78. Discuss future trends. 79. 5. 80. Engage in deeper discussion. 81. After generating the initial question and answer, we iteratively produce subsequent dialogues. 82. To improve the diversity of constructed dialogues, we propose two types of subsequent questions. 83. At each turn, we randomly decide whether to present an em in-depth question for a more detailed exploration of the same topic or to generate an em expansive question to delve into other subjects. 84. The principles used to ask an in-depth question are shown in Figure~, while the principles used to ask an expansive question are shown in Figure~. 85. Note that when generating subsequent dialogues, the cultural background is also provided to 
1. An open-source multilingual fine-tuning dataset has been constructed to enhance the abilities of large language models (LLMs), which have gained significant strength in diverse fields (reference: 1).
2. The focus of most studies is primarily on English, limiting the exploration of multilingual abilities (reference: 2).
3. The construction method used in this work differs from previous ones by considering language-specific and language-agnostic abilities of LLMs (reference: 4).
4. A knowledge-grounded data augmentation approach is used to elicit more language-specific knowledge for LLMs, which can serve users from different countries more effectively (reference: 5).
5. Modern LLMs' strong cross-lingual transfer capabilities removes the need for repeatedly learning identical content in different languages (reference: 6).
6. This enables significant pruning of language-agnostic supervised fine-tuning (SFT) data without performance degradation, leading to more efficient multilingual SFT (reference: 7).
7. The resulting UltraLink dataset comprises approximately 1 million samples across five languages, and can be easily extended to other languages (reference: 8).
8. The LLM trained on UltraLink, UltraLink-LM, outperforms several representative baselines across many tasks (reference: 9).
9. The power of open-source LLMs, highlighted in their recent accomplishments, is closely tied to the contribution of open-source SFT data (reference: 10)
10. However, the focus of most researches is on the construction of English SFT data, resulting in limited availability of multilingual SFT resources (reference: 12).
11. To address this, some researchers propose translating English SFT data into multiple languages (reference: 13).
12. Direct translation of English SFT data has two major drawbacks: low cultural diversity and imprecise translations, and linearly increased data volume (reference: 16, 18).
13. The researchers took a new approach to construct multilingual SFT data which increases cultural diversity and reduces data volume (reference: 23, 24, 28).
14. They trained an SFT LLM on the proposed UltraLink dataset, which showed superior performance when compared to several representative open-source multilingual LLMs (reference: 32).
15. Automatically generating SFT data is important in LLMs research, and it's crucial to consider cultural diversity and integrating language-agnostic universal data for multilingual SFT (reference: 34).
16. The proposed data construction framework encompasses two pipelines to handle these concerns (reference: 35).
17. The researchers proposed a knowledge-grounded data augmentation method to improve cultural diversity of multilingual LLMs (reference: 38).
18. For generating multi-turn dialogues, a question generator and an answer generator based on GPT-3.5 were designed (reference: 49).


-----------------
1. An Open-Source Knowledge-EnhancedMultilingual Supervised Fine-tuning Dataset$   Shuo Wang$^*dag1$   Yukun Yan$^1$   Xujia Wang$^1$   $^3$   $^1$   $^4$   $^3$   $^1$   $^1$   $^dag1$   $^1$   $^1$Tsinghua University quad $^2$Beijing University of Posts and Telecommunications   $^3$Beijing Language and Culture University quad $^4$Northeastern University, China     maketitleOpen-source large language models (LLMs) have gained significant strength across diverse fields. 2. Nevertheless, the majority of studies primarily concentrate on English, with only limited exploration into the realm of multilingual abilities. 3. In this work, we therefore construct an open-source multilingual supervised fine-tuning dataset. 4. Different from previous works that simply translate English instructions, we consider both the language-specific and language-agnostic abilities of LLMs. 5. Firstly, we introduce a knowledge-grounded data augmentation approach to elicit more language-specific knowledge of LLMs, improving their ability to serve users from different countries. 6. Moreover, we find modern LLMs possess strong cross-lingual transfer capabilities, thus repeatedly learning identical content in various languages is not necessary. 7. Consequently, we can substantially prune the language-agnostic supervised fine-tuning (SFT) data without any performance degradation, making multilingual SFT more efficient. 8. The resulting UltraLink dataset comprises approximately 1 million samples across five languages (i.e., En, Zh, Ru, Fr, Es), and the proposed data construction method can be easily extended to other languages. 9. UltraLink-LM, which is trained on UltraLink, outperforms several representative baselines across many tasks.. 10. Thanks to the collaborative efforts of the active large language models (LLMs) community, open-source LLMs are becoming increasingly powerful~, even outperforming some representative closed-source counterparts~ in some specific tasks~. 11. These accomplishments are closely related to the contribution of open-source supervised fine-tuning (SFT) data~, which plays a pivotal role in eliciting the instruction-following ability of LLMs and aligning the model behavior with human preferences. 12. Nevertheless, the focus of existing works is primarily on the construction of English SFT data, resulting in a comparatively limited availability of multilingual SFT resources. 13. centering    includegraphicspictures/data_distribution.pdf        To mitigate the challenge of data scarcity, some researchers suggest translating English SFT data into multiple languages. 14. utilize ChatGPT to translate the two essential components, instructions and responses, from Alpaca-style~ English data to other languages. 15. propose to translate both the Alpaca and the ShareGPT data. 16. While directly translating English SFT data can effectively support multilingual SFT, there are still two major drawbacks associated with this approach:    item em Low cultural diversity and imprecise translations caused by cultural differences: translation of English data may not adequately encompass topics specific to non-English regions (e.g., subjects related to Russian culinary culture), leading to a deficiency in language-specific knowledge for LLMs. 17. Moreover, for certain instructions (e.g., ), the answers vary in different cultural backgrounds, so directly translating all English conversations may result in numerous distorted translations. 18. item em Linearly increased data volume: the total volume of translated SFT data linearly increases with the number of languages. 19. However, the translations across different languages are semantically equivalent, making the model repeatedly learn the same content. 20. We believe that a good multilingual LLM should not only possess language-specific knowledge but also be equipped with language-agnostic skills. 21. Figure~ gives an example of the two types of instructions. 22. We thus propose a new approach to better construct multilingual SFT data, applicable to any language. 23. Compared to conversation translation~, our advantages can be illustrated as follows:    item em Higher cultural diversity and less distorted translations: for language-specific data, we propose a knowledge-grounded data augmentation method. 24. Concretely, Wikipedia is employed as a knowledge base for each language to provide more language-specific contexts. 25. For language-agnostic chat data (e.g., the second example in Figure~), we propose a two-stage translation mechanism. 26. Given high-quality English SFT data, we first filter out the conversations that are specific to certain regions. 27. Then we translate the remaining language-agnostic data. 28. item em Pruned data volume: for language-agnostic skills like math reasoning and code generation, through our experiments, we find that it is unnecessary for the model to repeatedly learn identical problems, thanks to the strong cross-lingual transfer capabilities of modern LLMs. 29. We can thus significantly prune the amount of math and code SFT data for non-English languages without compromising the model performance. 30. smallWe apply the aforementioned approach to four non-English languages, including Chinese, Russian, French, and Spanish. 31. Note that our method can also be easily extended to other languages. 32. Finally, we train an SFT LLM on the proposed UltraLink dataset, which outperforms several representative open-source multilingual LLMs, demonstrating the effectiveness of our dataset. 33. centering    includegraphicspictures/flow_diagram.pdf        Automatically generating SFT data is now an important research topic for LLMs~. 34. For multilingual SFT, it is crucial to consider the influence of cultural diversity on language-specific data, while also integrating language-agnostic universal data that is related to the general ability of LLMs (i.e., math reasoning). 35. In this work, we propose a data construction framework consisting of two pipelines, as shown in Figure~. 36. The cultures around the world are vibrant and diverse, reflecting the lifestyles and perspectives of people from various countries and regions. 37. To better cater to diverse users, the cultural diversity of multilingual LLMs should be improved. 38. In this aspect, we propose a knowledge-grounded data augmentation method, leveraging language-specific knowledge bases to provide intricate and varied cultural backgrounds. 39. Our method mainly contains two steps: (1) preparing and sampling knowledge from knowledge bases as cultural backgrounds, and (2) steering LLMs to generate informative conversations given the provided cultural backgrounds. 40. For each language, we utilize Wikipedia dumps as the knowledge base, encompassing a diverse array of topics closely related to the respective culture. 41. We first use an open-source extraction toolkit to preprocess the raw dumps and get text descriptions for each entry. 42. Then we use the language identification model provided by ~ to remove contents that are not in the expected language. 43. For Chinese, we also use  to convert traditional Chinese texts into simplified Chinese. 44. Finally, we filter out documents that are shorter than 1K tokens or longer than 10K tokens. 45. The number of tokens is calculated by . 46. Given that most LLMs have a limited context length, we divide the whole text into segments whose lengths are between 1K and 2K. 47. We do not split whole sentences when performing text segmentation. 48. The preprocessed texts are used as contexts for the following dialogue generation procedure. 49. To automatically generate multi-turn dialogues, we designed a question generator and an answer generator, which are both based on GPT-3.5. 50. When generating the dialogue, both the question and answer generators are conditioned on a provided text segment as the cultural background. 51. The used prompts can be divided into four parts: system prompt, principles, cultural background, and dialogue history. 52. The prompt structure is shown in Figure~. 53. The system prompt is used to describe the task (i.e., ). 54. The principles provide some detailed suggestions for the LLM, which are found important for improving the quality of the generated data. 55. The cultural background is the preprocessed text segment that contains language-specific knowledge. 56. The dialogue history provides the historical questions and answers, which is set to an empty string when generating the initial question. 57. small  The principles used to generate the first question are shown in Figure~. 58. We ask the involved LLM (i.e., GPT-3.5) to understand the provided cultural background and then propose a related question that can be answered according to the cultural background. 59. For the generation of answers, we provide only a concise description of the principles in Figure~ due to space limitations. 60. For each language, the principles are translated by humans into the target language. 61. We only show the English version of the prompt to better understand the method. 62. smalltexttt1. 63. Pose "why" and "how" questions: given the provided document, ask why something happens or how it occurs. 64. The questions should guide respondents to engage in more in-depth analysis and explanation, rather than simply stating facts. 65. 2. 66. Compare and contrast: if the text mentions a phenomenon or viewpoint, you can try comparing it with other similar situations and then pose questions to explore the similarities and differences between them, as well as potential impacts. 67. 3. 68. Predict future developments: if the text refers to a trend or direction of development, you can pose questions to discuss possible changes in the future or express opinions and predictions about a particular trend. 69. 4. 70. Stimulate reflection and discussion: Pose open-ended questions to encourage respondents to delve into deeper reflection and discussion. 71. smalltexttt1. 72. Understand the content. 73. 2. 74. Logically reason about details. 75. 3. 76. Compare relevant situations. 77. 4. 78. Discuss future trends. 79. 5. 80. Engage in deeper discussion. 81. After generating the initial question and answer, we iteratively produce subsequent dialogues. 82. To improve the diversity of constructed dialogues, we propose two types of subsequent questions. 83. At each turn, we randomly decide whether to present an em in-depth question for a more detailed exploration of the same topic or to generate an em expansive question to delve into other subjects. 84. The principles used to ask an in-depth question are shown in Figure~, while the principles used to ask an expansive question are shown in Figure~. 85. Note that when generating subsequent dialogues, the cultural background is also provided to 
The authors present an open-source multilingual supervised fine-tuning dataset to improve the multilingual abilities of large language models (LLMs) (reference: 3). They introduce a knowledge-grounded data augmentation approach and demonstrate the strong cross-lingual transfer capabilities of modern LLMs, leading to substantial pruning of language-agnostic supervised fine-tuning data without performance degradation (reference: 5, 6, 7). The resulting UltraLink dataset comprises approximately 1 million samples across five languages and outperforms several representative baselines across various tasks (reference: 8, 9). The authors also propose a new approach to construct multilingual supervised fine-tuning data, emphasizing higher cultural diversity and pruned data volume (reference: 23, 24, 25, 28, 30). They engage in automatically generating multilingual supervised fine-tuning data and emphasize the importance of considering cultural diversity and integrating language-agnostic universal data (reference: 33, 34). The authors propose a data construction framework involving two pipelines and employ a knowledge-grounded data augmentation method based on Wikipedia dumps to improve cultural diversity (reference: 35, 37, 39, 40, 41). They use prompts to generate multi-turn dialogues conditioned on provided cultural backgrounds and dialogue history (reference: 49, 50, 51, 57, 58). The authors also propose two types of subsequent questions to improve the diversity of constructed dialogues (reference: 82, 83, 84).

-----------------

-----------------
['backgrounds', 'help', 'humans', 'explore', 'carefully', 'serves', '7.6', '3-3', 'calculated', 'data', 'symbols', 'model', 'select', 'features', '1.7', 'LLM', 'Organization', 'provide', 'divide', 'languages', 'constructs', 'contribution', 'unlock', 'community', '13.4', 'translating', 'vocabulary', 'preparing', 'employ', 'filtering', 'experiments', 'jokes', '3.1', 'general', 'Guanaco-13b', 'undergo', 'utilizing', 'study', 'Bloomz-7b1', 'mentions', 'improved', 'intricate', 'examples', '5.1', 'Extend', 'tasks', '37.6', 'initial', '39.6', '63.6', 'users', 'array', '5.5', 'Fine-Tuning', 'differences', 'item', 'counterparts~', 'supervised', 'documents', 'shorter', 'Food', 'achieved', '18.3', '2.8', 'AlpacaEval~', '53.0', 'prompt', 'closely', 'ascertain', 'skills', 'slightly', 'focus', 'politics', 'thorough', 'Okapi-7b~', 'construct', 'better', 'Ru', 'Bloomz-7b1-mt~', 'efforts', 'Hence', 'benchmark', 'crucial', 'tune', 'inference', '60.4']

-----------------
{'sifted_words': ['focus', 'multiple', 'task', 'directly', 'respondents', 'programming', 'results', 'bases', 'Guanaco-13b', 'Northeastern', 'Phoenix-inst-chat-7b', 'serves', 'subjects', 'future', 'encourage', 'preferences', 'backgrounds', 'conversations', 'samples', 'answers', 'conversation', 'highlighting', 'CodeLlama-34b-Python~', 'Llama-2-7b~', 'toxicity', 'code', 'thanks', 'professional', 'Question', 'Average', 'relevant', 'encounter', 'Tsinghua', 'province', 'various', 'encompass', 'cultures', 'LaTex', 'outperforming', 'Yan', 'filter', 'GPUs', 'effectiveness', 'wide', 'help', 'drawbacks', 'role', 'iterations', 'approach', 'lower', 'notably', 'purposes', 'necessary', 'explanation', 'observe', 'epochs', 'compromising', 'Abstract', 'Turn', 'languages', 'ways', 'deeper', 'conduct', 'highly', 'shares', 'achieves', 'segment']}

-----------------
{'sifted_words': ['expansive', 'conversations', 'problems', '0.3', '13.4', 'contextualizing', 'spatial', '43.9', 'provide', 'toolkit', 'languages', 'rate', 'test', 'knowledge-grounded', 'Shuo', 'Knowledge-Enhanced', 'supervised', 'performances', 'range', 'knowledge', 'mentions', '10K', 'tasks', 'potential', 'statistics', 'Understand', 'transfer', '30K', 'aligning', '0.5', 'filters', 'scarcity', 'process', '123.57', 'China', 'essential', 'changes', 'temporal', '1.6', '1K', 'constructing', '2-5', 'culinary', 'Food', 'descriptions', 'dialogues', 'GPT-3.5-turbo', 'peak', 'Answer', '83.31', 'translation~', 'questions', 'system', 'Backbone', 'Wikipedia', 'generated', 'facts', '3', 'materials', 'users', 'logic', 'Take', 'reflecting', 'maintain', '60.4', 'schedule', '5.6', 'evaluate', 'observe', 'Guanaco-7b~', 'Length', 'fine-tune', 'highly', 'Instead', 'self-instruct~', 'pose', 'cosine', 'culture', '200.07', '2K', 'superiority']}

-----------------
{'sifted_words': ['Huggingface', 'express', 'number', 'phenomenon', 'discussion', 'estimated', 'tied', 'code', 'suggest', 'utilized', 'direction', 'work', 'question', 'learn', 'reduce', 'warm-up', 'sets', 'Pose', 'preparing', 'essential', 'similar', 'intricate', 'results', 'superiority', 'Notably', 'Base', 'undergo', 'cloth', 'learning', 'evaluate', 'outlined', 'Turns', 'differences', 'references', 'yield', 'recommend', 'important', 'goal', 'tasks', 'imposed', 'criteria', 'spectrum', 'combine', 'component', 'majority', 'pivotal', 'Bloomz-7b1-mt~', 'contrast', 'language-specific', 'consider', 'background', 'equivalent', 'focus', 'multi-turn', 'Answer', 'split', 'total', 'varied', 'proposes', 'prescribed', 'subjects', 'segments', 'dataset', 'multiple', 'conditioned', 'Phoenix', 'occurs', 'self-instruct~', 'propose', 'Alpaca', 'generator']}

-----------------
['Figure', 'target-language', 'major', 'fields', 'question', 'people', 'explore', 'Turns', 'encompass', 'pressing', 'toolkit', 'downloaded', 'fine-tune', 'propose', 'similarities', 'consider', 'vocabulary', 'transfer', 'training', 'Length', 'proposes', 'vibrant', 'Question', 'overarching', 'seed', 'fine-tuned', 'Lang.Spec', 'epochs', 'found', 'several', 'longer', 'retain', 'slightly', 'construct', 'performing', 'conversation', 'filter', 'imprecise', 'segments', 'contribution', 'Engage', 'expressions', 'combine', 'whose', 'tasks', 'Thanks', 'Model', 'initial', 'results']

-----------------
['relevant', 'faces', 'ascertain', 'consider', 'chat', 'set', 'string', 'adhere', 'counterparts~', 'Task', 'mathematical', 'target', 'impacts', 'Take', 'Table~', 'Culture', 'GPT-3.5-turbo', 'challenges', 'PolyLM-Chat-13b', 'imposed', 'UltraLink-LM', 'Challenge', 'constructing', 'overarching', 'furniture', 'Phoenix-inst-chat-7b', 'Full', 'differences', 'bases', 'full', 'PolyLM-13b', 'need', 'subsequent', 'cloth', 'token', 'shows', 'longest', 'Supervised', 'role', 'costs', 'tied', 'materials', 'stage', 'maximum', 'respondents', 'goal', 'addition', 'preprocess', 'latter', 'scale', 'Use', 'UltraChat~', 'particular', 'calculated', 'terms', 'utilizing', 'consistency', 'various', 'midrule', 'Guanaco-13b~', 'Finally', 'Food', 'Figure~', 'spatial', 'Avg', 'equal', 'lower', 'range']

-----------------
['yield', 'studies', 'confirms', 'turns', 'crucial', 'lengths', 'Stimulate', 'performance', 'includegraphicspictures/flow_diagram.pdf', 'document', 'constructing', 'Alpaca-style~', 'reflecting', 'shorter', 'found', 'guide', 'select', 'benchmark', 'split', 'responses', 'data~', 'fine-tuning', 'Engage', 'powerful', 'Chimera-inst-chat-13b', 'exclusively', 'Organization', 'mitigate', 'undergo', 'AlpacaEval~', 'content', 'highly', 'two-stage', 'significantly', '893K', 'conditioned', 'extend', 'encompass', 'data', 'Surprisingly', 'maintain', 'estimate', 'various', 'steps', 'citations', 'help', 'peak', 'fine-tune', 'mathematical', 'Lang.Spec', 'users', 'contexts', 'tied', 'quantities', 'documents', 'assumption', 'respectively', 'perspective', 'viewpoint', 'cosine', 'total', 'open-ended', 'overarching', 'applicable', 'introduce', 'leading', 'Length', 'Our', 'Okapi-7b~', 'believe', 'includegraphicspictures/data_distribution.pdf', 'also', 'demonstrating', 'bases', 'OMGEval', 'shows', 'raw', 'complex', 'amounts', 'primarily']

-----------------
['provide', 'remaining', 'Pose', 'Turn', 'facts', 'furniture', 'account', 'diversity', 'generating', 'apply', 'seed', 'challenges', 'fine-tuning', 'answer', 'tasks', 'completeness', 'informative', 'GPUs', 'shows', 'constructing', 'Chimera-inst-chat-13b', 'vocabulary', 'comparison', 'result', 'retain', 'real-world', 'illustrated', 'state', 'employed', 'topic', 'confirms', 'efficacy', 'amount', 'open-ended', 'improve', 'get', 'process', 'LaTex', 'Llama-13b', 'better', 'Zh', 'religion', 'well', 'answered', 'predictions', 'notably', 'abilities', 'example', 'demonstrating', 'involved', 'difficult', 'Conventions', 'Phoenix', 'degradation', 'cosine', 'identification', 'Predict', 'trends', 'OMGEval', 'cloth', 'supplementary', 'Guanaco-13b', 'background', 'Hence', 'designed', 'examples', 'constraints', 'essential', 'pre-training', 'test', 'Chimera-inst-chat-13b~', 'document', 'researchers', 'knowledge-grounded', 'use', 'underscores', 'Discuss', 'centering', 'parts', 'three', 'consider', 'segment', 'consistency', 'something', 'Open-source', 'volume', 'translation~', 'delve', 'combine', 'preprocess', 'dialogues', 'English-specific', 'pioneers', 'leveraging', 'semantically', 'rhymes', 'similar', 'iterations', 'localizes', 'diverse', 'specific', 'Wikipedia', 'used', 'describe', 'chat', 'Uncover', 'topics', 'Different', 'utility', 'longest', 'Answer', 'automatically', 'rewrites', 'traditional', 'Organization', 'contributes', 'expressions', 'surpassing', 'filters', 'tied', 'samples', 'tune', 'purposes', 'maximum', 'representative', 'toprule', 'Connect', 'counterparts~', 'plays', 'system', 'small', 'Shuo', 'SFT', 'observe', 'questions', 'availability', 'lengths', 'relevant', 'already', 'benchmark', 'suggests', 'humans', 'resulting', 'advantages', 'increase', 'occurs', 'Huggingface', 'epochs', 'evaluation', 'fluent', 'quantities', 'address', 'viewpoint', 'citations', 'product', 'convert', 'dag1', 'English', 'Okapi-7b', 'highly', 'language-specific', 'model', 'supervised', 'types', 'given', 'Based', 'calculated', 'PolyLM-Chat-13b', 'downloaded', 'guide', 'details', 'professional', 'Length', 'examine', 'substantially', 'divided', 'Backbone', 'learning', 'increased', 'Extend', 'large-scale', 'toolkit', 'contents', 'initial', 'world', 'performances', 'concise', 'reason', 'expression', 'cater', 'crucial', 'Ensure', 'graph', 'culture', 'temporal', 'leave', 'refers', 'aligning', 'adhere', 'split', 'countries', 'created', 'context', 'highlighting']

-----------------
['equivalent', 'abilities', 'linear', 'identical', 'thorough', 'expected', 'difficult', 'related', 'language', 'specific', 'UltraChat~', 'conditioned', 'mainly', 'datasets', 'open-source', 'explanation', 'MGSM', 'tree-style', 'descriptions', 'tune', 'documents', 'training', 'answers', 'Llama-13b', 'segment', 'Pose', 'copy', 'consistency', 'utilized', 'contribution', 'simplified', 'imposed', 'reflecting', 'materials', 'hallucination', 'Notably', 'backgrounds', 'questions', 'prescribed', 'based', 'issues', 'space', 'backbone', 'whole', 'accuracy', 'language-specific', 'dag1', 'particular', 'Bloomz-7b1-mt', 'fine-tune', 'ascertain', 'scale', 'number', 'tales', 'powerful', 'divide', 'situations', 'question', 'Yan', 'problems', 'thanks', 'version', 'instance', 'politics', 'advantages', 'exhibits', 'scarcity', 'score', 'Phoenix-inst-chat-7b', 'comparatively', 'include', 'trends', 'effectiveness', 'full', 'highly', 'construct', 'Discuss', 'Fr', 'xP3mt', 'construction', 'translation', 'system', 'Data', 'test', 'Open-Source', 'turns', 'viewpoints', 'Bloomz-7b1-mt~', 'produce', 'estimate', 'errors', 'active', 'quality', 'leading', 'Engage', 'filter', 'total', 'approach', 'Magicoder~', 'maintain', 'GPUs', 'users', 'contains', 'dialogue', 'preprocess', 'experiments', 'slang', 'countries', 'incorporate', 'Connect', 'integrating', 'detailed', 'contextualizing', 'caused', 'unnecessary', 'humans', 'achieves', 'Language', 'becoming', 'future', 'GPT-3.5', 'natural', 'human', 'multiple', 'translated', 'generate', 'cloth', 'Challenge', 'global', 'generation', 'maketitle', 'frequently', 'applied', 'Llama-2-13b~', 'Using', 'key', 'two', 'completeness', 'UltraLink-LM', 'centering', 'proposes', 'realm', 'efforts', 'Compared', 'graph', 'remove', 'terms', 'document', 'believe', 'temporal', 'utility', 'happens', 'second', 'Chinese', 'exploration', 'resources', 'removed', 'easily', 'Considering', 'criteria', 'strong', 'two-stage', 'Uncover', 'pivotal', 'Russian', 'distorted', 'extended', 'effective', 'account', 'differences', 'data~', 'languages', 'refers', 'includegraphicspictures/token_len.pdf', 'making', 'ability', 'subsequent', 'Open-source', 'simply', 'Guanaco', 'PolyLM-Chat-13b', 'Llama-2-7b', 'tasks~', 'city', 'imprecise', 'SFT', 'longer', 'steering', 'localizes', 'target', 'transfer', 'facts', 'volume', 'consists', 'introducing', 'Multilingual', 'automatically', 'En', 'iterations', 'availability', 'intricate', 'exemplified', 'superiority', 'Turns', 'directly', 'PolyLM-13b', 'longest', 'item', 'MGSM~', 'baselines', 'Beijing', 'preprocessed', 'acquire', 'spectrum', 'try', 'identification', 'limitations', 'ways', 'PolyLM-Multialpaca-13b~', 'example', 'slightly', 'segmentation', 'split', 'get', 'prompt', 'potential', 'principles', 'development', 'confirms', 'China', 'contents', 'paper', 'words', 'ask', 'responses', 'help', 'verify', 'Average', 'spatial', 'translate', 'proposed', 'better', 'notably', 'Llama-2-7b~', 'Guanaco-13b', 'components', 'shown', 'pressing', 'study', 'Backbone', 'serves', 'Low', 'large', 'Note', 'math', 'fine-tuning', 'Our', 'Task', 'expansive', 'state', 'filters', 'comprising', 'vibrant', 'translations', 'self-instruct~', 'PolyLM-Chat-13b~', 'text', 'select', 'repeatedly', 'subjects', 'comprises', 'LLMs', 'Turn', 'Multialpaca', 'AlpacaEval', 'closely', 'Predict', 'Experiments', 'supplementary', 'Answer', 'Abstract', 'address', 'generators', 'challenges', 'Knowledge-Enhanced', 'Understand', 'chat', 'string', 'used', 'reason', 'discussion', 'divided', 'Firstly', 'code', 'supervised', 'contrast', 'generated', 'Each', 'culture', 'focus', 'toxicity', 'range', 'general', 'information', 'world', 'UltraLink', 'Avg', 'concise', 'pre-training', 'direction', 'take', 'models', 'several', 'dialogues', 'compromising', 'concentrate', 'increases', 'contributes', 'stage', 'consider', 'cbf', 'single-turn', 'filtering', 'tied', 'Based', 'thus', 'new', 'use', 'expressions', 'features', 'informative', 'trend', 'would', 'perform', 'Use', 'Yukun', 'adequately', 'OMGEval', 'Since', 'Base', 'UltraLink~', 'accurately', 'improving', 'increased', 'tokens', 'attach', 'PolyLM-Multialpaca-13b', 'behavior', 'employing', 'million', 'calculated', 'Length', 'corresponding', 'including', 'important', 'express', 'attained', 'discussed', 'good', 'Table~', 'generator', 'augmentation', 'enabling', 'Ru', 'Given', 'similar', 'Lang', 'downloaded', 'respective', 'Magicoder', 'multilingual', 'Country', 'Chat', 'Culture', 'dataset', 'expression', 'includegraphicspictures/code_data_volume.pdf', 'occurs', 'Okapi-7b~', 'imply', 'consisting', 'multi-criteria', 'Closed-source', 'contexts', 'details', 'influence', 'amount', 'encourage', 'Alpaca~', 'linearly', 'community', 'deficiency', 'show', 'content', 'utilizing', 'span', 'efficacy', 'University', 'five', 'highlighting', 'best', 'many', 'GPT-3.5-turbo', 'benchmark', 'entry', 'constraints', 'lifestyles', 'illustrated', 'Okapi-7b', 'Alpaca', 'Phoenix-inst-chat-7b~', 'estimated', 'Fine-tuning', 'ratio', 'parts', 'Spanish', 'programming', 'substantially', 'constructing', 'Figure~', 'OMGEval~', 'tasks', 'lead', 'support', 'outlined', 'schedule', 'constructs', 'SODA~', 'Model', 'essential', 'length', 'Ensure', 'sampling', 'peak', 'conversation', 'social', 'utilizes', 'representative', 'opinions', 'Dataset', 'base', 'researchers', 'references', 'higher', 'culinary', 'Zh', 'applicable', 'performances', 'comparing', 'segments', 'accomplishments', 'prune', 'Telecommunications', 'efficient', 'steps', 'need', 'Linearly', 'provide', 'Stimulate', 'employ', 'open-ended', 'specified', 'common', 'topics', 'engage', 'faces', 'underscores', 'Full', 'officially', 'increasingly', 'majority', 'samples', 'involved', 'Okapi', 'multi-turn', 'Higher', 'furniture', 'cater', 'lengths', 'maximum', 'fields', 'Different', 'leave', 'phenomenon', 'framework', 'HumanEval', 'collaborative', 'shorter', 'Phoenix', 'reflection', 'LaTex', 'adhere', 'utilization', 'preparing', 'resulting', 'combine', 'vary', 'sets', 'learning', 'result', 'suggest', 'quantities', 'limited', 'historical', 'answered', 'technique', 'rrrr', 'context', 'array', 'retain', 'trained', 'introduce', 'Surprisingly', 'Supervised', 'assumption', 'Pruned', 'Chimera-inst-chat-13b', 'challenge', 'Bloomz-7b1', 'enhances', 'us', 'employed', 'seed', 'English-specific', 'Wang', 'deeper', 'sequence', 'similarities', 'Chimera-inst-chat-13b~', 'latter', 'costs', 'small', 'Automatically', 'Question', 'recommend', 'numerous', 'Section~', 'initial', 'Lang.Spec', 'statistics', 'logic', 'traditional', 'equipped', 'already', 'prompts', 'overarching', 'enhance', 'predictions', 'created', 'propose', 'Then', 'Conventions', 'evaluate', 'Guanaco-13b~', 'dataset~', 'solely', 'target-language', 'people', 'leverage', 'ShareGPT', 'discuss', 'semantically', 'addition', 'pipelines', 'necessary', 'previous', 'shows', 'LLMs~', 'company', 'Poetry', 'elicit', 'These', 'superior', 'commonsense', 'knowledge', 'knowledge-grounded', 'per', 'perspectives', 'answer', 'works', 'degradation', 'yield', 'real-world', 'understand', 'bottomrule', 'deleting', 'model', 'sentences', 'comparison', 'rhymes', 'performance', 'improvement', 'regions', 'modern', 'There', 'MetaMath~', 'encompassing', 'mechanism', 'four', 'delve', 'Finally', 'Dataset~', 'changes', 'cultural', 'original', 'vocabulary', 'impacts', 'respondents', 'demonstrating', 'stating', 'method', 'overall', 'types', 'part', 'results', 'times', 'evaluation', 'Lang.Agno', 'outperforms', 'Delve', 'turn', 'broad', 'maximize', 'English', 'bases', 'goal', 'follows', 'randomly', 'surpassing', 'designed', 'conduct', 'wide', 'Es', 'provided', 'train', 'jokes', 'carefully', 'Shuo', 'cultures', 'shares', 'name', 'ChatGPT', 'rich', 'Xujia', 'gives', 'universal', 'presents', 'epochs', 'associated', 'Llama-2-13b', 'This', 'set', 'different', 'lr', 'mitigate', 'citations', 'unlock', 'improved', 'large-scale', 'mentions', 'found', 'cosine', 'around', 'outperforming', 'analysis', 'Concretely', 'developments', 'Fine-Tuning', 'preferences', 'dumps', 'major', 'data', 'achieved', 'If', 'midrule', 'instructions', 'observe', 'rate', 'powerful~', 'procedure', 'Across', 'aspect', 'whether', 'Code', 'reasoning', 'religion', 'HumanEval~', 'utilize', 'Posts', 'existing', 'complex', 'Nevertheless', 'diverse', 'one', 'leveraging', 'average', 'learn', 'extend', 'still', 'suggestions', 'Food', 'Hence', 'purposes', 'process', 'mere', 'province', 'French', 'explore', 'fluency', 'topic', 'language-agnostic', 'N', 'reduce', 'lower', 'Guanaco-7b~', 'From', 'Guanaco-7b', 'AlpacaEval~']
