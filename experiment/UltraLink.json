{"text": "An Open-Source Knowledge-Enhanced\nMultilingual Supervised Fine-tuning Dataset\n\n$ \n  Shuo Wang$^*dag1$ \n  Yukun Yan$^1$ \n  Xujia Wang$^1$ \n  $^3$ \n  $^1$ \n  $^4$ \n  $^3$ \n  $^1$ \n  $^1$ \n  $^dag1$ \n  $^1$ \n  $^1$Tsinghua University quad $^2$Beijing University of Posts and Telecommunications \n  $^3$Beijing Language and Culture University quad $^4$Northeastern University, China \n  \n  \nmaketitle\n\nOpen-source large language models (LLMs) have gained significant strength across diverse fields. Nevertheless, the majority of studies primarily concentrate on English, with only limited exploration into the realm of multilingual abilities. In this work, we therefore construct an open-source multilingual supervised fine-tuning dataset. Different from previous works that simply translate English instructions, we consider both the language-specific and language-agnostic abilities of LLMs. Firstly, we introduce a knowledge-grounded data augmentation approach to elicit more language-specific knowledge of LLMs, improving their ability to serve users from different countries. Moreover, we find modern LLMs possess strong cross-lingual transfer capabilities, thus repeatedly learning identical content in various languages is not necessary. Consequently, we can substantially prune the language-agnostic supervised fine-tuning (SFT) data without any performance degradation, making multilingual SFT more efficient. The resulting UltraLink dataset comprises approximately 1 million samples across five languages (i.e., En, Zh, Ru, Fr, Es), and the proposed data construction method can be easily extended to other languages. UltraLink-LM, which is trained on UltraLink, outperforms several representative baselines across many tasks..\n\n\n\n\nThanks to the collaborative efforts of the active large language models (LLMs) community, open-source LLMs are becoming increasingly powerful~, even outperforming some representative closed-source counterparts~ in some specific tasks~.\nThese accomplishments are closely related to the contribution of open-source supervised fine-tuning (SFT) data~, which plays a pivotal role in eliciting the instruction-following ability of LLMs and aligning the model behavior with human preferences. Nevertheless, the focus of existing works is primarily on the construction of English SFT data, resulting in a comparatively limited availability of multilingual SFT resources.\n\n\n    centering\n    includegraphicspictures/data_distribution.pdf\n    \n    \n\n\nTo mitigate the challenge of data scarcity, some researchers suggest translating English SFT data into multiple languages.  utilize ChatGPT to translate the two essential components, instructions and responses, from Alpaca-style~ English data to other languages.  propose to translate both the Alpaca and the ShareGPT data. While directly translating English SFT data can effectively support multilingual SFT, there are still two major drawbacks associated with this approach:\n\n\n    item em Low cultural diversity and imprecise translations caused by cultural differences: translation of English data may not adequately encompass topics specific to non-English regions (e.g., subjects related to Russian culinary culture), leading to a deficiency in language-specific knowledge for LLMs.\n    Moreover, for certain instructions (e.g., ), the answers vary in different cultural backgrounds, so directly translating all English conversations may result in numerous distorted translations.\n    item em Linearly increased data volume: the total volume of translated SFT data linearly increases with the number of languages. However, the translations across different languages are semantically equivalent, making the model repeatedly learn the same content.  \n\n\nWe believe that a good multilingual LLM should not only possess language-specific knowledge but also be equipped with language-agnostic skills. Figure~ gives an example of the two types of instructions. We thus propose a new approach to better construct multilingual SFT data, applicable to any language. Compared to conversation translation~, our advantages can be illustrated as follows:\n\n\n    item em Higher cultural diversity and less distorted translations: for language-specific data, we propose a knowledge-grounded data augmentation method. Concretely, Wikipedia is employed as a knowledge base for each language to provide more language-specific contexts.\n    For language-agnostic chat data (e.g., the second example in Figure~), we propose a two-stage translation mechanism. Given high-quality English SFT data, we first filter out the conversations that are specific to certain regions. Then we translate the remaining language-agnostic data.\n    item em Pruned data volume: for language-agnostic skills like math reasoning and code generation, through our experiments, we find that it is unnecessary for the model to repeatedly learn identical problems, thanks to the strong cross-lingual transfer capabilities of modern LLMs. We can thus significantly prune the amount of math and code SFT data for non-English languages without compromising the model performance.\n\n\n\n\nsmall\n\n\n\n\n\n\n\n\n\nWe apply the aforementioned approach to four non-English languages, including Chinese, Russian, French, and Spanish. Note that our method can also be easily extended to other languages. Finally, we train an SFT LLM on the proposed UltraLink dataset, which outperforms several representative open-source multilingual LLMs, demonstrating the effectiveness of our dataset.\n\n\n\n    centering\n    includegraphicspictures/flow_diagram.pdf\n    \n    \n\n\nAutomatically generating SFT data is now an important research topic for LLMs~. For multilingual SFT, it is crucial to consider the influence of cultural diversity on language-specific data, while also integrating language-agnostic universal data that is related to the general ability of LLMs (i.e., math reasoning). In this work, we propose a data construction framework consisting of two pipelines, as shown in Figure~. \n\n\n\nThe cultures around the world are vibrant and diverse, reflecting the lifestyles and perspectives of people from various countries and regions. To better cater to diverse users, the cultural diversity of multilingual LLMs should be improved. In this aspect, we propose a knowledge-grounded data augmentation method, leveraging language-specific knowledge bases to provide intricate and varied cultural backgrounds. Our method mainly contains two steps: (1) preparing and sampling knowledge from knowledge bases as cultural backgrounds, and (2) steering LLMs to generate informative conversations given the provided cultural backgrounds.\n\n\nFor each language, we utilize Wikipedia dumps as the knowledge base, encompassing a diverse array of topics closely related to the respective culture. We first use an open-source extraction toolkit to preprocess the raw dumps and get text descriptions for each entry. Then we use the language identification model provided by ~ to remove contents that are not in the expected language. For Chinese, we also use  to convert traditional Chinese texts into simplified Chinese. Finally, we filter out documents that are shorter than 1K tokens or longer than 10K tokens. The number of tokens is calculated by .\n\nGiven that most LLMs have a limited context length, we divide the whole text into segments whose lengths are between 1K and 2K. We do not split whole sentences when performing text segmentation.\nThe preprocessed texts are used as contexts for the following dialogue generation procedure.\n\n\n\nTo automatically generate multi-turn dialogues, we designed a question generator and an answer generator, which are both based on GPT-3.5. \nWhen generating the dialogue, both the question and answer generators are conditioned on a provided text segment as the cultural background.\nThe used prompts can be divided into four parts: system prompt, principles, cultural background, and dialogue history. The prompt structure is shown in Figure~.\nThe system prompt is used to describe the task (i.e., ). The principles provide some detailed suggestions for the LLM, which are found important for improving the quality of the generated data. The cultural background is the preprocessed text segment that contains language-specific knowledge. The dialogue history provides the historical questions and answers, which is set to an empty string when generating the initial question.\n\n\n\nsmall\n\n \n\n\n\n\n\n\n\n\n\n The principles used to generate the first question are shown in Figure~. We ask the involved LLM (i.e., GPT-3.5) to understand the provided cultural background and then propose a related question that can be answered according to the cultural background.\nFor the generation of answers, we provide only a concise description of the principles in Figure~ due to space limitations.\nFor each language, the principles are translated by humans into the target language. We only show the English version of the prompt to better understand the method.\n\n\n\nsmall\ntexttt1. Pose \"why\" and \"how\" questions: given the provided document, ask why something happens or how it occurs. The questions should guide respondents to engage in more in-depth analysis and explanation, rather than simply stating facts. \n2. Compare and contrast: if the text mentions a phenomenon or viewpoint, you can try comparing it with other similar situations and then pose questions to explore the similarities and differences between them, as well as potential impacts. \n3. Predict future developments: if the text refers to a trend or direction of development, you can pose questions to discuss possible changes in the future or express opinions and predictions about a particular trend. \n4. Stimulate reflection and discussion: Pose open-ended questions to encourage respondents to delve into deeper reflection and discussion.\n\n\n\n\n\n\n\nsmall\ntexttt1. Understand the content.\n2. Logically reason about details.\n3. Compare relevant situations.\n4. Discuss future trends.\n5. Engage in deeper discussion.\n\n\n\n\n\n\n\nAfter generating the initial question and answer, we iteratively produce subsequent dialogues. To improve the diversity of constructed dialogues, we propose two types of subsequent questions.\nAt each turn, we randomly decide whether to present an em in-depth question for a more detailed exploration of the same topic or to generate an em expansive question to delve into other subjects. The principles used to ask an in-depth question are shown in Figure~, while the principles used to ask an expansive question are shown in Figure~. Note that when generating subsequent dialogues, the cultural background is also provided to the model. We will attach all the full prompts in supplementary materials.\n\n\n\nsmall\ntexttt1. Understand the context.\n2. Uncover implicit information.\n3. Challenge existing viewpoints.\n4. Extend the topic.\n5. Pose open-ended questions.\n6. Delve into more complex logic.\n\n\n\n\n\n\n\nsmall\ntexttt1. Abstract the theme.\n2. Turn into overarching topics.\n3. Considering temporal and spatial span.\n4. Connect to related fields. \n5. Take a global perspective.\n\n\n\n\n\nUsing the aforementioned approach, we automatically construct language-specific multi-turn conversations in four languages.\nThe details of constructed data will be illustrated in Section~, including the average length and some other statistics.\nNote that the proposed knowledge-grounded data augmentation approach can also be applied to any other language.\n\n\n\n\nIn addition to language-specific abilities, the general abilities that are language-agnostic are also essential for LLMs. As numerous high-quality English SFT datasets already encompass a broad spectrum of general abilities, we suggest employing a two-stage translation mechanism to maximize the utility of existing English resources. Our goal is to reduce translation errors caused by cultural differences since some questions can not be directly translated into other languages (e.g., ). In the first stage, we introduce a multi-criteria mechanism to filter out English-specific conversations that are difficult to translate accurately into other languages. Then we use GPT-3.5 to translate the remaining language-agnostic data. \nIn this study, we consider three key components of general abilities for LLMs: chat, math reasoning, and code generation. For chat, we use ShareGPT as the English chat data, which consists of multi-turn dialogues between human users and ChatGPT. For math reasoning, we use MetaMath~ as the English math data. For code generation, we use the Magicoder dataset~ as the English code data.\n\n\n\nThe criteria employed to filter out English-specific conversations are outlined in Figure~. Our goal is to retain only conversations whose topics can be discussed in any cultural background.\nGPT-3.5 is utilized to ascertain whether a conversation contains information relevant to the specified features.\nFor instance, the conversations that include English jokes will be removed before translation.\n\n\n\nsmall\ntexttt1. Full name of *human*.\n2. Country, region, state, province, city, address.\n3. Conventions, politics, history, and religion.\n4. Poetry, rhymes, myths, tales, jokes, and slang.\n5. Food, cloth, furniture, construction.\n6. Organization, company, product, brand.\n\n\n\n\n\n\n\nAfter the filtering process, the remaining conversations undergo the translation procedure, wherein they are translated into four languages using GPT-3.5-turbo to maintain fluency and accuracy. We also provide some translation principles to help GPT-3.5 better perform the translation, which is shown in Figure~.\n\n\n\nsmall\ntexttt1. Ensure the completeness and consistency of content during the translation process, without adding or deleting any information.\n2. Ensure that the translated text is fluent and natural, using the most common expressions in the target language whenever possible. Use officially prescribed translations for professional terms and adhere to the target-language expression conventions.\n3. If certain terms are not in natural language but are mathematical symbols, programming languages, or LaTex language, please directly copy the original text.\n4. If there are no equivalent translation terms for certain vocabulary, please directly copy the original text.\n5. For citations and references, please directly copy the original text.\n\n\n\n\n\n\n\n\nEnglish math and code datasets are frequently extensive, exemplified by MetaMath~ with 395K training examples and Magicoder~ comprising 186K training examples. Assuming the English data consists of $N$ training examples, the overall multilingual dataset would encompass $k times N$ examples if we translate all the English training examples into other languages, where $k$ is the number of languages. The linear increase in data volume will result in higher training costs during SFT. As math and code problems are not closely tied to the cultural backgrounds of different countries, LLMs may have the capability to transfer English math and code abilities into other languages with only limited training examples. In other words, it may not be necessary for LLMs to learn all translated math and code problems.\nTo verify the assumption mentioned above, we conduct experiments on Chinese math and code tasks. For comparison, we fine-tune Llama-2-7b~ in the following two different ways:\n\n    item em From En SFT Model: we first use English math or code data to fine-tune the base model, and then use different amounts of Chinese data to further tune the model.\n    item em From Base Model: we directly use Chinese math or code data to fine-tune the base model.\n\n\n    centering\n    includegraphicspictures/math_data_volume.pdf\n    \n    \n\n\n    centering\n    includegraphicspictures/code_data_volume.pdf\n    \n    \n\nFigure~ and  show the performances of the two types of models. Surprisingly, the involved LLM exhibits strong cross-lingual transfer capabilities. For instance, utilizing only 2K Chinese mathematical training examples can yield a score of 45.6 when fine-tuning from the English SFT model. In contrast, directly fine-tuning the base model with an equivalent amount of Chinese data results in a significantly lower score of 22.0, highlighting the superior performance achieved through transfer from the English SFT model. In the Chinese code generation task, we observe a similar trend, wherein transfer learning from the English SFT model substantially enhances the performance of the model.\n\nMoreover, we find that using more Chinese SFT data does not consistently lead to improved performance. For the math task, using 32K Chinese training examples achieves the best result. For the code task, the peak performance is attained with 16K Chinese code generation examples. Hence, we incorporate only 32K mathematical training examples and 16K code training examples for each non-English language in the UltraLink dataset.\n\n\n    centering\n    small\n    l rrrr\n      toprule\n      *bf Lang. & cbf Lang.Spec. & cbf Lang.Agno. \n      cmidrule(lr)2-2 cmidrule(lr)3-5\n      & cbf Chat & cbf Chat & cbf Math & cbf Code \n      midrule\n      En & 10K & 67K & 395K & 186K \n      cmidrule(lr)1-1 cmidrule(lr)2-5\n      Zh & 36K & 11K & 32K & 16K \n      Ru & 37K & 11K & 32K & 16K \n      Fr  & 30K & 11K & 32K & 16K \n      Es & 34K & 11K & 32K & 16K \n      cmidrule(lr)1-1 cmidrule(lr)2-5\n      UltraLink    & 147K & 112K & 523K & 250K \n      ~w/o En & 137K & 45K & 128K & 64K \n      bottomrule\n    \n    \n    \n\n\n\n13.5pt\ncentering\nsmall\nl rrrrr\n  toprule\n  *bf Dataset & *bf Dialogues & *bf Turns & cbf Average Length \n  cmidrule(lr)4-6\n  & & & bf Question & bf Answer & bf Turn \n  midrule\n  Okapi Dataset~ & 207K & 207K & 28.64 & 95.72 & 124.36\n  Guanaco Dataset~ & bf 1173K & 1173K & 77.58 & 83.31 & 160.89\n  Multialpaca~ & 132K & 132K & 39.86 & 83.71 & 123.57\n  Phoenix SFT data~ & 464K & 893K & bf 165.27 & 200.07 & 365.34\n  midrule\n  UltraLink~() & 1032K & bf 1623K & 87.86 & bf 290.35 & bf 378.21 \n  bottomrule\n\n\n\n\n\n\n  8pt\n  centering\n  small\n  l l l rrrrrr \n    toprule\n    *bf Model &  *bf Backbone & *bf SFT Data & cbf OMGEval (Chat)  \n    cmidrule(lr)4-9\n    & & & En & Zh & Es & Ru & Fr & Avg. \n    midrule\n     Bloomz-7b1-mt & Bloomz-7b1 & xP3mt & 0.0 & 0.9 & 0.1 & 0.5 & 0.3 & 0.4 \n     Phoenix-inst-chat-7b & Bloomz-7b1 & Phoenix SFT data & 6.9 & 13.3 & 7.4 & 2.9 & 8.1 & 7.7 \n     PolyLM-Multialpaca-13b & PolyLM-13b & Multialpaca & 3.4 & 5.0 & 2.1 & 5.1 & 2.2 & 3.6 \n     PolyLM-Chat-13b & PolyLM-13b & Closed-source & 7.7 & 14.0 & 6.1 & 5.5 & 4.8 & 7.6 \n     Chimera-inst-chat-13b & Llama-13b & Phoenix SFT data & 15.5 & 9.7 & 11.8 & 13.7 & 13.8 & 12.9 \n     Okapi-7b &  Llama-2-7b &  Okapi Dataset & 8.8 & 6.2 & 5.0 & 12.1 & 8.7 & 8.2 \n     Guanaco-7b & Llama-2-7b & Guanaco Dataset & 4.6 & 3.8 & 0.4 & 1.8 & 1.2 & 2.4 \n     Guanaco-13b & Llama-2-13b & Guanaco Dataset & bf 29.0 & 8.6 & 16.9 & 15.4 & 17.3 & 17.5 \n     cmidrule(lr)1-1 cmidrule(lr)2-2 cmidrule(lr)3-3 cmidrule(lr)4-9  \n     UltraLink-LM & Llama-2-13b & UltraLink &  28.8 & bf 21.9 & bf 23.5 & bf 37.6 & bf 29.0 & bf 28.2  \n     midrule\n     *bf Model &  *bf Backbone & *bf SFT Data & cbf Multilingual HumanEval (Code)  \n    cmidrule(lr)4-9\n    & & & En & Zh & Es & Ru & Fr & Avg. \n    midrule\n     Bloomz-7b1-mt & Bloomz-7b1 & xP3mt & 8.5 & 7.3 & 6.1 & 8.5 & 6.1 & 7.3 \n     Phoenix-inst-chat-7b & Bloomz-7b1 & Phoenix SFT data & 11.0 & 10.4 & 8.5 & 1.2 & 13.4 & 12.2 \n     PolyLM-Multialpaca-13b & PolyLM-13b & Multialpaca & 8.5 & 7.3 & 6.1 & 6.1 & 6.1 & 6.8 \n     PolyLM-Chat-13b & PolyLM-13b & Closed-source & 10.4 & 7.9 & 6.1 & 7.3 & 8.5 & 8.1 \n     Chimera-inst-chat-13b & Llama-13b & Phoenix SFT data & 14.6 & 13.4 & 14.6 & 12.8 & 14.0 & 13.9 \n     Okapi-7b &  Llama-2-7b &  Okapi Dataset & 12.2 & 11.0 & 8.5 & 8.5 & 8.5 & 9.8 \n     Guanaco-7b & Llama-2-7b & Guanaco Dataset & 9.2 & 6.7 & 11.0 & 9.8 & 12.8 & 9.9 \n     Guanaco-13b & Llama-2-13b & Guanaco Dataset & 18.3 & 15.9 & 9.8 & 8.5 & 14.6 & 12.2 \n     cmidrule(lr)1-1 cmidrule(lr)2-2 cmidrule(lr)3-3 cmidrule(lr)4-9 \n     UltraLink-LM & Llama-2-13b & UltraLink & bf 60.4 & bf 43.9 & bf 40.9 & bf 49.4 & bf 39.6 & bf 46.8  \n    midrule\n        *bf Model &  *bf Backbone & *bf SFT Data & cbf MGSM (Math)  \n    cmidrule(lr)4-9\n    & & & En & Zh & Es & Ru & Fr & Avg. \n    midrule\n     Bloomz-7b1-mt & Bloomz-7b1 & xP3mt & 2.8 & 1.6 & 2.0 & 0.4 & 2.8 & 1.7 \n     Phoenix-inst-chat-7b & Bloomz-7b1 & Phoenix SFT data & 3.2 & 3.2 & 2.8 & 3.2 & 3.2 & 3.1 \n     PolyLM-Multialpaca-13b & PolyLM-13b & Multialpaca & 1.2 & 2.8 & 1.6 & 2.8 & 2.4 & 2.4 \n     PolyLM-Chat-13b & PolyLM-13b & Closed-source & 10.8 & 6.4 & 4.8 & 4.4 & 5.6 & 5.3 \n     Chimera-inst-chat-13b & Llama-13b & Phoenix SFT data & 14.0 & 11.6 & 10.0 & 12.0 & 12.8 & 11.6 \n     Okapi-7b &  Llama-2-7b &  Okapi Dataset & 4.0 & 2.4 & 3.6 & 4.4 & 4.8 & 3.8 \n     Guanaco-7b & Llama-2-7b & Guanaco Dataset & 4.0 & 1.6 & 3.2 & 2.8 & 4.4 & 3.0 \n     Guanaco-13b & Llama-2-13b & Guanaco Dataset & 13.6 & 10.8 & 11.2 & 6.4 & 5.2 & 8.4 \n     cmidrule(lr)1-1 cmidrule(lr)2-2 cmidrule(lr)3-3 cmidrule(lr)4-9 \n     UltraLink-LM & Llama-2-13b & UltraLink & bf 70.4 & bf 56.0 & bf 70.4 & bf 64.8 & bf 63.6 & bf 63.7  \n    bottomrule\n  \n\n\n\n\n\n\n\n\nTable~ presents the scale of each component in UltraLink, encompassing five languages. Each language contributes four types of SFT data: chat data with language-specific knowledge, chat data with language-agnostic knowledge, math data, and code data. The quantities of language-agnostic segments are approximately equal for the four non-English languages.\n\n\n\nBefore us, there are some existing multilingual SFT datasets, where we select four representative datasets for comparison, including the Okapi dataset~, the Guanaco dataset~, Multialpaca~, and the Phoenix SFT data~. We conduct a comparison based on the number of dialogues, the number of conversation turns, and the average lengths across the respective datasets. As shown in Table~, we find that UltraLink contains fewer dialogues than the Guanaco dataset, but the latter only contains single-turn conversations. Only the Phoenix SFT data and UltraLink include multi-turn conversations.\n\nWe use the number of tokens estimated by  as the length for each question and answer. The question token length does not include the document. On average, UltraLink exhibits the longest average length per turn (i.e., 378.21 tokens), considering both questions and their corresponding answers. Compared to UltraLink, the Phoenix SFT data has longer questions (165.27 vs. 87.86), but its answers are shorter (200.07 vs. 290.35).\n\nFor each language, we also estimate the average lengths of questions and answers, and the results are shown in Figure~. Across all languages, the answer is significantly longer than the question.\n\n\n    centering\n    includegraphicspictures/token_len.pdf\n    \n    \n\n\n\n\n For thorough comparison, we select several representative multilingual baselines in our experiments, including Bloomz-7b1-mt~, Phoenix-inst-chat-7b~, PolyLM-Multialpaca-13b~, PolyLM-Chat-13b~, Chimera-inst-chat-13b~, Okapi-7b~, Guanaco-7b~, and Guanaco-13b~.\nOkapi-7b is fine-tuned by ourselves based on Llama-2-7b using the Okapi dataset, while other baselines are downloaded from Huggingface.\n\n Based on Llama-2-13b~, UltraLink-LM is fine-tuned with the constructed UltraLink dataset for 3 epochs. We use the cosine learning rate schedule and the peak learning rate is set to 2e-5. The warm-up ratio is set to 0.04. Each mini-batch contains 128 training examples in total. The maximum sequence length is 4096. We train the model using 32 A100 GPUs for about 140 hours.\n\n We examine the model performance on three tasks, including chat, math reasoning, and code generation. For chat, we use OMGEval~ for evaluation, which is a multilingual version of the widely-used English benchmark  AlpacaEval~.\nOMGEval is not a mere translated version of AlpacaEval. Instead, it localizes the English questions according to the cultural backgrounds of each language.\nWe employ MGSM~ to evaluate math reasoning abilities, which is also a multilingual benchmark.\nSince there are no existing multilingual test sets for code generation, we use GPT-3.5 with carefully designed prompts to translate HumanEval~ into other languages, which serves as the multilingual benchmark to evaluate the code abilities of LLMs. We use the  toolkit for model inference and evaluation, which supports a wide range of open-source models.\n\n\n\nTable~ shows the results of the involved multilingual SFT LLMs on different tasks. In terms of general chat abilities, our model achieves the best average results. While Guanaco-13b slightly outperforms us in English (29.0 vs. 28.8), its performance is notably lower than ours in non-English languages. Given that Guanaco-13b shares the same backbone (i.e., Llama-2-13b) with our model, the results imply the superiority of the proposed UltraLink dataset.\n\nFor the code generation Task, previous multilingual SFT datasets did not take into account the multilingual code abilities, which we think is very important in many real-world scenarios. Our model achieves a score of 60.4 in the English HumanEval benchmark, surpassing even CodeLlama-34b-Python~. For comparison, training the model solely on the English Magicoder~ dataset results in a HumanEval score of 53.0. The improvement of UltraLink-LM over the model trained on the English Magicoder dataset (i.e., 60.4 vs. 53.0) suggests that the constructed code SFT data in other languages can also enhance English code abilities. This confirms our assumption that modern LLMs possess strong transfer abilities for language-agnostic skills.\n\nIn the math reasoning task, our model consistently outperforms all other baselines across all five languages. The performance of UltraLink-LM in both math and code tasks underscores the effectiveness of our method in enabling multilingual LLMs to acquire general abilities.\n\n\n SFT is now a crucial part of constructing a powerful LLM. SODA~ constructs high-quality social dialogues by contextualizing social commonsense knowledge from a knowledge graph. Using the technique of self-instruct~, Alpaca~ is one of the pioneers to leverage ChatGPT to collect SFT data. \nUltraChat~ utilizes ChatGPT to generate topics in a tree-style structure for the construction of large-scale dialogues. With these efforts, English SFT resources are becoming increasingly rich and effective.\n\n\nTo enhance the global utility of LLMs, numerous multilingual SFT datasets have been created.  employ ChatGPT to translate Alpaca into various languages.  combine ShareGPT with Alpaca and then translate the two datasets.  and  extend tasks from Alpaca by introducing filters and rewrites of seed tasks in different languages, generating datasets through multiple iterations. This work proposes the utilization of a multilingual knowledge base to enhance the cultural diversity of multilingual Supervised Fine-Tuning data, as well as to improve the language-agnostic general abilities of LLMs through cross-lingual transfer learning.\n\n\nIn this work, we propose a knowledge-grounded data augmentation method and a two-stage translation mechanism to construct language-specific and language-agnostic multilingual SFT data, respectively.\nExperiments demonstrate that the proposed dataset is effective for multilingual LLMs.\n\n\n\nWe present a framework for generating SFT data across diverse languages and use the proposed dataset to learn an LLM. Our LLM may inevitably encounter common challenges, including issues such as hallucination and toxicity. We highly recommend users utilize our work exclusively for research purposes, to enhance the efficacy of LLMs across various languages.\n\n\n\nIn the paper, our proposed data construction framework is only applied to four language types. Nevertheless, the framework can be easily extended to other languages. We leave it to the future work to include more languages.\nMoreover, due to constraints imposed by the base model, the multilingual capability still faces several limitations. Notably, the model exhibits significantly better performance in English across many tasks. There is a pressing need to continue constructing high-quality pre-training multilingual datasets, to unlock the full potential of multilingual abilities in LLMs.\n\n\n\n\n"}